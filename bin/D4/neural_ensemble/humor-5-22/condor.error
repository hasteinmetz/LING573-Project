Using the GPU:Tesla M10
Fitted PCA. Previously there were 13204 features. Now there are 6399 features.
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
(0, 192) Loss: 0.650629160925746
(0, 384) Loss: 0.6572490008547902
(0, 576) Loss: 0.579143101349473
(0, 768) Loss: 0.5265264892950654
(0, 960) Loss: 0.3418425237759948
(0, 1152) Loss: 0.4267566096968949
(0, 1344) Loss: 0.3623110221233219
(0, 1536) Loss: 0.40106034837663174
(0, 1728) Loss: 0.26029515103437006
(0, 1920) Loss: 0.3101653449703008
(0, 2112) Loss: 0.25913904735352844
(0, 2304) Loss: 0.3965939315967262
(0, 2496) Loss: 0.21092516160570085
(0, 2688) Loss: 0.15042819501832128
(0, 2880) Loss: 0.16114154632668942
(0, 3072) Loss: 0.22652436245698482
(0, 3264) Loss: 0.1336180561920628
(0, 3456) Loss: 0.2630681898444891
(0, 3648) Loss: 0.23073169100098312
(0, 3840) Loss: 0.08426538144703954
(0, 4032) Loss: 0.29697619925718755
(0, 4224) Loss: 0.3148282337933779
(0, 4416) Loss: 0.32997469475958496
(0, 4608) Loss: 0.22891619696747512
(0, 4800) Loss: 0.23228783486410975
(0, 4992) Loss: 0.14986537920776755
(0, 5184) Loss: 0.1224108012393117
(0, 5376) Loss: 0.19216560845961794
(0, 5568) Loss: 0.17888003459665924
(0, 5760) Loss: 0.49535672465572134
(0, 5952) Loss: 0.14770579780451953
(0, 6144) Loss: 0.17828903393819928
(0, 6336) Loss: 0.2721790535724722
f1:
	 {'f1': 0.901249693702524}
accuracy:
	 {'accuracy': 0.8740428191904985}

	f1: {'f1': 0.9417670682730924}
	accuracy: {'accuracy': 0.9275}

(1, 192) Loss: 0.15091202355688438
(1, 384) Loss: 0.22516917646862566
(1, 576) Loss: 0.14615143183618784
(1, 768) Loss: 0.2986447991570458
(1, 960) Loss: 0.06863370520295575
(1, 1152) Loss: 0.39041546429507434
(1, 1344) Loss: 0.3578682648949325
(1, 1536) Loss: 0.2474439908983186
(1, 1728) Loss: 0.10825274221133441
(1, 1920) Loss: 0.1383264848846011
(1, 2112) Loss: 0.09489596675848588
(1, 2304) Loss: 0.08774388220626861
(1, 2496) Loss: 0.054306929756421596
(1, 2688) Loss: 0.08322575525380671
(1, 2880) Loss: 0.08392543997615576
(1, 3072) Loss: 0.03956350090447813
(1, 3264) Loss: 0.023712815367616713
(1, 3456) Loss: 0.16290019557345659
(1, 3648) Loss: 0.12441048008622602
(1, 3840) Loss: 0.09482997978921048
(1, 4032) Loss: 0.08460963220568374
(1, 4224) Loss: 0.13055193499894813
(1, 4416) Loss: 0.20759036546223797
(1, 4608) Loss: 0.1689039696357213
(1, 4800) Loss: 0.2664472202595789
(1, 4992) Loss: 0.17610748385777697
(1, 5184) Loss: 0.14331134042004123
(1, 5376) Loss: 0.07284593646181747
(1, 5568) Loss: 0.18839972963905893
(1, 5760) Loss: 0.3491947581351269
(1, 5952) Loss: 0.026409977697767317
(1, 6144) Loss: 0.024967474048025906
(1, 6336) Loss: 0.30112294436548837
f1:
	 {'f1': 0.9623356926188067}
accuracy:
	 {'accuracy': 0.9534302234724176}

	f1: {'f1': 0.937178166838311}
	accuracy: {'accuracy': 0.92375}

(2, 192) Loss: 0.047041691723279655
(2, 384) Loss: 0.21145526695181616
(2, 576) Loss: 0.05135978272301145
(2, 768) Loss: 0.16337679830030538
(2, 960) Loss: 0.06062047817977145
(2, 1152) Loss: 0.27689302133512683
(2, 1344) Loss: 0.1969058552349452
(2, 1536) Loss: 0.4385865319345612
(2, 1728) Loss: 0.06630695512285456
(2, 1920) Loss: 0.02930651218048297
(2, 2112) Loss: 0.02244705634075217
(2, 2304) Loss: 0.033055851847166196
(2, 2496) Loss: 0.013960958371171728
(2, 2688) Loss: 0.011994242231594399
(2, 2880) Loss: 0.012685260240687057
(2, 3072) Loss: 0.01048719449318014
(2, 3264) Loss: 0.14431011481792666
(2, 3456) Loss: 0.0260978193837218
(2, 3648) Loss: 0.22711321208043955
(2, 3840) Loss: 0.03172771303798072
(2, 4032) Loss: 0.03225310432026163
(2, 4224) Loss: 0.021127464569872245
(2, 4416) Loss: 0.14375740979448892
(2, 4608) Loss: 0.011093344044638798
(2, 4800) Loss: 0.1461727411369793
(2, 4992) Loss: 0.023842343915021047
(2, 5184) Loss: 0.026999363239156082
(2, 5376) Loss: 0.02481237032043282
(2, 5568) Loss: 0.03972037206403911
(2, 5760) Loss: 0.2927592500054743
(2, 5952) Loss: 0.05152328353142366
(2, 6144) Loss: 0.039440313703380525
(2, 6336) Loss: 0.032372227928135544
f1:
	 {'f1': 0.9795659347632949}
accuracy:
	 {'accuracy': 0.9748398187216752}

	f1: {'f1': 0.9379310344827586}
	accuracy: {'accuracy': 0.92125}


real	84m46.197s
user	78m32.276s
sys	5m30.957s

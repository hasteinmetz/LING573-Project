Using the GPU:NVIDIA Quadro RTX 8000
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 240) Loss: 0.5463096499443054
(0, 480) Loss: 0.36374202370643616
(0, 720) Loss: 0.38457587361335754
(0, 960) Loss: 0.30652156472206116
(0, 1200) Loss: 0.30720826983451843
(0, 1440) Loss: 0.3905690014362335
(0, 1680) Loss: 0.3423374593257904
(0, 1920) Loss: 0.3001268208026886
(0, 2160) Loss: 0.34416908025741577
(0, 2400) Loss: 0.36200591921806335
(0, 2640) Loss: 0.5978545546531677
(0, 2880) Loss: 0.24336214363574982
(0, 3120) Loss: 0.2914559543132782
(0, 3360) Loss: 0.3459955155849457
(0, 3600) Loss: 0.31935739517211914
(0, 3840) Loss: 0.24281148612499237
Training metrics:
	mse: {'mse': 0.5135895408399427}

Evaluation metrics:
	mse: {'mse': 0.3108383470609766}

(1, 240) Loss: 0.3081951439380646
(1, 480) Loss: 0.263302206993103
(1, 720) Loss: 0.2845294773578644
(1, 960) Loss: 0.3103163242340088
(1, 1200) Loss: 0.2604620158672333
(1, 1440) Loss: 0.29480376839637756
(1, 1680) Loss: 0.23911075294017792
(1, 1920) Loss: 0.2973787784576416
(1, 2160) Loss: 0.299844354391098
(1, 2400) Loss: 0.24908414483070374
(1, 2640) Loss: 0.5392293334007263
(1, 2880) Loss: 0.19041560590267181
(1, 3120) Loss: 0.27166709303855896
(1, 3360) Loss: 0.2741052806377411
(1, 3600) Loss: 0.2943468689918518
(1, 3840) Loss: 0.23674587905406952
Training metrics:
	mse: {'mse': 0.2735975007105832}

Evaluation metrics:
	mse: {'mse': 0.27251779945931837}

(2, 240) Loss: 0.2415223866701126
(2, 480) Loss: 0.22105951607227325
(2, 720) Loss: 0.23942089080810547
(2, 960) Loss: 0.22574332356452942
(2, 1200) Loss: 0.20679698884487152
(2, 1440) Loss: 0.10129647701978683
(2, 1680) Loss: 0.14089539647102356
(2, 1920) Loss: 0.24260829389095306
(2, 2160) Loss: 0.1629626750946045
(2, 2400) Loss: 0.1334640234708786
(2, 2640) Loss: 0.3662227988243103
(2, 2880) Loss: 0.15056926012039185
(2, 3120) Loss: 0.19605596363544464
(2, 3360) Loss: 0.16511030495166779
(2, 3600) Loss: 0.2394055873155594
(2, 3840) Loss: 0.1729697734117508
Training metrics:
	mse: {'mse': 0.19022902965824395}

Evaluation metrics:
	mse: {'mse': 0.29537402445526884}

(3, 240) Loss: 0.09027054160833359
(3, 480) Loss: 0.09083673357963562
(3, 720) Loss: 0.10248743742704391
(3, 960) Loss: 0.25006386637687683
(3, 1200) Loss: 0.10472413152456284
(3, 1440) Loss: 0.08667641878128052
(3, 1680) Loss: 0.08304160088300705
(3, 1920) Loss: 0.15713290870189667
(3, 2160) Loss: 0.13329973816871643
(3, 2400) Loss: 0.36201193928718567
(3, 2640) Loss: 0.2067372053861618
(3, 2880) Loss: 0.17541836202144623
(3, 3120) Loss: 0.1501159965991974
(3, 3360) Loss: 0.19813013076782227
(3, 3600) Loss: 0.22573857009410858
(3, 3840) Loss: 0.12116517871618271
Training metrics:
	mse: {'mse': 0.14384079156410676}

Evaluation metrics:
	mse: {'mse': 0.362515594896045}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
(0, 240) Loss: 0.6240246795117855
(0, 480) Loss: 0.5292310610413552
(0, 720) Loss: 0.4138990417122841
(0, 960) Loss: 0.27258222247473896
(0, 1200) Loss: 0.31534934588707986
(0, 1440) Loss: 0.30512959347106516
(0, 1680) Loss: 0.26218446709681303
(0, 1920) Loss: 0.33166611236520116
(0, 2160) Loss: 0.08188628514762969
(0, 2400) Loss: 0.3982609480735846
(0, 2640) Loss: 0.3611207518260926
(0, 2880) Loss: 0.3209841772215441
(0, 3120) Loss: 0.2512837651418522
(0, 3360) Loss: 0.43691724945092575
(0, 3600) Loss: 0.16721342732198538
(0, 3840) Loss: 0.1774769900366664
(0, 4080) Loss: 0.17949247881770136
(0, 4320) Loss: 0.33746203581104056
(0, 4560) Loss: 0.22297973723616452
(0, 4800) Loss: 0.23943177745677532
(0, 5040) Loss: 0.2004469041712582
(0, 5280) Loss: 0.18600958241149784
(0, 5520) Loss: 0.1825491413823329
(0, 5760) Loss: 0.3148616815218702
(0, 6000) Loss: 0.26980844523059205
(0, 6240) Loss: 0.24574602424399927
Training metrics:
	f1: {'f1': 0.9008695652173913}
	accuracy: {'accuracy': 0.8752930145335208}

Evaluation metrics:
	f1: {'f1': 0.9451645064805583}
	accuracy: {'accuracy': 0.93125}

(1, 240) Loss: 0.21584155174787156
(1, 480) Loss: 0.15006893206737004
(1, 720) Loss: 0.11780856651603244
(1, 960) Loss: 0.03096656571433414
(1, 1200) Loss: 0.12248829165182543
(1, 1440) Loss: 0.04359709356504027
(1, 1680) Loss: 0.2841085478430614
(1, 1920) Loss: 0.09523851121775806
(1, 2160) Loss: 0.13024401921138634
(1, 2400) Loss: 0.024331460740359037
(1, 2640) Loss: 0.15801718503062148
(1, 2880) Loss: 0.05275750903238077
(1, 3120) Loss: 0.024497363550472075
(1, 3360) Loss: 0.2762179555109469
(1, 3600) Loss: 0.04853209253633395
(1, 3840) Loss: 0.07117391510982998
(1, 4080) Loss: 0.11150500849471429
(1, 4320) Loss: 0.20059660185215764
(1, 4560) Loss: 0.05604588543210412
(1, 4800) Loss: 0.24103509136184587
(1, 5040) Loss: 0.1841309119714424
(1, 5280) Loss: 0.04805188132449985
(1, 5520) Loss: 0.12254669455578551
(1, 5760) Loss: 0.048853697182494216
(1, 6000) Loss: 0.19695669158827514
(1, 6240) Loss: 0.025208402413409204
Training metrics:
	f1: {'f1': 0.9709742554265524}
	accuracy: {'accuracy': 0.9640568838881075}

Evaluation metrics:
	f1: {'f1': 0.9327817993795243}
	accuracy: {'accuracy': 0.91875}

(2, 240) Loss: 0.12969164417445428
(2, 480) Loss: 0.261120608386409
(2, 720) Loss: 0.04913895075296751
(2, 960) Loss: 0.011409187382378151
(2, 1200) Loss: 0.0045012687536655
(2, 1440) Loss: 0.013992290252645034
(2, 1680) Loss: 0.00341855468286667
(2, 1920) Loss: 0.005158804604434408
(2, 2160) Loss: 0.0024096976339933462
(2, 2400) Loss: 0.0035496560856699945
(2, 2640) Loss: 0.004412249727465678
(2, 2880) Loss: 0.007120953373669182
(2, 3120) Loss: 0.004007303081016289
(2, 3360) Loss: 0.01312715403691982
(2, 3600) Loss: 0.000746732307743514
(2, 3840) Loss: 0.08186064042383806
(2, 4080) Loss: 0.04790576052982942
(2, 4320) Loss: 0.03497356519728783
(2, 4560) Loss: 0.029989610839402304
(2, 4800) Loss: 0.15005714289800381
(2, 5040) Loss: 0.26653119625407273
(2, 5280) Loss: 0.02252500588365365
(2, 5520) Loss: 0.012835303324391135
(2, 5760) Loss: 0.011705063686531503
(2, 6000) Loss: 0.08645750317664352
(2, 6240) Loss: 0.014051930527784862
Training metrics:
	f1: {'f1': 0.9903724347605777}
	accuracy: {'accuracy': 0.9881231442412877}

Evaluation metrics:
	f1: {'f1': 0.9350393700787402}
	accuracy: {'accuracy': 0.9175}

(3, 240) Loss: 0.26742669704253785
(3, 480) Loss: 0.0328299623914063
(3, 720) Loss: 0.03601195638475474
(3, 960) Loss: 0.02230437458929373
(3, 1200) Loss: 0.022668923123274
(3, 1440) Loss: 0.001092950739985099
(3, 1680) Loss: 0.007561193692527013
(3, 1920) Loss: 0.00061488895007642
(3, 2160) Loss: 0.000729626113025006
(3, 2400) Loss: 0.07071280013187789
(3, 2640) Loss: 0.02306013719935436
(3, 2880) Loss: 0.0034258935804245996
(3, 3120) Loss: 0.2858994624548359
(3, 3360) Loss: 0.36202936572299227
(3, 3600) Loss: 0.11909360283752904
(3, 3840) Loss: 0.012876014386711177
(3, 4080) Loss: 0.00810527512076078
(3, 4320) Loss: 0.00924530247139046
(3, 4560) Loss: 0.024061472274479457
(3, 4800) Loss: 0.005673472540365765
(3, 5040) Loss: 0.1484895601977769
(3, 5280) Loss: 0.008770237168209861
(3, 5520) Loss: 0.014970263854775113
(3, 5760) Loss: 0.0335513714409899
(3, 6000) Loss: 0.0165566447089077
(3, 6240) Loss: 0.03713029189602821
Training metrics:
	f1: {'f1': 0.9908860759493671}
	accuracy: {'accuracy': 0.9887482419127989}

Evaluation metrics:
	f1: {'f1': 0.942942942942943}
	accuracy: {'accuracy': 0.92875}


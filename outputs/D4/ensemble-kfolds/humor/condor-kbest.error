Using the GPU:Tesla M10
	(epoch 2, fold 1, samples 1600) FClassifier Loss: 0.10430163204000564 Transformer Loss: 0.12576245594027569
	(epoch 2, fold 1, samples 1920) FClassifier Loss: 0.43994423382537207 Transformer Loss: 0.08466939894424286
	(epoch 2, fold 1, samples 2240) FClassifier Loss: 0.16721348860482976 Transformer Loss: 0.09010285514523275
	(epoch 2, fold 1, samples 2560) FClassifier Loss: 0.10458247869974002 Transformer Loss: 0.0881134985411336
	(epoch 2, fold 1, samples 2880) FClassifier Loss: 0.1911489885987976 Transformer Loss: 0.06968980563760852
	(epoch 2, fold 1, samples 3200) FClassifier Loss: 0.17730215180381492 Transformer Loss: 0.02021412291651359
	(epoch 2, fold 1, samples 3520) FClassifier Loss: 0.27809481742770004 Transformer Loss: 0.05061490972730098
	(epoch 2, fold 1, samples 3840) FClassifier Loss: 0.18078212374894065 Transformer Loss: 0.023780230860211304
	(epoch 2, fold 1, samples 4160) FClassifier Loss: 0.3194819147865928 Transformer Loss: 0.027597548654739512
(epoch 2, fold 1, samples 640) Regression Accuracy: 1.0, Loss: 0.012106696143746376
(epoch 2, fold 1, samples 1280) Regression Accuracy: 1.0, Loss: 0.020082827657461166
(epoch 2, fold 1, samples 1920) Regression Accuracy: 1.0, Loss: 0.027625277638435364
Fold 0 accuracies:
Ensemble	f1: {'f1': 0.9484327603640041}
Ensemble	accuracy: {'accuracy': 0.93625}

Transformer	f1: {'f1': 0.947906026557712}
Transformer	accuracy: {'accuracy': 0.93625}

Featurizer	f1: {'f1': 0.8522954091816366}
Featurizer	accuracy: {'accuracy': 0.815}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
	(epoch 2, fold 2, samples 320) FClassifier Loss: 0.14018167299195738 Transformer Loss: 0.014194809678883757
	(epoch 2, fold 2, samples 640) FClassifier Loss: 0.17286121849087976 Transformer Loss: 0.033184839210662176
	(epoch 1, fold 1, samples 320) FClassifier Loss: 0.7125415503978729 Transformer Loss: 0.7445711176842451
	(epoch 2, fold 2, samples 960) FClassifier Loss: 0.07294239921975532 Transformer Loss: 0.0473473765723611
	(epoch 1, fold 1, samples 640) FClassifier Loss: 0.6127242371439934 Transformer Loss: 0.38109304729732685
	(epoch 2, fold 2, samples 1280) FClassifier Loss: 0.14958531122033492 Transformer Loss: 0.014694513358335826
	(epoch 1, fold 1, samples 960) FClassifier Loss: 0.6389461662620306 Transformer Loss: 0.12464740275754593
	(epoch 2, fold 2, samples 1600) FClassifier Loss: 0.0708590131348501 Transformer Loss: 0.0023459842341253534
	(epoch 1, fold 1, samples 1280) FClassifier Loss: 0.6547144930809736 Transformer Loss: 0.2988428373937495
	(epoch 2, fold 2, samples 1920) FClassifier Loss: 0.22651240872619383 Transformer Loss: 0.48361198760358093
	(epoch 1, fold 1, samples 1600) FClassifier Loss: 0.534093982540071 Transformer Loss: 0.28803283820161596
	(epoch 2, fold 2, samples 2240) FClassifier Loss: 0.17457006018435095 Transformer Loss: 0.14245003986434313
	(epoch 1, fold 1, samples 1920) FClassifier Loss: 0.6458140294998884 Transformer Loss: 0.2876366813434288
	(epoch 2, fold 2, samples 2560) FClassifier Loss: 0.09200003825390013 Transformer Loss: 0.04730823881618562
	(epoch 1, fold 1, samples 2240) FClassifier Loss: 0.6557187736034393 Transformer Loss: 0.27471593755763024
	(epoch 2, fold 2, samples 2880) FClassifier Loss: 0.20132451421841324 Transformer Loss: 0.0038545552852156106
	(epoch 1, fold 1, samples 2560) FClassifier Loss: 0.5805036490783095 Transformer Loss: 0.3302622005576268
	(epoch 2, fold 2, samples 3200) FClassifier Loss: 0.176715770255214 Transformer Loss: 0.008789606446953258
	(epoch 1, fold 1, samples 2880) FClassifier Loss: 0.5863948790356517 Transformer Loss: 0.18675848442944698
	(epoch 2, fold 2, samples 3520) FClassifier Loss: 0.27030574314335354 Transformer Loss: 0.06164563246238686
	(epoch 1, fold 1, samples 3200) FClassifier Loss: 0.4816487424541265 Transformer Loss: 0.07803485794283915
	(epoch 2, fold 2, samples 3840) FClassifier Loss: 0.1956417443616374 Transformer Loss: 0.00936000913316093
	(epoch 1, fold 1, samples 3520) FClassifier Loss: 0.5582668804563582 Transformer Loss: 0.19991689904418308
	(epoch 2, fold 2, samples 4160) FClassifier Loss: 0.2027166080497409 Transformer Loss: 0.0053639705965906614
	(epoch 1, fold 1, samples 3840) FClassifier Loss: 0.4419413321884349 Transformer Loss: 0.21695028856629506
(epoch 2, fold 2, samples 640) Regression Accuracy: 0.96875, Loss: 0.029345814138650894
	(epoch 1, fold 1, samples 4160) FClassifier Loss: 0.5494470658886712 Transformer Loss: 0.4155070103734033
(epoch 2, fold 2, samples 1280) Regression Accuracy: 0.96875, Loss: 0.13060377538204193
(epoch 1, fold 1, samples 640) Regression Accuracy: 0.9375, Loss: 0.24044455587863922
(epoch 2, fold 2, samples 1920) Regression Accuracy: 1.0, Loss: 0.0317513532936573
Fold 1 accuracies:
(epoch 1, fold 1, samples 1280) Regression Accuracy: 0.9375, Loss: 0.23549824953079224
Ensemble	f1: {'f1': 0.9458917835671343}
Ensemble	accuracy: {'accuracy': 0.9325}

Transformer	f1: {'f1': 0.9447236180904522}
Transformer	accuracy: {'accuracy': 0.93125}

Featurizer	f1: {'f1': 0.8548387096774193}
Featurizer	accuracy: {'accuracy': 0.82}

(epoch 1, fold 1, samples 1920) Regression Accuracy: 0.875, Loss: 0.16902124881744385
	(epoch 2, fold 3, samples 320) FClassifier Loss: 0.11448631255780128 Transformer Loss: 0.04127696939940506
Fold 0 accuracies:

Using the GPU:NVIDIA Quadro RTX 8000
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6857610121369362
(0, 384) Loss: 0.696671862155199
(0, 576) Loss: 0.6976103279739618
(0, 768) Loss: 0.6017749160528183
(0, 960) Loss: 0.5573554299771786
(0, 1152) Loss: 0.49241108912974596
(0, 1344) Loss: 0.4638592442497611
(0, 1536) Loss: 0.502568768337369
(0, 1728) Loss: 0.441338405944407
(0, 1920) Loss: 0.43238964676856995
(0, 2112) Loss: 0.4772939207032323
(0, 2304) Loss: 0.45411624759435654
(0, 2496) Loss: 0.44689532648772
(0, 2688) Loss: 0.40868857968598604
(0, 2880) Loss: 0.5754569144919515
(0, 3072) Loss: 0.617414828389883
(0, 3264) Loss: 0.4553961921483278
(0, 3456) Loss: 0.49665930680930614
(0, 3648) Loss: 0.4409687127918005
(0, 3840) Loss: 0.36224433965981007
(0, 4032) Loss: 0.49987812992185354
(0, 4224) Loss: 0.4067773846909404
(0, 4416) Loss: 0.4432921474799514
(0, 4608) Loss: 0.40571851935237646
(0, 4800) Loss: 0.4499926557764411
(0, 4992) Loss: 0.40050273947417736
(0, 5184) Loss: 0.3773203045129776
(0, 5376) Loss: 0.39211265835911036
(0, 5568) Loss: 0.37645670864731073
(0, 5760) Loss: 0.405499299056828
(0, 5952) Loss: 0.36768185067921877
(0, 6144) Loss: 0.3805439891293645
(0, 6336) Loss: 0.37761060800403357
Evaluation metrics:
	f1: {'f1': 0.9196337741607324}
	accuracy: {'accuracy': 0.90125}

(1, 192) Loss: 0.3761262306943536
(1, 384) Loss: 0.3913876935839653
(1, 576) Loss: 0.35473459865897894
(1, 768) Loss: 0.4451412418857217
(1, 960) Loss: 0.3878686623647809
(1, 1152) Loss: 0.4075152827426791
(1, 1344) Loss: 0.3715916173532605
(1, 1536) Loss: 0.407014699652791
(1, 1728) Loss: 0.4069944554939866
(1, 1920) Loss: 0.3154806988313794
(1, 2112) Loss: 0.3828591499477625
(1, 2304) Loss: 0.3741500098258257
(1, 2496) Loss: 0.38947315234690905
(1, 2688) Loss: 0.3899485729634762
(1, 2880) Loss: 0.4045920828357339
(1, 3072) Loss: 0.34495848044753075
(1, 3264) Loss: 0.3181476416066289
(1, 3456) Loss: 0.3540204428136349
(1, 3648) Loss: 0.42092218436300755
(1, 3840) Loss: 0.3744323141872883
(1, 4032) Loss: 0.3510695295408368
(1, 4224) Loss: 0.39833543077111244
(1, 4416) Loss: 0.37152483966201544
(1, 4608) Loss: 0.3638958828523755
(1, 4800) Loss: 0.4316565953195095
(1, 4992) Loss: 0.35062734317034483
(1, 5184) Loss: 0.38937554229050875
(1, 5376) Loss: 0.3450838616117835
(1, 5568) Loss: 0.35054610297083855
(1, 5760) Loss: 0.39782458916306496
(1, 5952) Loss: 0.31551904045045376
(1, 6144) Loss: 0.3517361590638757
(1, 6336) Loss: 0.4142374284565449
Evaluation metrics:
	f1: {'f1': 0.9325513196480938}
	accuracy: {'accuracy': 0.91375}

(2, 192) Loss: 0.34456394985318184
(2, 384) Loss: 0.40874364133924246
(2, 576) Loss: 0.37566328793764114
(2, 768) Loss: 0.3574680099263787
(2, 960) Loss: 0.313960456289351
(2, 1152) Loss: 0.40495510399341583
(2, 1344) Loss: 0.3447384173050523
(2, 1536) Loss: 0.37213151808828115
(2, 1728) Loss: 0.37539871875196695
(2, 1920) Loss: 0.318067311309278
(2, 2112) Loss: 0.37662382889539003
(2, 2304) Loss: 0.38937218952924013
(2, 2496) Loss: 0.3290205616503954
(2, 2688) Loss: 0.42995830718427896
(2, 2880) Loss: 0.3141278875991702
(2, 3072) Loss: 0.36630168184638023
(2, 3264) Loss: 0.31343525368720293
(2, 3456) Loss: 0.35217927675694227
(2, 3648) Loss: 0.4690347211435437
(2, 3840) Loss: 0.34372898656874895
(2, 4032) Loss: 0.36283468920737505
(2, 4224) Loss: 0.37550011835992336
(2, 4416) Loss: 0.3902363497763872
(2, 4608) Loss: 0.35507233161479235
(2, 4800) Loss: 0.3467848552390933
(2, 4992) Loss: 0.343975274823606
(2, 5184) Loss: 0.3539364952594042
(2, 5376) Loss: 0.3322537811473012
(2, 5568) Loss: 0.36033507250249386
(2, 5760) Loss: 0.3714689565822482
(2, 5952) Loss: 0.3144161952659488
(2, 6144) Loss: 0.3456037547439337
(2, 6336) Loss: 0.37570419162511826
Evaluation metrics:
	f1: {'f1': 0.9312820512820512}
	accuracy: {'accuracy': 0.91625}

(3, 192) Loss: 0.34031648840755224
(3, 384) Loss: 0.39336142409592867
(3, 576) Loss: 0.31653985287994146
(3, 768) Loss: 0.3655900815501809
(3, 960) Loss: 0.3134481096640229
(3, 1152) Loss: 0.3462771475315094
(3, 1344) Loss: 0.3446543160825968
(3, 1536) Loss: 0.40412250999361277
(3, 1728) Loss: 0.3751992806792259
(3, 1920) Loss: 0.31332422979176044
(3, 2112) Loss: 0.34196559712290764
(3, 2304) Loss: 0.34712978824973106
(3, 2496) Loss: 0.3738228129222989
(3, 2688) Loss: 0.3757166424766183
(3, 2880) Loss: 0.3441473599523306
(3, 3072) Loss: 0.34474214259535074
(3, 3264) Loss: 0.3139260560274124
(3, 3456) Loss: 0.3790872683748603
(3, 3648) Loss: 0.34498616494238377
(3, 3840) Loss: 0.354734699241817
(3, 4032) Loss: 0.34626931697130203
(3, 4224) Loss: 0.37559577357023954
(3, 4416) Loss: 0.345994614996016
(3, 4608) Loss: 0.31537084840238094
(3, 4800) Loss: 0.3794194394722581
(3, 4992) Loss: 0.34298615902662277
(3, 5184) Loss: 0.3136435141786933
(3, 5376) Loss: 0.3514413721859455
(3, 5568) Loss: 0.3446285258978605
(3, 5760) Loss: 0.34467228408902884
(3, 5952) Loss: 0.32024567667394876
(3, 6144) Loss: 0.31356554105877876
(3, 6336) Loss: 0.35358624905347824
Evaluation metrics:
	f1: {'f1': 0.950253807106599}
	accuracy: {'accuracy': 0.93875}

Saving model to src/models/neural_ensemble/neural_ensemble/humor/...

real	69m35.065s
user	67m38.693s
sys	1m5.960s

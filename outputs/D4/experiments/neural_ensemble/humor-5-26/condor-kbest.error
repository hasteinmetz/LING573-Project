Using the GPU:NVIDIA Quadro RTX 8000
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 240) Loss: 0.6602816313505173
(0, 480) Loss: 0.6294485382735729
(0, 720) Loss: 0.5068808861076832
(0, 960) Loss: 0.43606652542948726
(0, 1200) Loss: 0.5392043866217137
(0, 1440) Loss: 0.3229985646903515
(0, 1680) Loss: 0.5263608112931252
(0, 1920) Loss: 0.4642196461558342
(0, 2160) Loss: 0.5113436661660672
(0, 2400) Loss: 0.33925750702619556
(0, 2640) Loss: 0.3536007605493069
(0, 2880) Loss: 0.4653466120362282
(0, 3120) Loss: 0.3881653591990471
(0, 3360) Loss: 0.4533634774386883
(0, 3600) Loss: 0.37428432628512387
(0, 3840) Loss: 0.38313567638397217
(0, 4080) Loss: 0.4115919150412083
(0, 4320) Loss: 0.39720606282353405
(0, 4560) Loss: 0.4127957008779049
(0, 4800) Loss: 0.489709010720253
(0, 5040) Loss: 0.46320582851767544
(0, 5280) Loss: 0.38875966891646385
(0, 5520) Loss: 0.36592841893434525
(0, 5760) Loss: 0.5188350804150105
(0, 6000) Loss: 0.43765048682689667
(0, 6240) Loss: 0.3476731434464455
Evaluation metrics:
	f1: {'f1': 0.8169014084507042}
	accuracy: {'accuracy': 0.805}

(1, 240) Loss: 0.41677232757210736
(1, 480) Loss: 0.44186244383454326
(1, 720) Loss: 0.40241432860493664
(1, 960) Loss: 0.3719315074384213
(1, 1200) Loss: 0.3727931872010231
(1, 1440) Loss: 0.36428628414869313
(1, 1680) Loss: 0.4865592308342457
(1, 1920) Loss: 0.36540982425212865
(1, 2160) Loss: 0.4387792646884918
(1, 2400) Loss: 0.33831909447908404
(1, 2640) Loss: 0.36902076154947283
(1, 2880) Loss: 0.4326721057295799
(1, 3120) Loss: 0.389956983178854
(1, 3360) Loss: 0.41025732755661015
(1, 3600) Loss: 0.3816835165023804
(1, 3840) Loss: 0.31788389980793
(1, 4080) Loss: 0.41022043377161027
(1, 4320) Loss: 0.3900354489684105
(1, 4560) Loss: 0.3891199789941311
(1, 4800) Loss: 0.4264862611889839
(1, 5040) Loss: 0.42885415628552437
(1, 5280) Loss: 0.3704085782170296
(1, 5520) Loss: 0.4245190039277077
(1, 5760) Loss: 0.42710076048970225
(1, 6000) Loss: 0.3634413480758667
(1, 6240) Loss: 0.33590303212404254
Evaluation metrics:
	f1: {'f1': 0.9237199582027169}
	accuracy: {'accuracy': 0.90875}

(2, 240) Loss: 0.40753936246037487
(2, 480) Loss: 0.42449951246380807
(2, 720) Loss: 0.3286070726811886
(2, 960) Loss: 0.3398572541773319
(2, 1200) Loss: 0.35637348145246506
(2, 1440) Loss: 0.36284757778048515
(2, 1680) Loss: 0.42023096680641175
(2, 1920) Loss: 0.3872683346271515
(2, 2160) Loss: 0.33818217888474467
(2, 2400) Loss: 0.3542619109153748
(2, 2640) Loss: 0.3632417649030686
(2, 2880) Loss: 0.3313974462449551
(2, 3120) Loss: 0.3185140833258629
(2, 3360) Loss: 0.3927455343306065
(2, 3600) Loss: 0.31483966633677485
(2, 3840) Loss: 0.35963821187615397
(2, 4080) Loss: 0.34755392223596576
(2, 4320) Loss: 0.40478136762976646
(2, 4560) Loss: 0.3254846930503845
(2, 4800) Loss: 0.3631887421011925
(2, 5040) Loss: 0.3899924293160439
(2, 5280) Loss: 0.3382802210748196
(2, 5520) Loss: 0.34999565556645396
(2, 5760) Loss: 0.3884838238358498
(2, 6000) Loss: 0.3403809316456318
(2, 6240) Loss: 0.41542242243885996
Evaluation metrics:
	f1: {'f1': 0.9326530612244897}
	accuracy: {'accuracy': 0.9175}

Saving model to src/models/neural_ensemble/neural_ensemble/humor/...

real	54m4.059s
user	51m48.098s
sys	0m48.870s

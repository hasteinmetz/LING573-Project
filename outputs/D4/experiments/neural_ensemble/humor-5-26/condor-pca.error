Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 240) Loss: 0.6343903832137585
(0, 480) Loss: 0.5633419558405877
(0, 720) Loss: 0.46759486198425293
(0, 960) Loss: 0.45593976601958275
(0, 1200) Loss: 0.47884856164455414
(0, 1440) Loss: 0.3814602091908455
(0, 1680) Loss: 0.4532197400927544
(0, 1920) Loss: 0.3168488644063473
(0, 2160) Loss: 0.3978607162833214
(0, 2400) Loss: 0.43366129100322726
(0, 2640) Loss: 0.3891351841390133
(0, 2880) Loss: 0.38640529885888103
(0, 3120) Loss: 0.3776478081941605
(0, 3360) Loss: 0.42990799918770795
(0, 3600) Loss: 0.3833872050046921
(0, 3840) Loss: 0.3834929741919041
(0, 4080) Loss: 0.4203114271163941
(0, 4320) Loss: 0.3760178431868553
(0, 4560) Loss: 0.43011409789323807
(0, 4800) Loss: 0.4658083327114582
(0, 5040) Loss: 0.4376539267599583
(0, 5280) Loss: 0.3926448702812195
(0, 5520) Loss: 0.3744760848581791
(0, 5760) Loss: 0.3899588987231255
(0, 6000) Loss: 0.36766555011272434
(0, 6240) Loss: 0.37837093770504
Evaluation metrics:
	f1: {'f1': 0.9284253578732107}
	accuracy: {'accuracy': 0.9125}

(1, 240) Loss: 0.42736991792917256
(1, 480) Loss: 0.38863630518317227
(1, 720) Loss: 0.4052782982587815
(1, 960) Loss: 0.41755973920226097
(1, 1200) Loss: 0.3630307719111443
(1, 1440) Loss: 0.3728025726974011
(1, 1680) Loss: 0.4366891406476498
(1, 1920) Loss: 0.3143184185028076
(1, 2160) Loss: 0.31544069051742557
(1, 2400) Loss: 0.3497391238808632
(1, 2640) Loss: 0.3563759990036488
(1, 2880) Loss: 0.3646254636347294
(1, 3120) Loss: 0.3633096061646939
(1, 3360) Loss: 0.4884073033928871
(1, 3600) Loss: 0.42772793844342233
(1, 3840) Loss: 0.36329491734504704
(1, 4080) Loss: 0.4634794369339943
(1, 4320) Loss: 0.4353452160954476
(1, 4560) Loss: 0.3906137429177761
(1, 4800) Loss: 0.3600640743970871
(1, 5040) Loss: 0.45047349706292156
(1, 5280) Loss: 0.4149143390357495
(1, 5520) Loss: 0.46224212870001796
(1, 5760) Loss: 0.4337562516331673
(1, 6000) Loss: 0.3636749438941479
(1, 6240) Loss: 0.41948944181203845
Evaluation metrics:
	f1: {'f1': 0.9312377210216111}
	accuracy: {'accuracy': 0.9125}

(2, 240) Loss: 0.37512493655085566
(2, 480) Loss: 0.41202497705817226
(2, 720) Loss: 0.3681882865726948
(2, 960) Loss: 0.3825494945049286
(2, 1200) Loss: 0.36845378503203396
(2, 1440) Loss: 0.37454168274998667
(2, 1680) Loss: 0.4405009500682354
(2, 1920) Loss: 0.3632810957729817
(2, 2160) Loss: 0.3633088864386082
(2, 2400) Loss: 0.33836087062954906
(2, 2640) Loss: 0.3386347942054272
(2, 2880) Loss: 0.33883118927478795
(2, 3120) Loss: 0.3989257924258709
(2, 3360) Loss: 0.4270375765860081
(2, 3600) Loss: 0.32285992056131363
(2, 3840) Loss: 0.34247638657689095
(2, 4080) Loss: 0.4193922564387322
(2, 4320) Loss: 0.36329058408737186
(2, 4560) Loss: 0.341207204759121
(2, 4800) Loss: 0.37607287466526035
(2, 5040) Loss: 0.43792185336351397
(2, 5280) Loss: 0.31359832584857944
(2, 5520) Loss: 0.3600133515894413
(2, 5760) Loss: 0.4455679915845394
(2, 6000) Loss: 0.43635951429605485
(2, 6240) Loss: 0.37294271737337115
Evaluation metrics:
	f1: {'f1': 0.9156398104265402}
	accuracy: {'accuracy': 0.88875}

Saving model to src/models/neural_ensemble/neural_ensemble/humor/...

real	86m1.881s
user	78m58.273s
sys	5m56.001s

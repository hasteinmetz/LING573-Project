Using cpu device
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.7352006891742349
(0, 384) Loss: 0.690955301746726
(0, 576) Loss: 0.7044633273035288
(0, 768) Loss: 0.6940968930721283
(0, 960) Loss: 0.7122148871421814
(0, 1152) Loss: 0.6941824648529291
(0, 1344) Loss: 0.7015393432229757
(0, 1536) Loss: 0.7061490677297115
(0, 1728) Loss: 0.6919947694987059
(0, 1920) Loss: 0.690020602196455
(0, 2112) Loss: 0.6912942193448544
(0, 2304) Loss: 0.6946272123605013
(0, 2496) Loss: 0.6991174016147852
(0, 2688) Loss: 0.6885348875075579
(0, 2880) Loss: 0.6923825740814209
(0, 3072) Loss: 0.6892406046390533
(0, 3264) Loss: 0.6895385384559631
(0, 3456) Loss: 0.70774863101542
(0, 3648) Loss: 0.694006809964776
(0, 3840) Loss: 0.7042759843170643
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(1, 192) Loss: 0.6758779771625996
(1, 384) Loss: 0.6846526507288218
(1, 576) Loss: 0.6997025460004807
(1, 768) Loss: 0.6919706501066685
(1, 960) Loss: 0.6870849300175905
(1, 1152) Loss: 0.6912206839770079
(1, 1344) Loss: 0.6950046699494123
(1, 1536) Loss: 0.7003705129027367
(1, 1728) Loss: 0.6927606705576181
(1, 1920) Loss: 0.6915589962154627
(1, 2112) Loss: 0.6876786798238754
(1, 2304) Loss: 0.6944535225629807
(1, 2496) Loss: 0.6905882433056831
(1, 2688) Loss: 0.6932182740420103
(1, 2880) Loss: 0.6955127157270908
(1, 3072) Loss: 0.6921181175857782
(1, 3264) Loss: 0.6912930235266685
(1, 3456) Loss: 0.6936797574162483
(1, 3648) Loss: 0.6936674769967794
(1, 3840) Loss: 0.6924493778496981
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(2, 192) Loss: 0.6832246873527765
(2, 384) Loss: 0.6843767836689949
(2, 576) Loss: 0.7028485704213381
(2, 768) Loss: 0.6967048533260822
(2, 960) Loss: 0.6994995400309563
(2, 1152) Loss: 0.6924805324524641
(2, 1344) Loss: 0.6939389891922474
(2, 1536) Loss: 0.7044807225465775
(2, 1728) Loss: 0.6914189010858536
(2, 1920) Loss: 0.693630576133728
(2, 2112) Loss: 0.691642589867115
(2, 2304) Loss: 0.693312756717205
(2, 2496) Loss: 0.6915657836943865
(2, 2688) Loss: 0.693227855488658
(2, 2880) Loss: 0.6971575915813446
(2, 3072) Loss: 0.6949539836496115
(2, 3264) Loss: 0.692782225087285
(2, 3456) Loss: 0.6933282222598791
(2, 3648) Loss: 0.6934124026447535
(2, 3840) Loss: 0.6925794333219528
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(3, 192) Loss: 0.686664653941989
(3, 384) Loss: 0.6872391067445278
(3, 576) Loss: 0.6993906758725643
(3, 768) Loss: 0.6954461671411991
(3, 960) Loss: 0.6974055636674166
(3, 1152) Loss: 0.6917875576764345
(3, 1344) Loss: 0.6949359104037285
(3, 1536) Loss: 0.7058041077107191
(3, 1728) Loss: 0.691802816465497
(3, 1920) Loss: 0.6923422049731016
(3, 2112) Loss: 0.6897214986383915
(3, 2304) Loss: 0.6936423629522324
(3, 2496) Loss: 0.6905402150005102
(3, 2688) Loss: 0.6932921651750803
(3, 2880) Loss: 0.6977991480380297
(3, 3072) Loss: 0.6948673259466887
(3, 3264) Loss: 0.6924612317234278
(3, 3456) Loss: 0.6934421602636576
(3, 3648) Loss: 0.6934688221663237
(3, 3840) Loss: 0.6926924716681242
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

Saving model to src/models/neural_ensemble/neural_ensemble/controversy/...

real	274m43.618s
user	272m58.361s
sys	0m40.549s

Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.7215746715664864
(0, 384) Loss: 0.6807190198451281
(0, 576) Loss: 0.6939679253846407
(0, 768) Loss: 0.6844276655465364
(0, 960) Loss: 0.6919972766190767
(0, 1152) Loss: 0.6850973907858133
(0, 1344) Loss: 0.6949213799089193
(0, 1536) Loss: 0.7026097699999809
(0, 1728) Loss: 0.6928846649825573
(0, 1920) Loss: 0.6934792660176754
(0, 2112) Loss: 0.6850914880633354
(0, 2304) Loss: 0.6923957224935293
(0, 2496) Loss: 0.7039374429732561
(0, 2688) Loss: 0.6962484568357468
(0, 2880) Loss: 0.6996517106890678
(0, 3072) Loss: 0.6858993601053953
(0, 3264) Loss: 0.6947593465447426
(0, 3456) Loss: 0.6905651558190584
(0, 3648) Loss: 0.6935055535286665
(0, 3840) Loss: 0.6951854582875967
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(1, 192) Loss: 0.6809102315455675
(1, 384) Loss: 0.6881716921925545
(1, 576) Loss: 0.6991035137325525
(1, 768) Loss: 0.6932211089879274
(1, 960) Loss: 0.6893037315458059
(1, 1152) Loss: 0.6911854278296232
(1, 1344) Loss: 0.6956379059702158
(1, 1536) Loss: 0.703859182074666
(1, 1728) Loss: 0.6921117920428514
(1, 1920) Loss: 0.6920353043824434
(1, 2112) Loss: 0.6885300427675247
(1, 2304) Loss: 0.6941522676497698
(1, 2496) Loss: 0.6908007003366947
(1, 2688) Loss: 0.6932307295501232
(1, 2880) Loss: 0.6959047447890043
(1, 3072) Loss: 0.6925575118511915
(1, 3264) Loss: 0.6914573218673468
(1, 3456) Loss: 0.693673999980092
(1, 3648) Loss: 0.6936309617012739
(1, 3840) Loss: 0.6924878936260939
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(2, 192) Loss: 0.684658182784915
(2, 384) Loss: 0.6854019537568092
(2, 576) Loss: 0.701995737850666
(2, 768) Loss: 0.6955412290990353
(2, 960) Loss: 0.6947313919663429
(2, 1152) Loss: 0.6911840289831161
(2, 1344) Loss: 0.6961034443229437
(2, 1536) Loss: 0.7039978392422199
(2, 1728) Loss: 0.6923277974128723
(2, 1920) Loss: 0.6920255739241838
(2, 2112) Loss: 0.6891471613198519
(2, 2304) Loss: 0.6938518192619085
(2, 2496) Loss: 0.6905416119843721
(2, 2688) Loss: 0.693295681849122
(2, 2880) Loss: 0.6972774621099234
(2, 3072) Loss: 0.6938155703246593
(2, 3264) Loss: 0.691834319382906
(2, 3456) Loss: 0.6937239095568657
(2, 3648) Loss: 0.6935907267034054
(2, 3840) Loss: 0.6928044687956572
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(3, 192) Loss: 0.6854538265615702
(3, 384) Loss: 0.6867991089820862
(3, 576) Loss: 0.6998641211539507
(3, 768) Loss: 0.6949921455234289
(3, 960) Loss: 0.6946165077388287
(3, 1152) Loss: 0.691161721944809
(3, 1344) Loss: 0.6959773115813732
(3, 1536) Loss: 0.7038432713598013
(3, 1728) Loss: 0.6924389842897654
(3, 1920) Loss: 0.6917705088853836
(3, 2112) Loss: 0.6886002719402313
(3, 2304) Loss: 0.6940516121685505
(3, 2496) Loss: 0.6896313447505236
(3, 2688) Loss: 0.6934003587812185
(3, 2880) Loss: 0.6988061685115099
(3, 3072) Loss: 0.6955987010151148
(3, 3264) Loss: 0.692659541964531
(3, 3456) Loss: 0.6933963224291801
(3, 3648) Loss: 0.6934098415076733
(3, 3840) Loss: 0.6926029324531555
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

Saving model to src/models/neural_ensemble/neural_ensemble/controversy/...

real	60m58.251s
user	55m14.220s
sys	4m42.280s

Using the GPU:NVIDIA Quadro RTX 8000
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
	(epoch 1, fold 1, samples 320) FClassifier Loss: 0.6927139591425657 Transformer Loss: 0.6864084595581517
	(epoch 1, fold 1, samples 640) FClassifier Loss: 0.6977766938507557 Transformer Loss: 0.6983518737761187
	(epoch 1, fold 1, samples 960) FClassifier Loss: 0.7008317466825247 Transformer Loss: 0.6900550115024089
	(epoch 1, fold 1, samples 1280) FClassifier Loss: 0.6903211139142513 Transformer Loss: 0.6954008966567926
	(epoch 1, fold 1, samples 1600) FClassifier Loss: 0.6957683246582747 Transformer Loss: 0.6971566624306433
	(epoch 1, fold 1, samples 1920) FClassifier Loss: 0.6891365628689528 Transformer Loss: 0.6864408898400143
	(epoch 1, fold 1, samples 2240) FClassifier Loss: 0.6976616960018873 Transformer Loss: 0.700366638822743
	(epoch 1, fold 1, samples 2560) FClassifier Loss: 0.6978594660758972 Transformer Loss: 0.681190230530774
	(epoch 1, fold 1, samples 2880) FClassifier Loss: 0.6912144459784031 Transformer Loss: 0.696747257785546
(epoch 1, fold 1, samples 640) Regression Accuracy: 0.40625, Loss: 0.7453829646110535
Fold 0 accuracies:
Ensemble	f1: {'f1': 0.0}
Ensemble	accuracy: {'accuracy': 0.4969574036511156}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.18241042345276876}
Featurizer	accuracy: {'accuracy': 0.4908722109533469}

	(epoch 1, fold 2, samples 320) FClassifier Loss: 0.6777730323374271 Transformer Loss: 0.6865822041872889
	(epoch 1, fold 2, samples 640) FClassifier Loss: 0.6937986984848976 Transformer Loss: 0.7030432626852416
	(epoch 1, fold 2, samples 960) FClassifier Loss: 0.6987128667533398 Transformer Loss: 0.6982587006641552
	(epoch 1, fold 2, samples 1280) FClassifier Loss: 0.669091708958149 Transformer Loss: 0.6987765041703824
	(epoch 1, fold 2, samples 1600) FClassifier Loss: 0.6861039362847805 Transformer Loss: 0.6946450304330938
	(epoch 1, fold 2, samples 1920) FClassifier Loss: 0.6960901133716106 Transformer Loss: 0.6926076838572044
	(epoch 1, fold 2, samples 2240) FClassifier Loss: 0.690049821510911 Transformer Loss: 0.6921903310030757
	(epoch 1, fold 2, samples 2560) FClassifier Loss: 0.6863788440823555 Transformer Loss: 0.6897403840466723
	(epoch 1, fold 2, samples 2880) FClassifier Loss: 0.6758800223469734 Transformer Loss: 0.689369852116215
(epoch 1, fold 2, samples 640) Regression Accuracy: 0.5, Loss: 0.7121356129646301
Fold 1 accuracies:
Ensemble	f1: {'f1': 0.0}
Ensemble	accuracy: {'accuracy': 0.4969574036511156}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.3636363636363636}
Featurizer	accuracy: {'accuracy': 0.5172413793103449}

	(epoch 1, fold 3, samples 320) FClassifier Loss: 0.6801379323005676 Transformer Loss: 0.6929396439190896
	(epoch 1, fold 3, samples 640) FClassifier Loss: 0.6947406511753798 Transformer Loss: 0.7039820836216677
	(epoch 1, fold 3, samples 960) FClassifier Loss: 0.6834536530077457 Transformer Loss: 0.7061109426431358
	(epoch 1, fold 3, samples 1280) FClassifier Loss: 0.6842657625675201 Transformer Loss: 0.6952400094851328
	(epoch 1, fold 3, samples 1600) FClassifier Loss: 0.6679988894611597 Transformer Loss: 0.691152505329228
	(epoch 1, fold 3, samples 1920) FClassifier Loss: 0.6937686875462532 Transformer Loss: 0.6921024966322875
	(epoch 1, fold 3, samples 2240) FClassifier Loss: 0.6736600883305073 Transformer Loss: 0.6880438174775918
	(epoch 1, fold 3, samples 2560) FClassifier Loss: 0.6496681105345488 Transformer Loss: 0.6937837920559105
	(epoch 1, fold 3, samples 2880) FClassifier Loss: 0.6575736440718174 Transformer Loss: 0.6924579901074139
(epoch 1, fold 3, samples 640) Regression Accuracy: 0.5625, Loss: 0.6963289976119995
Fold 2 accuracies:
Ensemble	f1: {'f1': 0.0}
Ensemble	accuracy: {'accuracy': 0.4969574036511156}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.4213075060532688}
Featurizer	accuracy: {'accuracy': 0.5152129817444219}

	(epoch 1, fold 4, samples 320) FClassifier Loss: 0.6529465075582266 Transformer Loss: 0.6961978718463797
	(epoch 1, fold 4, samples 640) FClassifier Loss: 0.6794203668832779 Transformer Loss: 0.7082498207164463
	(epoch 1, fold 4, samples 960) FClassifier Loss: 0.6422716975212097 Transformer Loss: 0.6929952050459178
	(epoch 1, fold 4, samples 1280) FClassifier Loss: 0.6558307688683271 Transformer Loss: 0.6936065066693118
	(epoch 1, fold 4, samples 1600) FClassifier Loss: 0.6096989624202251 Transformer Loss: 0.6774706193536986
	(epoch 1, fold 4, samples 1920) FClassifier Loss: 0.6481463145464659 Transformer Loss: 0.6951055813487983
	(epoch 1, fold 4, samples 2240) FClassifier Loss: 0.6593071119859815 Transformer Loss: 0.6940885284348042
	(epoch 1, fold 4, samples 2560) FClassifier Loss: 0.6071875197812915 Transformer Loss: 0.6976476936215477
	(epoch 1, fold 4, samples 2880) FClassifier Loss: 0.6158410515636206 Transformer Loss: 0.6935134057707728
(epoch 1, fold 4, samples 640) Regression Accuracy: 0.5625, Loss: 0.7213487029075623
Fold 3 accuracies:
Ensemble	f1: {'f1': 0.0}
Ensemble	accuracy: {'accuracy': 0.4969574036511156}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.4366197183098592}
Featurizer	accuracy: {'accuracy': 0.513184584178499}

	(epoch 1, fold 5, samples 320) FClassifier Loss: 0.5729895737022161 Transformer Loss: 0.6913486050564188
	(epoch 1, fold 5, samples 640) FClassifier Loss: 0.6328032435849309 Transformer Loss: 0.693680053591379
	(epoch 1, fold 5, samples 960) FClassifier Loss: 0.5730704311281443 Transformer Loss: 0.6960095027977786
	(epoch 1, fold 5, samples 1280) FClassifier Loss: 0.5589972389861941 Transformer Loss: 0.692744687472441
	(epoch 1, fold 5, samples 1600) FClassifier Loss: 0.6036446089856327 Transformer Loss: 0.685620877360634
	(epoch 1, fold 5, samples 1920) FClassifier Loss: 0.6155330268666148 Transformer Loss: 0.6930841642652013
	(epoch 1, fold 5, samples 2240) FClassifier Loss: 0.6639444180764258 Transformer Loss: 0.6999654318206012
	(epoch 1, fold 5, samples 2560) FClassifier Loss: 0.5221115085296333 Transformer Loss: 0.6905959030336817
	(epoch 1, fold 5, samples 2880) FClassifier Loss: 0.5273238010704517 Transformer Loss: 0.7031829508596275
(epoch 1, fold 5, samples 640) Regression Accuracy: 0.46875, Loss: 0.762122631072998
Fold 4 accuracies:
Ensemble	f1: {'f1': 0.015873015873015872}
Ensemble	accuracy: {'accuracy': 0.4969574036511156}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.4392523364485982}
Featurizer	accuracy: {'accuracy': 0.513184584178499}

Epoch 0 accuracies:
Ensemble	f1: {'f1': 0.015873015873015872}
Ensemble	accuracy: {'accuracy': 0.4969574036511156}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.4392523364485982}
Featurizer	accuracy: {'accuracy': 0.513184584178499}


real	45m20.520s
user	42m1.865s
sys	1m46.453s

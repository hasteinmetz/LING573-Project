Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
	(epoch 1, fold 1, samples 320) FClassifier Loss: 0.6999889016151428 Transformer Loss: 0.705191433429718
	(epoch 1, fold 1, samples 640) FClassifier Loss: 0.704495906829834 Transformer Loss: 0.7095251679420471
	(epoch 1, fold 1, samples 960) FClassifier Loss: 0.6942493915557861 Transformer Loss: 0.7126550078392029
	(epoch 1, fold 1, samples 1280) FClassifier Loss: 0.6967971324920654 Transformer Loss: 0.686244785785675
	(epoch 1, fold 1, samples 1600) FClassifier Loss: 0.6914573311805725 Transformer Loss: 0.7063192129135132
	(epoch 1, fold 1, samples 1920) FClassifier Loss: 0.6914496421813965 Transformer Loss: 0.7001178860664368
	(epoch 1, fold 1, samples 2240) FClassifier Loss: 0.6972723603248596 Transformer Loss: 0.6844145059585571
	(epoch 1, fold 1, samples 2560) FClassifier Loss: 0.6956533193588257 Transformer Loss: 0.6961047649383545
(epoch 1, fold 1, samples 640) Regression Accuracy: 0.59375, Loss: 0.6891213059425354
(epoch 1, fold 1, samples 1280) Regression Accuracy: 0.5625, Loss: 0.6906735301017761
Fold 0 accuracies:
Ensemble	f1: {'f1': 0.6693657219973009}
Ensemble	accuracy: {'accuracy': 0.5030425963488844}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.023622047244094488}
Featurizer	accuracy: {'accuracy': 0.4969574036511156}

	(epoch 1, fold 2, samples 320) FClassifier Loss: 0.6847150921821594 Transformer Loss: 0.67423415184021
	(epoch 1, fold 2, samples 640) FClassifier Loss: 0.7037764191627502 Transformer Loss: 0.7064076066017151
	(epoch 1, fold 2, samples 960) FClassifier Loss: 0.6915276646614075 Transformer Loss: 0.695279061794281
	(epoch 1, fold 2, samples 1280) FClassifier Loss: 0.6895497441291809 Transformer Loss: 0.6956767439842224
	(epoch 1, fold 2, samples 1600) FClassifier Loss: 0.6907901167869568 Transformer Loss: 0.6993063688278198
	(epoch 1, fold 2, samples 1920) FClassifier Loss: 0.6828536987304688 Transformer Loss: 0.6788747906684875
	(epoch 1, fold 2, samples 2240) FClassifier Loss: 0.6865466237068176 Transformer Loss: 0.6987095475196838
	(epoch 1, fold 2, samples 2560) FClassifier Loss: 0.6797053217887878 Transformer Loss: 0.6997814178466797
(epoch 1, fold 2, samples 640) Regression Accuracy: 0.46875, Loss: 0.696526288986206
(epoch 1, fold 2, samples 1280) Regression Accuracy: 0.5, Loss: 0.6952639818191528
Fold 1 accuracies:
Ensemble	f1: {'f1': 0.6693657219973009}
Ensemble	accuracy: {'accuracy': 0.5030425963488844}

Transformer	f1: {'f1': 0.016}
Transformer	accuracy: {'accuracy': 0.5010141987829615}

Featurizer	f1: {'f1': 0.14532871972318337}
Featurizer	accuracy: {'accuracy': 0.49898580121703856}

	(epoch 1, fold 3, samples 320) FClassifier Loss: 0.6699970364570618 Transformer Loss: 0.6879642605781555
	(epoch 1, fold 3, samples 640) FClassifier Loss: 0.6711376905441284 Transformer Loss: 0.7138389348983765
	(epoch 1, fold 3, samples 960) FClassifier Loss: 0.6698487401008606 Transformer Loss: 0.6985350251197815
	(epoch 1, fold 3, samples 1280) FClassifier Loss: 0.6701356768608093 Transformer Loss: 0.6919686794281006
	(epoch 1, fold 3, samples 1600) FClassifier Loss: 0.6459766030311584 Transformer Loss: 0.647540807723999
	(epoch 1, fold 3, samples 1920) FClassifier Loss: 0.6482563018798828 Transformer Loss: 0.6890194416046143
	(epoch 1, fold 3, samples 2240) FClassifier Loss: 0.6622666120529175 Transformer Loss: 0.6864730715751648
	(epoch 1, fold 3, samples 2560) FClassifier Loss: 0.6260008811950684 Transformer Loss: 0.6951190233230591
(epoch 1, fold 3, samples 640) Regression Accuracy: 0.5625, Loss: 0.6951045989990234
(epoch 1, fold 3, samples 1280) Regression Accuracy: 0.40625, Loss: 0.700836181640625
Fold 2 accuracies:
Ensemble	f1: {'f1': 0.6638655462184874}
Ensemble	accuracy: {'accuracy': 0.513184584178499}

Transformer	f1: {'f1': 0.3188405797101449}
Transformer	accuracy: {'accuracy': 0.5233265720081136}

Featurizer	f1: {'f1': 0.4}
Featurizer	accuracy: {'accuracy': 0.5192697768762677}

	(epoch 2, fold 1, samples 320) FClassifier Loss: 0.6095491647720337 Transformer Loss: 0.7036139965057373
	(epoch 2, fold 1, samples 640) FClassifier Loss: 0.5945082306861877 Transformer Loss: 0.7020713090896606
	(epoch 2, fold 1, samples 960) FClassifier Loss: 0.5694181323051453 Transformer Loss: 0.6353418231010437
	(epoch 2, fold 1, samples 1280) FClassifier Loss: 0.6034690141677856 Transformer Loss: 0.6733182072639465
	(epoch 2, fold 1, samples 1600) FClassifier Loss: 0.5707407593727112 Transformer Loss: 0.6992217898368835
	(epoch 2, fold 1, samples 1920) FClassifier Loss: 0.6205252408981323 Transformer Loss: 0.6849803924560547
	(epoch 2, fold 1, samples 2240) FClassifier Loss: 0.6092746257781982 Transformer Loss: 0.685322105884552
	(epoch 2, fold 1, samples 2560) FClassifier Loss: 0.6014999747276306 Transformer Loss: 0.6893420219421387
(epoch 2, fold 1, samples 640) Regression Accuracy: 0.25, Loss: 0.6976320743560791
(epoch 2, fold 1, samples 1280) Regression Accuracy: 0.46875, Loss: 0.6964820027351379
Fold 0 accuracies:
Ensemble	f1: {'f1': 0.618066561014263}
Ensemble	accuracy: {'accuracy': 0.5111561866125761}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.4513064133016627}
Featurizer	accuracy: {'accuracy': 0.5314401622718052}

	(epoch 2, fold 2, samples 320) FClassifier Loss: 0.5331606864929199 Transformer Loss: 0.6801706552505493
	(epoch 2, fold 2, samples 640) FClassifier Loss: 0.5480409264564514 Transformer Loss: 0.6915522217750549
	(epoch 2, fold 2, samples 960) FClassifier Loss: 0.4255916178226471 Transformer Loss: 0.6768689155578613
	(epoch 2, fold 2, samples 1280) FClassifier Loss: 0.5143543481826782 Transformer Loss: 0.6942936778068542
	(epoch 2, fold 2, samples 1600) FClassifier Loss: 0.4301488995552063 Transformer Loss: 0.724541962146759
	(epoch 2, fold 2, samples 1920) FClassifier Loss: 0.5134437680244446 Transformer Loss: 0.6949983835220337
	(epoch 2, fold 2, samples 2240) FClassifier Loss: 0.5237849354743958 Transformer Loss: 0.6764253973960876
	(epoch 2, fold 2, samples 2560) FClassifier Loss: 0.49477460980415344 Transformer Loss: 0.6904619336128235
(epoch 2, fold 2, samples 640) Regression Accuracy: 0.9375, Loss: 0.6840900182723999
(epoch 2, fold 2, samples 1280) Regression Accuracy: 0.90625, Loss: 0.6678918600082397
Fold 1 accuracies:
Ensemble	f1: {'f1': 0.4377880184331797}
Ensemble	accuracy: {'accuracy': 0.5050709939148073}

Transformer	f1: {'f1': 0.6478405315614618}
Transformer	accuracy: {'accuracy': 0.5699797160243407}

Featurizer	f1: {'f1': 0.4454545454545454}
Featurizer	accuracy: {'accuracy': 0.5050709939148073}

	(epoch 2, fold 3, samples 320) FClassifier Loss: 0.37499356269836426 Transformer Loss: 0.6822192668914795
	(epoch 2, fold 3, samples 640) FClassifier Loss: 0.4438786804676056 Transformer Loss: 0.7074946165084839
	(epoch 2, fold 3, samples 960) FClassifier Loss: 0.37448254227638245 Transformer Loss: 0.6913754343986511
	(epoch 2, fold 3, samples 1280) FClassifier Loss: 0.39336147904396057 Transformer Loss: 0.6847715973854065
	(epoch 2, fold 3, samples 1600) FClassifier Loss: 0.29547372460365295 Transformer Loss: 0.678870439529419
	(epoch 2, fold 3, samples 1920) FClassifier Loss: 0.4256078898906708 Transformer Loss: 0.6966648697853088
	(epoch 2, fold 3, samples 2240) FClassifier Loss: 0.41777607798576355 Transformer Loss: 0.7049553990364075
	(epoch 2, fold 3, samples 2560) FClassifier Loss: 0.33865562081336975 Transformer Loss: 0.6893023252487183
(epoch 2, fold 3, samples 640) Regression Accuracy: 0.84375, Loss: 0.6331460475921631
(epoch 2, fold 3, samples 1280) Regression Accuracy: 0.8125, Loss: 0.6169658899307251
Fold 2 accuracies:
Ensemble	f1: {'f1': 0.4409799554565701}
Ensemble	accuracy: {'accuracy': 0.4908722109533469}

Transformer	f1: {'f1': 0.25846153846153846}
Transformer	accuracy: {'accuracy': 0.5111561866125761}

Featurizer	f1: {'f1': 0.4608695652173913}
Featurizer	accuracy: {'accuracy': 0.4969574036511156}

Saving models to /src/models/nn_ensemble_kfolds/controversy/...

real	84m31.614s
user	73m58.829s
sys	7m48.922s

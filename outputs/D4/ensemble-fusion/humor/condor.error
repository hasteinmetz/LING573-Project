Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6174705447629094
(0, 384) Loss: 0.553400419652462
(0, 576) Loss: 0.4636892139678821
(0, 768) Loss: 0.4278175905928947
(0, 960) Loss: 0.4159667495987378
(0, 1152) Loss: 0.17519247042946517
(0, 1344) Loss: 0.40028835996054113
(0, 1536) Loss: 0.29246012939256616
(0, 1728) Loss: 0.16404362890170887
(0, 1920) Loss: 0.3720514354936313
(0, 2112) Loss: 0.06946714507648721
(0, 2304) Loss: 0.3803744258621009
(0, 2496) Loss: 0.09094276669202372
(0, 2688) Loss: 0.09579367807600647
(0, 2880) Loss: 0.1018728173658019
(0, 3072) Loss: 0.2936460626369808
(0, 3264) Loss: 0.031129801311180927
(0, 3456) Loss: 0.24282249173847958
(0, 3648) Loss: 0.23938151921902318
(0, 3840) Loss: 0.11545403269701637
(0, 4032) Loss: 0.19224282952200156
(0, 4224) Loss: 0.2673168287728913
(0, 4416) Loss: 0.39072311800555326
(0, 4608) Loss: 0.2254149494401645
(0, 4800) Loss: 0.239148638676852
(0, 4992) Loss: 0.16991963170585223
(0, 5184) Loss: 0.33136666117206914
(0, 5376) Loss: 0.13276430561381858
(0, 5568) Loss: 0.0993757680116687
(0, 5760) Loss: 0.313506452657748
(0, 5952) Loss: 0.0362592542078346
(0, 6144) Loss: 0.05623787210060982
(0, 6336) Loss: 0.14606413328147028
f1:
	 {'f1': 0.9190881763527053}
accuracy:
	 {'accuracy': 0.8990467260509455}

	f1: {'f1': 0.9455645161290323}
	accuracy: {'accuracy': 0.9325}

(1, 192) Loss: 0.1833594161325891
(1, 384) Loss: 0.2559192657645326
(1, 576) Loss: 0.10661606268695323
(1, 768) Loss: 0.19987820243113674
(1, 960) Loss: 0.09684703455786803
(1, 1152) Loss: 0.115109008467698
(1, 1344) Loss: 0.0961705979279941
(1, 1536) Loss: 0.10912357514462201
(1, 1728) Loss: 0.1235832939055399
(1, 1920) Loss: 0.1133298995810037
(1, 2112) Loss: 0.19501576718175784
(1, 2304) Loss: 0.12360240765701747
(1, 2496) Loss: 0.05416349224105943
(1, 2688) Loss: 0.14336029831247288
(1, 2880) Loss: 0.020389533245179337
(1, 3072) Loss: 0.044256667602894595
(1, 3264) Loss: 0.015808356418347103
(1, 3456) Loss: 0.05258963171218056
(1, 3648) Loss: 0.05963548996260215
(1, 3840) Loss: 0.1414091027018003
(1, 4032) Loss: 0.037311332884200965
(1, 4224) Loss: 0.0683950096827175
(1, 4416) Loss: 0.17550563127952046
(1, 4608) Loss: 0.017713100402033888
(1, 4800) Loss: 0.12591369888468762
(1, 4992) Loss: 0.15785732096264837
(1, 5184) Loss: 0.03365240825587534
(1, 5376) Loss: 0.028383890386976418
(1, 5568) Loss: 0.07433300426055212
(1, 5760) Loss: 0.17469512189927627
(1, 5952) Loss: 0.0890803811216756
(1, 6144) Loss: 0.07971419762179721
(1, 6336) Loss: 0.040503262700440246
f1:
	 {'f1': 0.9726166328600405}
accuracy:
	 {'accuracy': 0.9662447257383966}

	f1: {'f1': 0.9363369245837414}
	accuracy: {'accuracy': 0.91875}

(2, 192) Loss: 0.0211586783407256
(2, 384) Loss: 0.1508934630164731
(2, 576) Loss: 0.06516786570682598
(2, 768) Loss: 0.033185537347890204
(2, 960) Loss: 0.05331226182897808
(2, 1152) Loss: 0.23005042487602623
(2, 1344) Loss: 0.01578684776541195
(2, 1536) Loss: 0.008265160853625275
(2, 1728) Loss: 0.08841625133936759
(2, 1920) Loss: 0.094937278652651
(2, 2112) Loss: 0.025368820188305108
(2, 2304) Loss: 0.1962671238652547
(2, 2496) Loss: 0.02208360742952209
(2, 2688) Loss: 0.0056460738596797455
(2, 2880) Loss: 0.005087766552605899
(2, 3072) Loss: 0.1579404413650991
(2, 3264) Loss: 0.07267761832554243
(2, 3456) Loss: 0.005109587393235415
(2, 3648) Loss: 0.029785023294607527
(2, 3840) Loss: 0.005134457227541134
(2, 4032) Loss: 0.04739758227879065
(2, 4224) Loss: 0.09938450872323301
(2, 4416) Loss: 0.003983985634476994
(2, 4608) Loss: 0.06539359623275232
(2, 4800) Loss: 0.15710311152724898
(2, 4992) Loss: 0.025361796746437903
(2, 5184) Loss: 0.024856038427969906
(2, 5376) Loss: 0.015648563854483655
(2, 5568) Loss: 0.003244759749577497
(2, 5760) Loss: 0.08102703460281191
(2, 5952) Loss: 0.0027933470737480093
(2, 6144) Loss: 0.0018620989576447755
(2, 6336) Loss: 0.009122792914240563
f1:
	 {'f1': 0.9858084135833756}
accuracy:
	 {'accuracy': 0.9824972651976871}

	f1: {'f1': 0.942713567839196}
	accuracy: {'accuracy': 0.92875}


real	65m18.781s
user	59m6.912s
sys	5m32.248s

Using the GPU:NVIDIA Quadro RTX 8000
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6454769782721996
(0, 384) Loss: 0.6447609029710293
(0, 576) Loss: 0.43706563115119934
(0, 768) Loss: 0.532412925735116
(0, 960) Loss: 0.5039282282814384
(0, 1152) Loss: 0.49010573886334896
(0, 1344) Loss: 0.49825396854430437
(0, 1536) Loss: 0.5010767756029963
(0, 1728) Loss: 0.44464622903615236
(0, 1920) Loss: 0.5035822279751301
(0, 2112) Loss: 0.37702043633908033
(0, 2304) Loss: 0.5329911513254046
(0, 2496) Loss: 0.4376868540421128
(0, 2688) Loss: 0.5192595580592752
(0, 2880) Loss: 0.4737056950107217
(0, 3072) Loss: 0.49948128405958414
(0, 3264) Loss: 0.3460976928472519
(0, 3456) Loss: 0.42712267860770226
(0, 3648) Loss: 0.48249846789985895
(0, 3840) Loss: 0.3143091043457389
(0, 4032) Loss: 0.4715419663116336
(0, 4224) Loss: 0.4301655599847436
(0, 4416) Loss: 0.35634214524179697
(0, 4608) Loss: 0.40512302331626415
(0, 4800) Loss: 0.5585426101461053
(0, 4992) Loss: 0.4093053201213479
(0, 5184) Loss: 0.4190953904762864
(0, 5376) Loss: 0.34761136677116156
(0, 5568) Loss: 0.3679332062602043
(0, 5760) Loss: 0.40840902738273144
(0, 5952) Loss: 0.36363083496689796
(0, 6144) Loss: 0.3879687711596489
(0, 6336) Loss: 0.4343073219060898
Evaluation metrics:
	f1: {'f1': 0.9286819859507085}
	accuracy: {'accuracy': 0.9063916236912017}

(1, 192) Loss: 0.4073048885911703
(1, 384) Loss: 0.4232710925862193
(1, 576) Loss: 0.3154045222327113
(1, 768) Loss: 0.45068366825580597
(1, 960) Loss: 0.37580729834735394
(1, 1152) Loss: 0.4420566139742732
(1, 1344) Loss: 0.37600170634686947
(1, 1536) Loss: 0.405524211935699
(1, 1728) Loss: 0.4207905912771821
(1, 1920) Loss: 0.36381285823881626
(1, 2112) Loss: 0.409529447555542
(1, 2304) Loss: 0.47426076233386993
(1, 2496) Loss: 0.38367447536438704
(1, 2688) Loss: 0.3388898987323046
(1, 2880) Loss: 0.3290817094966769
(1, 3072) Loss: 0.3435690449550748
(1, 3264) Loss: 0.32456052396446466
(1, 3456) Loss: 0.3747727433219552
(1, 3648) Loss: 0.3809067429974675
(1, 3840) Loss: 0.3219820922240615
(1, 4032) Loss: 0.3739304654300213
(1, 4224) Loss: 0.381298185326159
(1, 4416) Loss: 0.3582245232537389
(1, 4608) Loss: 0.4018366727977991
(1, 4800) Loss: 0.42981692496687174
(1, 4992) Loss: 0.37442167289555073
(1, 5184) Loss: 0.5072410218417645
(1, 5376) Loss: 0.37988415453583
(1, 5568) Loss: 0.3446238525211811
(1, 5760) Loss: 0.4293290786445141
(1, 5952) Loss: 0.3294387422502041
(1, 6144) Loss: 0.31487453635782003
(1, 6336) Loss: 0.4070900445804
Evaluation metrics:
	f1: {'f1': 0.9606619987269256}
	accuracy: {'accuracy': 0.9517112048757619}

(2, 192) Loss: 0.3437607493251562
(2, 384) Loss: 0.3984246524050832
(2, 576) Loss: 0.43364101741462946
(2, 768) Loss: 0.39042749255895615
(2, 960) Loss: 0.3643191047012806
(2, 1152) Loss: 0.34772141370922327
(2, 1344) Loss: 0.3967198245227337
(2, 1536) Loss: 0.45563786569982767
(2, 1728) Loss: 0.37563113775104284
(2, 1920) Loss: 0.34482003934681416
(2, 2112) Loss: 0.3802008954808116
(2, 2304) Loss: 0.44319192599505186
(2, 2496) Loss: 0.35779237654060125
(2, 2688) Loss: 0.3466083724051714
(2, 2880) Loss: 0.313517352566123
(2, 3072) Loss: 0.40306509379297495
(2, 3264) Loss: 0.3399349432438612
(2, 3456) Loss: 0.3456319384276867
(2, 3648) Loss: 0.3872653404250741
(2, 3840) Loss: 0.31667953077703714
(2, 4032) Loss: 0.41020681988447905
(2, 4224) Loss: 0.34491028264164925
(2, 4416) Loss: 0.3507037917152047
(2, 4608) Loss: 0.35411095805466175
(2, 4800) Loss: 0.41397615149617195
(2, 4992) Loss: 0.373218709602952
(2, 5184) Loss: 0.4586480753496289
(2, 5376) Loss: 0.3514705505222082
(2, 5568) Loss: 0.3614544430747628
(2, 5760) Loss: 0.3847181349992752
(2, 5952) Loss: 0.3273956049233675
(2, 6144) Loss: 0.3186702588573098
(2, 6336) Loss: 0.42609093617647886
Evaluation metrics:
	f1: {'f1': 0.968333752199045}
	accuracy: {'accuracy': 0.960618846694796}


real	92m43.724s
user	87m11.074s
sys	4m16.463s

Using the GPU:NVIDIA Quadro RTX 8000
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
	(epoch 1, fold 1, samples 320) FClassifier Loss: 0.6783857848495245 Transformer Loss: 0.6802859902381897
	(epoch 1, fold 1, samples 640) FClassifier Loss: 0.695969607681036 Transformer Loss: 0.6825048327445984
	(epoch 1, fold 1, samples 960) FClassifier Loss: 0.6907794140279293 Transformer Loss: 0.7014418840408325
	(epoch 1, fold 1, samples 1280) FClassifier Loss: 0.6923351194709539 Transformer Loss: 0.6979488730430603
	(epoch 1, fold 1, samples 1600) FClassifier Loss: 0.684743257239461 Transformer Loss: 0.6864502429962158
	(epoch 1, fold 1, samples 1920) FClassifier Loss: 0.6900335662066936 Transformer Loss: 0.6989530324935913
	(epoch 1, fold 1, samples 2240) FClassifier Loss: 0.7095401808619499 Transformer Loss: 0.69935142993927
	(epoch 1, fold 1, samples 2560) FClassifier Loss: 0.6925762556493282 Transformer Loss: 0.6996188163757324
	(epoch 1, fold 1, samples 2880) FClassifier Loss: 0.6996819321066141 Transformer Loss: 0.6933781504631042
(epoch 1, fold 1, samples 640) Regression Accuracy: 0.59375, Loss: 0.6767061948776245
Fold 0 accuracies:
Ensemble	f1: {'f1': 0.6693657219973009}
Ensemble	accuracy: {'accuracy': 0.5030425963488844}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.04687499999999999}
Featurizer	accuracy: {'accuracy': 0.5050709939148073}

	(epoch 1, fold 2, samples 320) FClassifier Loss: 0.6850903164595366 Transformer Loss: 0.6901032328605652
	(epoch 1, fold 2, samples 640) FClassifier Loss: 0.6905376873910427 Transformer Loss: 0.6900761723518372
	(epoch 1, fold 2, samples 960) FClassifier Loss: 0.684661004692316 Transformer Loss: 0.7028422951698303
	(epoch 1, fold 2, samples 1280) FClassifier Loss: 0.694663280621171 Transformer Loss: 0.69407057762146
	(epoch 1, fold 2, samples 1600) FClassifier Loss: 0.6789945978671312 Transformer Loss: 0.7006523013114929
	(epoch 1, fold 2, samples 1920) FClassifier Loss: 0.682608874514699 Transformer Loss: 0.6929686069488525
	(epoch 1, fold 2, samples 2240) FClassifier Loss: 0.6796836871653795 Transformer Loss: 0.698492169380188
	(epoch 1, fold 2, samples 2560) FClassifier Loss: 0.6948813628405333 Transformer Loss: 0.6989960074424744
	(epoch 1, fold 2, samples 2880) FClassifier Loss: 0.6830679047852755 Transformer Loss: 0.7049201130867004
(epoch 1, fold 2, samples 640) Regression Accuracy: 0.5, Loss: 0.7029416561126709
Fold 1 accuracies:
Ensemble	f1: {'f1': 0.6693657219973009}
Ensemble	accuracy: {'accuracy': 0.5030425963488844}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.23417721518987344}
Featurizer	accuracy: {'accuracy': 0.5091277890466531}

	(epoch 1, fold 3, samples 320) FClassifier Loss: 0.6902871448546648 Transformer Loss: 0.689454197883606
	(epoch 1, fold 3, samples 640) FClassifier Loss: 0.682774955406785 Transformer Loss: 0.7068366408348083
	(epoch 1, fold 3, samples 960) FClassifier Loss: 0.6718199122697115 Transformer Loss: 0.7030131816864014
	(epoch 1, fold 3, samples 1280) FClassifier Loss: 0.6818992644548416 Transformer Loss: 0.6779183149337769
	(epoch 1, fold 3, samples 1600) FClassifier Loss: 0.7055490650236607 Transformer Loss: 0.6953885555267334
	(epoch 1, fold 3, samples 1920) FClassifier Loss: 0.6679476872086525 Transformer Loss: 0.6935344338417053
	(epoch 1, fold 3, samples 2240) FClassifier Loss: 0.6745621543377638 Transformer Loss: 0.7026820182800293
	(epoch 1, fold 3, samples 2560) FClassifier Loss: 0.667031304910779 Transformer Loss: 0.6895142197608948
	(epoch 1, fold 3, samples 2880) FClassifier Loss: 0.6783227268606424 Transformer Loss: 0.6869692802429199
(epoch 1, fold 3, samples 640) Regression Accuracy: 0.4375, Loss: 0.7179043889045715
Fold 2 accuracies:
Ensemble	f1: {'f1': 0.6693657219973009}
Ensemble	accuracy: {'accuracy': 0.5030425963488844}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.36464088397790057}
Featurizer	accuracy: {'accuracy': 0.5334685598377282}

	(epoch 1, fold 4, samples 320) FClassifier Loss: 0.6407591514289379 Transformer Loss: 0.6963524222373962
	(epoch 1, fold 4, samples 640) FClassifier Loss: 0.6492076041176915 Transformer Loss: 0.7197864651679993
	(epoch 1, fold 4, samples 960) FClassifier Loss: 0.6394240418449044 Transformer Loss: 0.684234619140625
	(epoch 1, fold 4, samples 1280) FClassifier Loss: 0.615839104168117 Transformer Loss: 0.6876152157783508
	(epoch 1, fold 4, samples 1600) FClassifier Loss: 0.6226217746734619 Transformer Loss: 0.6901922821998596
	(epoch 1, fold 4, samples 1920) FClassifier Loss: 0.6470665633678436 Transformer Loss: 0.6951241493225098
	(epoch 1, fold 4, samples 2240) FClassifier Loss: 0.6251714387908578 Transformer Loss: 0.7066105604171753
	(epoch 1, fold 4, samples 2560) FClassifier Loss: 0.6057295193895698 Transformer Loss: 0.7036020755767822
	(epoch 1, fold 4, samples 2880) FClassifier Loss: 0.6295232279226184 Transformer Loss: 0.687073826789856
(epoch 1, fold 4, samples 640) Regression Accuracy: 0.4375, Loss: 0.7222983837127686
Fold 3 accuracies:
Ensemble	f1: {'f1': 0.6693657219973009}
Ensemble	accuracy: {'accuracy': 0.5030425963488844}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.43333333333333335}
Featurizer	accuracy: {'accuracy': 0.5172413793103449}

	(epoch 1, fold 5, samples 320) FClassifier Loss: 0.5644554486498237 Transformer Loss: 0.6886003613471985
	(epoch 1, fold 5, samples 640) FClassifier Loss: 0.5268494668416679 Transformer Loss: 0.6970864534378052
	(epoch 1, fold 5, samples 960) FClassifier Loss: 0.5095651508308947 Transformer Loss: 0.6824864745140076
	(epoch 1, fold 5, samples 1280) FClassifier Loss: 0.5645864261314273 Transformer Loss: 0.6822193264961243
	(epoch 1, fold 5, samples 1600) FClassifier Loss: 0.4845747221261263 Transformer Loss: 0.6746521592140198
	(epoch 1, fold 5, samples 1920) FClassifier Loss: 0.5086399856954813 Transformer Loss: 0.6866669654846191
	(epoch 1, fold 5, samples 2240) FClassifier Loss: 0.5512580880895257 Transformer Loss: 0.695948600769043
	(epoch 1, fold 5, samples 2560) FClassifier Loss: 0.5780022349208593 Transformer Loss: 0.6969361901283264
	(epoch 1, fold 5, samples 2880) FClassifier Loss: 0.5841494812630117 Transformer Loss: 0.6839580535888672
(epoch 1, fold 5, samples 640) Regression Accuracy: 0.5, Loss: 0.7050523161888123
Fold 4 accuracies:
Ensemble	f1: {'f1': 0.6693657219973009}
Ensemble	accuracy: {'accuracy': 0.5030425963488844}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.4640371229698376}
Featurizer	accuracy: {'accuracy': 0.5314401622718052}

Epoch 0 accuracies:
Ensemble	f1: {'f1': 0.6693657219973009}
Ensemble	accuracy: {'accuracy': 0.5030425963488844}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.4640371229698376}
Featurizer	accuracy: {'accuracy': 0.5314401622718052}


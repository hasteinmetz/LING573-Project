Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6800097953528166
(0, 384) Loss: 0.6876745820045471
(0, 576) Loss: 0.7139616440981627
(0, 768) Loss: 0.5995706478133798
(0, 960) Loss: 0.48494969960302114
(0, 1152) Loss: 0.3807045891880989
(0, 1344) Loss: 0.5069176405668259
(0, 1536) Loss: 0.3877069465816021
(0, 1728) Loss: 0.4310040194541216
(0, 1920) Loss: 0.4272210206836462
(0, 2112) Loss: 0.42463411670178175
(0, 2304) Loss: 0.413923560641706
(0, 2496) Loss: 0.40803145710378885
(0, 2688) Loss: 0.3219903027638793
(0, 2880) Loss: 0.40106014534831047
(0, 3072) Loss: 0.40731138829141855
(0, 3264) Loss: 0.31521235685795546
(0, 3456) Loss: 0.47208770364522934
(0, 3648) Loss: 0.43269871454685926
(0, 3840) Loss: 0.3465950796380639
(0, 4032) Loss: 0.3818898070603609
(0, 4224) Loss: 0.4209850085899234
(0, 4416) Loss: 0.40399579983204603
(0, 4608) Loss: 0.3857515985146165
(0, 4800) Loss: 0.47358054015785456
(0, 4992) Loss: 0.37795322202146053
(0, 5184) Loss: 0.38079495541751385
(0, 5376) Loss: 0.37382232025265694
(0, 5568) Loss: 0.3817898128181696
(0, 5760) Loss: 0.42250621132552624
(0, 5952) Loss: 0.3170834630727768
(0, 6144) Loss: 0.3360914709046483
(0, 6336) Loss: 0.410531104542315
Evaluation metrics:
	f1: {'f1': 0.9317738791423003}
	accuracy: {'accuracy': 0.9125}

(1, 192) Loss: 0.37707933131605387
(1, 384) Loss: 0.3866396499797702
(1, 576) Loss: 0.4864151608198881
(1, 768) Loss: 0.4686162630096078
(1, 960) Loss: 0.4078694013878703
(1, 1152) Loss: 0.37007780373096466
(1, 1344) Loss: 0.40082979295402765
(1, 1536) Loss: 0.3759545600041747
(1, 1728) Loss: 0.3446726482361555
(1, 1920) Loss: 0.3986876355484128
(1, 2112) Loss: 0.3443365581333637
(1, 2304) Loss: 0.3933705845847726
(1, 2496) Loss: 0.3450958952307701
(1, 2688) Loss: 0.31862671580165625
(1, 2880) Loss: 0.3697517327964306
(1, 3072) Loss: 0.35157005209475756
(1, 3264) Loss: 0.3145244438201189
(1, 3456) Loss: 0.4313754737377167
(1, 3648) Loss: 0.3374707503244281
(1, 3840) Loss: 0.3217476522549987
(1, 4032) Loss: 0.4205805519595742
(1, 4224) Loss: 0.4710532296448946
(1, 4416) Loss: 0.3142805276438594
(1, 4608) Loss: 0.35366963129490614
(1, 4800) Loss: 0.37641935609281063
(1, 4992) Loss: 0.3166120629757643
(1, 5184) Loss: 0.37586689088493586
(1, 5376) Loss: 0.3444788930937648
(1, 5568) Loss: 0.37581945955753326
(1, 5760) Loss: 0.37117167189717293
(1, 5952) Loss: 0.31414576154202223
(1, 6144) Loss: 0.3567897994071245
(1, 6336) Loss: 0.40697749238461256
Evaluation metrics:
	f1: {'f1': 0.9079069767441861}
	accuracy: {'accuracy': 0.87625}

(2, 192) Loss: 0.5451611885800958
(2, 384) Loss: 0.5058897435665131
(2, 576) Loss: 0.34452188294380903
(2, 768) Loss: 0.4331672955304384
(2, 960) Loss: 0.43802235927432775
(2, 1152) Loss: 0.31336445827037096
(2, 1344) Loss: 0.3748191352933645
(2, 1536) Loss: 0.3758526695892215
(2, 1728) Loss: 0.37608372885733843
(2, 1920) Loss: 0.35375385638326406
(2, 2112) Loss: 0.31599470414221287
(2, 2304) Loss: 0.40707673598080873
(2, 2496) Loss: 0.3247702531516552
(2, 2688) Loss: 0.32826842553913593
(2, 2880) Loss: 0.36898830253630877
(2, 3072) Loss: 0.31335967499762774
(2, 3264) Loss: 0.313801109790802
(2, 3456) Loss: 0.37583418749272823
(2, 3648) Loss: 0.4016161123290658
(2, 3840) Loss: 0.3135349350050092
(2, 4032) Loss: 0.40685312543064356
(2, 4224) Loss: 0.4070250913500786
(2, 4416) Loss: 0.34321772120893
(2, 4608) Loss: 0.3864890467375517
(2, 4800) Loss: 0.4069815445691347
(2, 4992) Loss: 0.39782934822142124
(2, 5184) Loss: 0.37583392299711704
(2, 5376) Loss: 0.34355887491256
(2, 5568) Loss: 0.34516812674701214
(2, 5760) Loss: 0.34478318504989147
(2, 5952) Loss: 0.34460549615323544
(2, 6144) Loss: 0.36284137424081564
(2, 6336) Loss: 0.40712108835577965
Evaluation metrics:
	f1: {'f1': 0.9285714285714286}
	accuracy: {'accuracy': 0.915}

(3, 192) Loss: 0.34572198800742626
(3, 384) Loss: 0.37602979969233274
(3, 576) Loss: 0.3446391997858882
(3, 768) Loss: 0.39989858120679855
(3, 960) Loss: 0.3784865001216531
(3, 1152) Loss: 0.3451696103438735
(3, 1344) Loss: 0.3728714846074581
(3, 1536) Loss: 0.37585561256855726
(3, 1728) Loss: 0.35567549616098404
(3, 1920) Loss: 0.35621121153235435
(3, 2112) Loss: 0.3482491485774517
(3, 2304) Loss: 0.40613315999507904
(3, 2496) Loss: 0.31361419428139925
(3, 2688) Loss: 0.31374181620776653
(3, 2880) Loss: 0.3380368696525693
(3, 3072) Loss: 0.34386239293962717
(3, 3264) Loss: 0.32025320176035166
(3, 3456) Loss: 0.3614702159538865
(3, 3648) Loss: 0.37617114651948214
(3, 3840) Loss: 0.317435797303915
(3, 4032) Loss: 0.34487778786569834
(3, 4224) Loss: 0.37587412167340517
(3, 4416) Loss: 0.3445510035380721
(3, 4608) Loss: 0.31363521609455347
(3, 4800) Loss: 0.34506584983319044
(3, 4992) Loss: 0.31345951836556196
(3, 5184) Loss: 0.3816601298749447
(3, 5376) Loss: 0.3737982353195548
(3, 5568) Loss: 0.34460277762264013
(3, 5760) Loss: 0.3758030142635107
(3, 5952) Loss: 0.3133236365392804
(3, 6144) Loss: 0.3450325606390834
(3, 6336) Loss: 0.41833438258618116
Evaluation metrics:
	f1: {'f1': 0.9232245681381958}
	accuracy: {'accuracy': 0.9}

Saving model to src/models/neural_ensemble/neural_ensemble/humor/...

real	119m25.492s
user	107m44.588s
sys	8m59.488s

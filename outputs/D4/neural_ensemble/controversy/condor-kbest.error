Using the GPU:NVIDIA Quadro RTX 8000
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 144) Loss: 0.6609862223267555
(0, 288) Loss: 0.6811735692123572
(0, 432) Loss: 0.6739448110262553
(0, 576) Loss: 0.7474693755308787
(0, 720) Loss: 0.6924945736924807
(0, 864) Loss: 0.6896540522575378
(0, 1008) Loss: 0.6857246359189351
(0, 1152) Loss: 0.649393996844689
(0, 1296) Loss: 0.6764917969703674
(0, 1440) Loss: 0.6837276195486386
(0, 1584) Loss: 0.6887821480631828
(0, 1728) Loss: 0.7015437905987103
(0, 1872) Loss: 0.6784137561917305
(0, 2016) Loss: 0.7350088531772295
(0, 2160) Loss: 0.6998142177859942
(0, 2304) Loss: 0.6917932455738385
(0, 2448) Loss: 0.7078336949149767
(0, 2592) Loss: 0.6966141238808632
(0, 2736) Loss: 0.6882240449388821
(0, 2880) Loss: 0.7158747911453247
(0, 3024) Loss: 0.6836290409167607
(0, 3168) Loss: 0.6930269102255503
(0, 3312) Loss: 0.6929389039675394
(0, 3456) Loss: 0.6948794772227604
(0, 3600) Loss: 0.7002155457933743

Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.7093198299407959
(0, 384) Loss: 0.6948872804641724
(0, 576) Loss: 0.6976783871650696
(0, 768) Loss: 0.6942834854125977
(0, 960) Loss: 0.6981831789016724
(0, 1152) Loss: 0.6861380338668823
(0, 1344) Loss: 0.691564679145813
(0, 1536) Loss: 0.695770263671875
(0, 1728) Loss: 0.7014021277427673
(0, 1920) Loss: 0.6686700582504272
(0, 2112) Loss: 0.6789441108703613
(0, 2304) Loss: 0.6963287591934204
(0, 2496) Loss: 0.6966187357902527
(0, 2688) Loss: 0.6798834800720215
(0, 2880) Loss: 0.6817988157272339
(0, 3072) Loss: 0.701299250125885
(0, 3264) Loss: 0.6883047819137573
(0, 3456) Loss: 0.6969262361526489
(0, 3648) Loss: 0.6867789030075073
(0, 3840) Loss: 0.71337890625
Evaluation metrics:
	f1: {'f1': 0.04669260700389105}
	accuracy: {'accuracy': 0.5030425963488844}

(1, 192) Loss: 0.6863464117050171
(1, 384) Loss: 0.6803188920021057
(1, 576) Loss: 0.6831920146942139
(1, 768) Loss: 0.6928426027297974
(1, 960) Loss: 0.6709129214286804
(1, 1152) Loss: 0.6470513343811035
(1, 1344) Loss: 0.6828600764274597
(1, 1536) Loss: 0.6919330358505249
(1, 1728) Loss: 0.6862265467643738
(1, 1920) Loss: 0.6524425745010376
(1, 2112) Loss: 0.645281195640564
(1, 2304) Loss: 0.6997503042221069
(1, 2496) Loss: 0.6768221855163574
(1, 2688) Loss: 0.6903983950614929
(1, 2880) Loss: 0.6713495850563049
(1, 3072) Loss: 0.701799750328064
(1, 3264) Loss: 0.6749824285507202
(1, 3456) Loss: 0.6972211599349976
(1, 3648) Loss: 0.6768669486045837
(1, 3840) Loss: 0.6868076324462891
Evaluation metrics:
	f1: {'f1': 0.0392156862745098}
	accuracy: {'accuracy': 0.5030425963488844}

(2, 192) Loss: 0.6697931885719299
(2, 384) Loss: 0.6585531234741211
(2, 576) Loss: 0.6355862021446228
(2, 768) Loss: 0.6652689576148987
(2, 960) Loss: 0.6076948046684265
(2, 1152) Loss: 0.5824031829833984
(2, 1344) Loss: 0.6962524652481079
(2, 1536) Loss: 0.6632766723632812
(2, 1728) Loss: 0.6330074667930603
(2, 1920) Loss: 0.5992430448532104
(2, 2112) Loss: 0.5587964057922363
(2, 2304) Loss: 0.6947667002677917
(2, 2496) Loss: 0.7062488794326782
(2, 2688) Loss: 0.6878229975700378
(2, 2880) Loss: 0.6966274976730347
(2, 3072) Loss: 0.7049242854118347
(2, 3264) Loss: 0.6691548824310303
(2, 3456) Loss: 0.6440396308898926
(2, 3648) Loss: 0.663040280342102
(2, 3840) Loss: 0.72635817527771
Evaluation metrics:
	f1: {'f1': 0.023904382470119518}
	accuracy: {'accuracy': 0.5030425963488844}

(3, 192) Loss: 0.6674419641494751
(3, 384) Loss: 0.6470491290092468
(3, 576) Loss: 0.6121833324432373
(3, 768) Loss: 0.6094167232513428
(3, 960) Loss: 0.4945136606693268
(3, 1152) Loss: 0.48413971066474915
(3, 1344) Loss: 0.5031626224517822
(3, 1536) Loss: 0.451493501663208
(3, 1728) Loss: 0.44002169370651245
(3, 1920) Loss: 0.45681101083755493
(3, 2112) Loss: 0.49546393752098083
(3, 2304) Loss: 0.7646340131759644
(3, 2496) Loss: 0.649240255355835
(3, 2688) Loss: 0.6916055679321289
(3, 2880) Loss: 0.6353236436843872
(3, 3072) Loss: 0.6977165341377258
(3, 3264) Loss: 0.5984789133071899
(3, 3456) Loss: 0.5738240480422974
(3, 3648) Loss: 0.4918309450149536
(3, 3840) Loss: 0.5763226747512817
Evaluation metrics:
	f1: {'f1': 0.2331288343558282}
	accuracy: {'accuracy': 0.49290060851926976}

(4, 192) Loss: 0.6488871574401855
(4, 384) Loss: 0.5472374558448792
(4, 576) Loss: 0.45304960012435913
(4, 768) Loss: 0.4088747203350067
(4, 960) Loss: 0.35030972957611084
(4, 1152) Loss: 0.3893260955810547
(4, 1344) Loss: 0.4094897508621216
(4, 1536) Loss: 0.46151238679885864
(4, 1728) Loss: 0.36068981885910034
(4, 1920) Loss: 0.3838402032852173
(4, 2112) Loss: 0.4333668649196625
(4, 2304) Loss: 0.7816053628921509
(4, 2496) Loss: 0.45483338832855225
(4, 2688) Loss: 0.5443466901779175
(4, 2880) Loss: 0.5709630250930786
(4, 3072) Loss: 0.7331011891365051
(4, 3264) Loss: 0.5006715655326843
(4, 3456) Loss: 0.5240151882171631
(4, 3648) Loss: 0.44081994891166687
(4, 3840) Loss: 0.48497360944747925
Evaluation metrics:
	f1: {'f1': 0.4567627494456763}
	accuracy: {'accuracy': 0.5030425963488844}

Saving model to src/models/neural_ensemble/neural_ensemble/controversy/...

real	74m28.074s
user	66m51.496s
sys	6m42.218s

Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.726084291934967
(0, 384) Loss: 0.6994258165359497
(0, 576) Loss: 0.7006939649581909
(0, 768) Loss: 0.688482403755188
(0, 960) Loss: 0.6921148896217346
(0, 1152) Loss: 0.6925586462020874
(0, 1344) Loss: 0.699682891368866
(0, 1536) Loss: 0.7034273743629456
(0, 1728) Loss: 0.6944745182991028
(0, 1920) Loss: 0.6865414381027222
(0, 2112) Loss: 0.6885690689086914
(0, 2304) Loss: 0.7051822543144226
(0, 2496) Loss: 0.6900238990783691
(0, 2688) Loss: 0.6964746713638306
(0, 2880) Loss: 0.6962177157402039
(0, 3072) Loss: 0.6901895403862
(0, 3264) Loss: 0.6943463683128357
(0, 3456) Loss: 0.6943261027336121
(0, 3648) Loss: 0.6896583437919617
(0, 3840) Loss: 0.7098903656005859
Evaluation metrics:
	f1: {'f1': 0.2613981762917933}
	accuracy: {'accuracy': 0.5070993914807302}

(1, 192) Loss: 0.686968207359314
(1, 384) Loss: 0.6792185306549072
(1, 576) Loss: 0.6904537677764893
(1, 768) Loss: 0.6918818950653076
(1, 960) Loss: 0.6840452551841736
(1, 1152) Loss: 0.6719024777412415
(1, 1344) Loss: 0.687685489654541
(1, 1536) Loss: 0.6926971673965454
(1, 1728) Loss: 0.6838803291320801
(1, 1920) Loss: 0.6440467834472656
(1, 2112) Loss: 0.6488409042358398
(1, 2304) Loss: 0.6920172572135925
(1, 2496) Loss: 0.6842868328094482
(1, 2688) Loss: 0.6886842250823975
(1, 2880) Loss: 0.692851722240448
(1, 3072) Loss: 0.6985783576965332
(1, 3264) Loss: 0.692647397518158
(1, 3456) Loss: 0.6894904375076294
(1, 3648) Loss: 0.689754843711853
(1, 3840) Loss: 0.6883658170700073
Evaluation metrics:
	f1: {'f1': 0.11152416356877325}
	accuracy: {'accuracy': 0.5152129817444219}

(2, 192) Loss: 0.6842973232269287
(2, 384) Loss: 0.6588168144226074
(2, 576) Loss: 0.6554204225540161
(2, 768) Loss: 0.6763061285018921
(2, 960) Loss: 0.6351709365844727
(2, 1152) Loss: 0.6254197359085083
(2, 1344) Loss: 0.681460976600647
(2, 1536) Loss: 0.6541458368301392
(2, 1728) Loss: 0.6618471741676331
(2, 1920) Loss: 0.588155210018158
(2, 2112) Loss: 0.5659377574920654
(2, 2304) Loss: 0.6693004965782166
(2, 2496) Loss: 0.6328269243240356
(2, 2688) Loss: 0.662411630153656
(2, 2880) Loss: 0.6390185356140137
(2, 3072) Loss: 0.6689141988754272
(2, 3264) Loss: 0.6612353324890137
(2, 3456) Loss: 0.6702768802642822
(2, 3648) Loss: 0.6570667028427124
(2, 3840) Loss: 0.6767903566360474
Evaluation metrics:
	f1: {'f1': 0.17627118644067796}
	accuracy: {'accuracy': 0.5070993914807302}

(3, 192) Loss: 0.638570249080658
(3, 384) Loss: 0.6501454710960388
(3, 576) Loss: 0.612068235874176
(3, 768) Loss: 0.6494227647781372
(3, 960) Loss: 0.5312851071357727
(3, 1152) Loss: 0.5147611498832703
(3, 1344) Loss: 0.5938995480537415
(3, 1536) Loss: 0.5208932757377625
(3, 1728) Loss: 0.5318626761436462
(3, 1920) Loss: 0.4155977666378021
(3, 2112) Loss: 0.46422165632247925
(3, 2304) Loss: 0.5716468691825867
(3, 2496) Loss: 0.4158037602901459
(3, 2688) Loss: 0.5244631767272949
(3, 2880) Loss: 0.510370135307312
(3, 3072) Loss: 0.5401177406311035
(3, 3264) Loss: 0.5471612215042114
(3, 3456) Loss: 0.5982203483581543
(3, 3648) Loss: 0.5222691297531128
(3, 3840) Loss: 0.5631378889083862
Evaluation metrics:
	f1: {'f1': 0.2696629213483146}
	accuracy: {'accuracy': 0.4726166328600406}

(4, 192) Loss: 0.4829285740852356
(4, 384) Loss: 0.5049504041671753
(4, 576) Loss: 0.4514271318912506
(4, 768) Loss: 0.4244394302368164
(4, 960) Loss: 0.3514849543571472
(4, 1152) Loss: 0.38450148701667786
(4, 1344) Loss: 0.4722347855567932
(4, 1536) Loss: 0.4532075822353363
(4, 1728) Loss: 0.40097951889038086
(4, 1920) Loss: 0.41458502411842346
(4, 2112) Loss: 0.35797104239463806
(4, 2304) Loss: 0.4903675317764282
(4, 2496) Loss: 0.41034770011901855
(4, 2688) Loss: 0.4485606551170349
(4, 2880) Loss: 0.438613623380661
(4, 3072) Loss: 0.4943726062774658
(4, 3264) Loss: 0.4236108958721161
(4, 3456) Loss: 0.41497841477394104
(4, 3648) Loss: 0.44210365414619446
(4, 3840) Loss: 0.449506938457489
Evaluation metrics:
	f1: {'f1': 0.45879732739420936}
	accuracy: {'accuracy': 0.5070993914807302}

Saving model to src/models/neural_ensemble/neural_ensemble/controversy/...

real	70m38.424s
user	64m7.928s
sys	5m40.600s

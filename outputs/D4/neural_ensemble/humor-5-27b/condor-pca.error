Using the GPU:NVIDIA Quadro RTX 8000
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6497541666030884
(0, 384) Loss: 0.69454026222229
(0, 576) Loss: 0.6967806816101074
(0, 768) Loss: 0.612055778503418
(0, 960) Loss: 0.5872057676315308
(0, 1152) Loss: 0.548654317855835
(0, 1344) Loss: 0.5112760066986084
(0, 1536) Loss: 0.4002778232097626
(0, 1728) Loss: 0.482435017824173
(0, 1920) Loss: 0.5144343972206116
(0, 2112) Loss: 0.3811678886413574
(0, 2304) Loss: 0.40358084440231323
(0, 2496) Loss: 0.40796002745628357
(0, 2688) Loss: 0.4021732807159424
(0, 2880) Loss: 0.4406103491783142
(0, 3072) Loss: 0.4695168733596802
(0, 3264) Loss: 0.3315022587776184
(0, 3456) Loss: 0.42350777983665466
(0, 3648) Loss: 0.36886581778526306
(0, 3840) Loss: 0.34920620918273926
(0, 4032) Loss: 0.3534449636936188
(0, 4224) Loss: 0.39954590797424316
(0, 4416) Loss: 0.37817609310150146
(0, 4608) Loss: 0.3862585723400116
(0, 4800) Loss: 0.4272969961166382
(0, 4992) Loss: 0.3480544090270996
(0, 5184) Loss: 0.3958101272583008
(0, 5376) Loss: 0.38127171993255615
(0, 5568) Loss: 0.3465665578842163
(0, 5760) Loss: 0.39678651094436646
(0, 5952) Loss: 0.3755449950695038
(0, 6144) Loss: 0.5105807781219482
(0, 6336) Loss: 0.43698710203170776
Evaluation metrics:
	f1: {'f1': 0.9247104247104247}
	accuracy: {'accuracy': 0.9025}

(1, 192) Loss: 0.3488876223564148
(1, 384) Loss: 0.4074924886226654
(1, 576) Loss: 0.4254988431930542
(1, 768) Loss: 0.42673301696777344
(1, 960) Loss: 0.35981667041778564
(1, 1152) Loss: 0.37842172384262085
(1, 1344) Loss: 0.39684581756591797
(1, 1536) Loss: 0.3789805769920349
(1, 1728) Loss: 0.41551879048347473
(1, 1920) Loss: 0.3423471450805664
(1, 2112) Loss: 0.3423314690589905
(1, 2304) Loss: 0.42954161763191223
(1, 2496) Loss: 0.4129800796508789
(1, 2688) Loss: 0.37674230337142944
(1, 2880) Loss: 0.4137313663959503
(1, 3072) Loss: 0.34481072425842285
(1, 3264) Loss: 0.35506975650787354
(1, 3456) Loss: 0.3759079575538635
(1, 3648) Loss: 0.3642197251319885
(1, 3840) Loss: 0.37290602922439575
(1, 4032) Loss: 0.40924158692359924
(1, 4224) Loss: 0.38575586676597595
(1, 4416) Loss: 0.35464879870414734
(1, 4608) Loss: 0.3658466935157776
(1, 4800) Loss: 0.34801486134529114
(1, 4992) Loss: 0.31386929750442505
(1, 5184) Loss: 0.33524876832962036
(1, 5376) Loss: 0.34953761100769043
(1, 5568) Loss: 0.3550337851047516
(1, 5760) Loss: 0.37590813636779785
(1, 5952) Loss: 0.3651355803012848
(1, 6144) Loss: 0.3481737971305847
(1, 6336) Loss: 0.40545016527175903
Evaluation metrics:
	f1: {'f1': 0.9329268292682926}
	accuracy: {'accuracy': 0.9175}

(2, 192) Loss: 0.3446876108646393
(2, 384) Loss: 0.37514203786849976
(2, 576) Loss: 0.34561610221862793
(2, 768) Loss: 0.38990622758865356
(2, 960) Loss: 0.3448446989059448
(2, 1152) Loss: 0.3247358500957489
(2, 1344) Loss: 0.3780214190483093
(2, 1536) Loss: 0.35334745049476624
(2, 1728) Loss: 0.41862133145332336
(2, 1920) Loss: 0.39344486594200134
(2, 2112) Loss: 0.3133748471736908
(2, 2304) Loss: 0.3760126233100891
(2, 2496) Loss: 0.3144060969352722
(2, 2688) Loss: 0.3146257698535919
(2, 2880) Loss: 0.3441672921180725
(2, 3072) Loss: 0.37794506549835205
(2, 3264) Loss: 0.31678506731987
(2, 3456) Loss: 0.38183167576789856
(2, 3648) Loss: 0.3757268488407135
(2, 3840) Loss: 0.3137524724006653
(2, 4032) Loss: 0.313925176858902
(2, 4224) Loss: 0.3770906329154968
(2, 4416) Loss: 0.3444281220436096
(2, 4608) Loss: 0.36263972520828247
(2, 4800) Loss: 0.37589699029922485
(2, 4992) Loss: 0.358946830034256
(2, 5184) Loss: 0.3157995939254761
(2, 5376) Loss: 0.32246533036231995
(2, 5568) Loss: 0.34455883502960205
(2, 5760) Loss: 0.4073798358440399
(2, 5952) Loss: 0.31504493951797485
(2, 6144) Loss: 0.3528551459312439
(2, 6336) Loss: 0.4053759276866913
Evaluation metrics:
	f1: {'f1': 0.8950749464668095}
	accuracy: {'accuracy': 0.8775}

(3, 192) Loss: 0.3150829076766968
(3, 384) Loss: 0.42896705865859985
(3, 576) Loss: 0.3135840892791748
(3, 768) Loss: 0.3758825659751892
(3, 960) Loss: 0.437041699886322
(3, 1152) Loss: 0.34462231397628784
(3, 1344) Loss: 0.37513095140457153
(3, 1536) Loss: 0.3656556010246277
(3, 1728) Loss: 0.3452981114387512
(3, 1920) Loss: 0.3134939670562744
(3, 2112) Loss: 0.31332483887672424
(3, 2304) Loss: 0.47211340069770813
(3, 2496) Loss: 0.316534161567688
(3, 2688) Loss: 0.3134722113609314
(3, 2880) Loss: 0.33909255266189575
(3, 3072) Loss: 0.3449168801307678
(3, 3264) Loss: 0.31347692012786865
(3, 3456) Loss: 0.34487414360046387
(3, 3648) Loss: 0.3446032404899597
(3, 3840) Loss: 0.3337028920650482
(3, 4032) Loss: 0.3447297215461731
(3, 4224) Loss: 0.3758169710636139
(3, 4416) Loss: 0.3429994583129883
(3, 4608) Loss: 0.34101352095603943
(3, 4800) Loss: 0.3542318344116211
(3, 4992) Loss: 0.3655758798122406
(3, 5184) Loss: 0.3176220655441284
(3, 5376) Loss: 0.31345346570014954
(3, 5568) Loss: 0.34455275535583496
(3, 5760) Loss: 0.40470176935195923
(3, 5952) Loss: 0.31334537267684937
(3, 6144) Loss: 0.3437923491001129
(3, 6336) Loss: 0.3448654115200043
Evaluation metrics:
	f1: {'f1': 0.9468405215646941}
	accuracy: {'accuracy': 0.93375}

(4, 192) Loss: 0.313301146030426
(4, 384) Loss: 0.3749515414237976
(4, 576) Loss: 0.45782482624053955
(4, 768) Loss: 0.35124480724334717
(4, 960) Loss: 0.3757036328315735
(4, 1152) Loss: 0.3310614228248596
(4, 1344) Loss: 0.4138479232788086
(4, 1536) Loss: 0.34473752975463867
(4, 1728) Loss: 0.36891916394233704
(4, 1920) Loss: 0.40811100602149963
(4, 2112) Loss: 0.31337061524391174
(4, 2304) Loss: 0.3776080012321472
(4, 2496) Loss: 0.47132211923599243
(4, 2688) Loss: 0.31336545944213867
(4, 2880) Loss: 0.3132916986942291
(4, 3072) Loss: 0.3783412277698517
(4, 3264) Loss: 0.31328707933425903
(4, 3456) Loss: 0.3445501923561096
(4, 3648) Loss: 0.3445332646369934
(4, 3840) Loss: 0.31330424547195435
(4, 4032) Loss: 0.38433146476745605
(4, 4224) Loss: 0.37577834725379944
(4, 4416) Loss: 0.4241954982280731
(4, 4608) Loss: 0.3445342779159546
(4, 4800) Loss: 0.37520861625671387
(4, 4992) Loss: 0.336628258228302
(4, 5184) Loss: 0.313290536403656
(4, 5376) Loss: 0.3171907067298889
(4, 5568) Loss: 0.3545609712600708
(4, 5760) Loss: 0.37616679072380066
(4, 5952) Loss: 0.34219664335250854
(4, 6144) Loss: 0.31350278854370117
(4, 6336) Loss: 0.37576550245285034
Evaluation metrics:
	f1: {'f1': 0.9294605809128631}
	accuracy: {'accuracy': 0.915}

Saving model to src/models/neural_ensemble/neural_ensemble/humor/...

real	90m54.453s
user	84m46.484s
sys	5m17.219s

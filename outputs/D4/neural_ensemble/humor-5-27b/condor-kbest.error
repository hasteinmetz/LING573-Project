Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6795069575309753
(0, 384) Loss: 0.6853501200675964
(0, 576) Loss: 0.7355185747146606
(0, 768) Loss: 0.6024441719055176
(0, 960) Loss: 0.5445748567581177
(0, 1152) Loss: 0.4166377782821655
(0, 1344) Loss: 0.47776591777801514
(0, 1536) Loss: 0.5047271251678467
(0, 1728) Loss: 0.4333154261112213
(0, 1920) Loss: 0.527677059173584
(0, 2112) Loss: 0.4361860156059265
(0, 2304) Loss: 0.3654913902282715
(0, 2496) Loss: 0.34623172879219055
(0, 2688) Loss: 0.413072407245636
(0, 2880) Loss: 0.42123809456825256
(0, 3072) Loss: 0.4894600510597229
(0, 3264) Loss: 0.39586058259010315
(0, 3456) Loss: 0.4029790461063385
(0, 3648) Loss: 0.3992335796356201
(0, 3840) Loss: 0.32012537121772766
(0, 4032) Loss: 0.3521963953971863
(0, 4224) Loss: 0.4101603925228119
(0, 4416) Loss: 0.4377064108848572
(0, 4608) Loss: 0.4042593836784363
(0, 4800) Loss: 0.4555216431617737
(0, 4992) Loss: 0.39601874351501465
(0, 5184) Loss: 0.37365132570266724
(0, 5376) Loss: 0.3483316898345947
(0, 5568) Loss: 0.37657588720321655
(0, 5760) Loss: 0.3880355954170227
(0, 5952) Loss: 0.34073060750961304
(0, 6144) Loss: 0.3456564247608185
(0, 6336) Loss: 0.47979190945625305
Evaluation metrics:
	f1: {'f1': 0.9281997918834547}
	accuracy: {'accuracy': 0.91375}

(1, 192) Loss: 0.41882801055908203
(1, 384) Loss: 0.38502153754234314
(1, 576) Loss: 0.38235169649124146
(1, 768) Loss: 0.4217846095561981
(1, 960) Loss: 0.34490710496902466
(1, 1152) Loss: 0.3717241585254669
(1, 1344) Loss: 0.40675169229507446
(1, 1536) Loss: 0.3865852355957031
(1, 1728) Loss: 0.31431329250335693
(1, 1920) Loss: 0.31485170125961304
(1, 2112) Loss: 0.3460948169231415
(1, 2304) Loss: 0.47421208024024963
(1, 2496) Loss: 0.3430997431278229
(1, 2688) Loss: 0.3462458848953247
(1, 2880) Loss: 0.31725019216537476
(1, 3072) Loss: 0.3421170711517334
(1, 3264) Loss: 0.31541678309440613
(1, 3456) Loss: 0.37663209438323975
(1, 3648) Loss: 0.31381669640541077
(1, 3840) Loss: 0.33285701274871826
(1, 4032) Loss: 0.3448673486709595
(1, 4224) Loss: 0.37616658210754395
(1, 4416) Loss: 0.39052578806877136
(1, 4608) Loss: 0.3704583942890167
(1, 4800) Loss: 0.36635327339172363
(1, 4992) Loss: 0.3760101795196533
(1, 5184) Loss: 0.347008615732193
(1, 5376) Loss: 0.39699065685272217
(1, 5568) Loss: 0.37611573934555054
(1, 5760) Loss: 0.37592679262161255
(1, 5952) Loss: 0.31439274549484253
(1, 6144) Loss: 0.3541402518749237
(1, 6336) Loss: 0.34594401717185974
Evaluation metrics:
	f1: {'f1': 0.9403437815975734}
	accuracy: {'accuracy': 0.92625}

(2, 192) Loss: 0.34510552883148193
(2, 384) Loss: 0.3817098140716553
(2, 576) Loss: 0.3586543798446655
(2, 768) Loss: 0.37521111965179443
(2, 960) Loss: 0.34469181299209595
(2, 1152) Loss: 0.3791425824165344
(2, 1344) Loss: 0.3909653127193451
(2, 1536) Loss: 0.3501521348953247
(2, 1728) Loss: 0.3476371169090271
(2, 1920) Loss: 0.32764583826065063
(2, 2112) Loss: 0.3716896176338196
(2, 2304) Loss: 0.4183942675590515
(2, 2496) Loss: 0.31638944149017334
(2, 2688) Loss: 0.3149001896381378
(2, 2880) Loss: 0.3205028772354126
(2, 3072) Loss: 0.34465137124061584
(2, 3264) Loss: 0.31340694427490234
(2, 3456) Loss: 0.37234488129615784
(2, 3648) Loss: 0.37486255168914795
(2, 3840) Loss: 0.32065534591674805
(2, 4032) Loss: 0.34536516666412354
(2, 4224) Loss: 0.3759409189224243
(2, 4416) Loss: 0.37033212184906006
(2, 4608) Loss: 0.31371957063674927
(2, 4800) Loss: 0.3479916751384735
(2, 4992) Loss: 0.34495729207992554
(2, 5184) Loss: 0.3347267508506775
(2, 5376) Loss: 0.34350574016571045
(2, 5568) Loss: 0.3764926791191101
(2, 5760) Loss: 0.3864574730396271
(2, 5952) Loss: 0.31390491127967834
(2, 6144) Loss: 0.34458842873573303
(2, 6336) Loss: 0.37345021963119507
Evaluation metrics:
	f1: {'f1': 0.9478957915831664}
	accuracy: {'accuracy': 0.935}

(3, 192) Loss: 0.34284690022468567
(3, 384) Loss: 0.3656342327594757
(3, 576) Loss: 0.34751710295677185
(3, 768) Loss: 0.3535679280757904
(3, 960) Loss: 0.3681202232837677
(3, 1152) Loss: 0.3455832302570343
(3, 1344) Loss: 0.4057502746582031
(3, 1536) Loss: 0.34857118129730225
(3, 1728) Loss: 0.32403671741485596
(3, 1920) Loss: 0.31536436080932617
(3, 2112) Loss: 0.34940674901008606
(3, 2304) Loss: 0.37630695104599
(3, 2496) Loss: 0.31499743461608887
(3, 2688) Loss: 0.3134785294532776
(3, 2880) Loss: 0.31342440843582153
(3, 3072) Loss: 0.31444305181503296
(3, 3264) Loss: 0.3133580684661865
(3, 3456) Loss: 0.35215550661087036
(3, 3648) Loss: 0.31340718269348145
(3, 3840) Loss: 0.31332921981811523
(3, 4032) Loss: 0.3470808267593384
(3, 4224) Loss: 0.37593701481819153
(3, 4416) Loss: 0.35748744010925293
(3, 4608) Loss: 0.3133378326892853
(3, 4800) Loss: 0.3680426776409149
(3, 4992) Loss: 0.3761111795902252
(3, 5184) Loss: 0.37230175733566284
(3, 5376) Loss: 0.3153122067451477
(3, 5568) Loss: 0.3447362184524536
(3, 5760) Loss: 0.34458857774734497
(3, 5952) Loss: 0.3295990824699402
(3, 6144) Loss: 0.3189154863357544
(3, 6336) Loss: 0.3133453130722046
Evaluation metrics:
	f1: {'f1': 0.926829268292683}
	accuracy: {'accuracy': 0.91375}

(4, 192) Loss: 0.3165619373321533
(4, 384) Loss: 0.35520029067993164
(4, 576) Loss: 0.31332218647003174
(4, 768) Loss: 0.39729663729667664
(4, 960) Loss: 0.3842596709728241
(4, 1152) Loss: 0.34439030289649963
(4, 1344) Loss: 0.37516239285469055
(4, 1536) Loss: 0.37580013275146484
(4, 1728) Loss: 0.34438246488571167
(4, 1920) Loss: 0.3133618235588074
(4, 2112) Loss: 0.31332075595855713
(4, 2304) Loss: 0.3667539358139038
(4, 2496) Loss: 0.3445751965045929
(4, 2688) Loss: 0.3161059319972992
(4, 2880) Loss: 0.34446030855178833
(4, 3072) Loss: 0.31330591440200806
(4, 3264) Loss: 0.3133048713207245
(4, 3456) Loss: 0.34548312425613403
(4, 3648) Loss: 0.313315749168396
(4, 3840) Loss: 0.3132998049259186
(4, 4032) Loss: 0.36470675468444824
(4, 4224) Loss: 0.37579330801963806
(4, 4416) Loss: 0.3445878028869629
(4, 4608) Loss: 0.3460976779460907
(4, 4800) Loss: 0.34605079889297485
(4, 4992) Loss: 0.34459900856018066
(4, 5184) Loss: 0.344560444355011
(4, 5376) Loss: 0.3435685932636261
(4, 5568) Loss: 0.4364140033721924
(4, 5760) Loss: 0.3445776700973511
(4, 5952) Loss: 0.34957724809646606
(4, 6144) Loss: 0.3431895971298218
(4, 6336) Loss: 0.3383347988128662
Evaluation metrics:
	f1: {'f1': 0.9366834170854271}
	accuracy: {'accuracy': 0.92125}

Saving model to src/models/neural_ensemble/neural_ensemble/humor/...

real	135m39.350s
user	118m59.832s
sys	9m14.146s

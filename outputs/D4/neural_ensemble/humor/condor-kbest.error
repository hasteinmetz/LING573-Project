Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 144) Loss: 0.6463082606593767
(0, 288) Loss: 0.6714638769626617
(0, 432) Loss: 0.6701365535457928
(0, 576) Loss: 0.6615183874964714
(0, 720) Loss: 0.49010090405742324
(0, 864) Loss: 0.6305862565835316
(0, 1008) Loss: 0.7385832940538724
(0, 1152) Loss: 0.6070919545988241
(0, 1296) Loss: 0.6634924970567226
(0, 1440) Loss: 0.5723556218047936
(0, 1584) Loss: 0.4319574137528737
(0, 1728) Loss: 0.5260178508857886
(0, 1872) Loss: 0.3653765904406706

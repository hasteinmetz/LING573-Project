Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6795071363449097
(0, 384) Loss: 0.6853571534156799
(0, 576) Loss: 0.7356343865394592
(0, 768) Loss: 0.6027780175209045
(0, 960) Loss: 0.5459680557250977
(0, 1152) Loss: 0.41645166277885437
(0, 1344) Loss: 0.48194634914398193
(0, 1536) Loss: 0.5089188814163208
(0, 1728) Loss: 0.4459246098995209
(0, 1920) Loss: 0.4887245297431946
(0, 2112) Loss: 0.41206878423690796
(0, 2304) Loss: 0.3988419771194458
(0, 2496) Loss: 0.38299360871315
(0, 2688) Loss: 0.40431684255599976
(0, 2880) Loss: 0.41564568877220154
(0, 3072) Loss: 0.38045817613601685
(0, 3264) Loss: 0.32254528999328613
(0, 3456) Loss: 0.4228995144367218
(0, 3648) Loss: 0.3883475661277771
(0, 3840) Loss: 0.3606840968132019
(0, 4032) Loss: 0.4045364260673523
(0, 4224) Loss: 0.42100459337234497
(0, 4416) Loss: 0.3638639450073242
(0, 4608) Loss: 0.3742797374725342
(0, 4800) Loss: 0.42210453748703003
(0, 4992) Loss: 0.3815133571624756
(0, 5184) Loss: 0.4728608727455139
(0, 5376) Loss: 0.4397396445274353
(0, 5568) Loss: 0.40508949756622314
(0, 5760) Loss: 0.39319196343421936
(0, 5952) Loss: 0.3427344262599945
(0, 6144) Loss: 0.34335944056510925
(0, 6336) Loss: 0.3872169256210327
Evaluation metrics:
	f1: {'f1': 0.944}
	accuracy: {'accuracy': 0.93}

(1, 192) Loss: 0.3453417718410492
(1, 384) Loss: 0.40933483839035034
(1, 576) Loss: 0.3448238968849182
(1, 768) Loss: 0.4011719822883606
(1, 960) Loss: 0.34588146209716797
(1, 1152) Loss: 0.38465172052383423
(1, 1344) Loss: 0.3610135316848755
(1, 1536) Loss: 0.3795773684978485
(1, 1728) Loss: 0.37589824199676514
(1, 1920) Loss: 0.3694065809249878
(1, 2112) Loss: 0.3647118806838989
(1, 2304) Loss: 0.3656116724014282
(1, 2496) Loss: 0.3278236389160156
(1, 2688) Loss: 0.3293096721172333
(1, 2880) Loss: 0.36261534690856934
(1, 3072) Loss: 0.37630361318588257
(1, 3264) Loss: 0.31355124711990356
(1, 3456) Loss: 0.3448408842086792
(1, 3648) Loss: 0.3536122143268585
(1, 3840) Loss: 0.3437585234642029
(1, 4032) Loss: 0.3485558032989502
(1, 4224) Loss: 0.35823187232017517
(1, 4416) Loss: 0.34779250621795654
(1, 4608) Loss: 0.3761748969554901
(1, 4800) Loss: 0.40618589520454407
(1, 4992) Loss: 0.3453100919723511
(1, 5184) Loss: 0.3752726912498474
(1, 5376) Loss: 0.38552361726760864
(1, 5568) Loss: 0.39414915442466736
(1, 5760) Loss: 0.47457513213157654
(1, 5952) Loss: 0.3139041066169739
(1, 6144) Loss: 0.3739672303199768
(1, 6336) Loss: 0.4193132519721985
Evaluation metrics:
	f1: {'f1': 0.9381237524950099}
	accuracy: {'accuracy': 0.9225}

(2, 192) Loss: 0.38796481490135193
(2, 384) Loss: 0.38884395360946655
(2, 576) Loss: 0.31355947256088257
(2, 768) Loss: 0.3460146188735962
(2, 960) Loss: 0.3437613844871521
(2, 1152) Loss: 0.36158713698387146
(2, 1344) Loss: 0.4014453887939453
(2, 1536) Loss: 0.34743577241897583
(2, 1728) Loss: 0.34568387269973755
(2, 1920) Loss: 0.37497466802597046
(2, 2112) Loss: 0.3445811867713928
(2, 2304) Loss: 0.34893798828125
(2, 2496) Loss: 0.3182598948478699
(2, 2688) Loss: 0.3134131133556366
(2, 2880) Loss: 0.3520992398262024
(2, 3072) Loss: 0.37584802508354187
(2, 3264) Loss: 0.31335824728012085
(2, 3456) Loss: 0.3446384072303772
(2, 3648) Loss: 0.35520297288894653
(2, 3840) Loss: 0.34395912289619446
(2, 4032) Loss: 0.3604898154735565
(2, 4224) Loss: 0.375740110874176
(2, 4416) Loss: 0.3635517358779907
(2, 4608) Loss: 0.3522241711616516
(2, 4800) Loss: 0.31348687410354614
(2, 4992) Loss: 0.40821021795272827
(2, 5184) Loss: 0.3447227478027344
(2, 5376) Loss: 0.3136858344078064
(2, 5568) Loss: 0.37580376863479614
(2, 5760) Loss: 0.4071543216705322
(2, 5952) Loss: 0.3446284532546997
(2, 6144) Loss: 0.3136577904224396
(2, 6336) Loss: 0.34809672832489014
Evaluation metrics:
	f1: {'f1': 0.934108527131783}
	accuracy: {'accuracy': 0.915}

(3, 192) Loss: 0.31348925828933716
(3, 384) Loss: 0.3562026619911194
(3, 576) Loss: 0.34746095538139343
(3, 768) Loss: 0.37945497035980225
(3, 960) Loss: 0.3260953426361084
(3, 1152) Loss: 0.3446395993232727
(3, 1344) Loss: 0.38762593269348145
(3, 1536) Loss: 0.3753460645675659
(3, 1728) Loss: 0.34149491786956787
(3, 1920) Loss: 0.3750673532485962
(3, 2112) Loss: 0.33591240644454956
(3, 2304) Loss: 0.3753049075603485
(3, 2496) Loss: 0.35257089138031006
(3, 2688) Loss: 0.31560811400413513
(3, 2880) Loss: 0.34455299377441406
(3, 3072) Loss: 0.3474920392036438
(3, 3264) Loss: 0.3133290410041809
(3, 3456) Loss: 0.34549376368522644
(3, 3648) Loss: 0.37508273124694824
(3, 3840) Loss: 0.3757133483886719
(3, 4032) Loss: 0.3765144944190979
(3, 4224) Loss: 0.344582736492157
(3, 4416) Loss: 0.34005171060562134
(3, 4608) Loss: 0.3536277711391449
(3, 4800) Loss: 0.34439533948898315
(3, 4992) Loss: 0.49516189098358154
(3, 5184) Loss: 0.3273777365684509
(3, 5376) Loss: 0.3137645721435547
(3, 5568) Loss: 0.3751932382583618
(3, 5760) Loss: 0.4065215587615967
(3, 5952) Loss: 0.31336480379104614
(3, 6144) Loss: 0.37228846549987793
(3, 6336) Loss: 0.3133307695388794
Evaluation metrics:
	f1: {'f1': 0.9156883671291356}
	accuracy: {'accuracy': 0.90125}

Saving model to src/models/neural_ensemble/neural_ensemble/humor/...
Traceback (most recent call last):
  File "/home2/hsteinm/573/repo/src/neural_ensemble.py", line 339, in <module>
    main(args)
  File "/home2/hsteinm/573/repo/src/neural_ensemble.py", line 285, in main
    test_out_d = {'sentence': dev_sentences, 'predicted': preds, 'transformer': robs, 'featurizer': feats, 'correct_label': dev_labels}
NameError: name 'dev_sentences' is not defined

real	114m29.237s
user	106m15.987s
sys	7m24.044s

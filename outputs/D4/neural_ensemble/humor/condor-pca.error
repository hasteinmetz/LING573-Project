Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6718025207519531
(0, 384) Loss: 0.6887525320053101
(0, 576) Loss: 0.7219254970550537
(0, 768) Loss: 0.6190127730369568
(0, 960) Loss: 0.5676056146621704
(0, 1152) Loss: 0.528382420539856
(0, 1344) Loss: 0.5100125670433044
(0, 1536) Loss: 0.3848141133785248
(0, 1728) Loss: 0.4239375591278076
(0, 1920) Loss: 0.4213692247867584
(0, 2112) Loss: 0.3954877555370331
(0, 2304) Loss: 0.6187158823013306
(0, 2496) Loss: 0.3935353755950928
(0, 2688) Loss: 0.3581024706363678
(0, 2880) Loss: 0.4110276997089386
(0, 3072) Loss: 0.4282327890396118
(0, 3264) Loss: 0.3377954959869385
(0, 3456) Loss: 0.41227883100509644
(0, 3648) Loss: 0.4163970947265625
(0, 3840) Loss: 0.35095512866973877
(0, 4032) Loss: 0.40539056062698364
(0, 4224) Loss: 0.4286165237426758
(0, 4416) Loss: 0.411609947681427
(0, 4608) Loss: 0.3937316834926605
(0, 4800) Loss: 0.40537241101264954
(0, 4992) Loss: 0.3743732273578644
(0, 5184) Loss: 0.3525097072124481
(0, 5376) Loss: 0.44224393367767334
(0, 5568) Loss: 0.35363247990608215
(0, 5760) Loss: 0.3669936954975128
(0, 5952) Loss: 0.31548061966896057
(0, 6144) Loss: 0.32635751366615295
(0, 6336) Loss: 0.4323185980319977
Evaluation metrics:
	f1: {'f1': 0.9233954451345756}
	accuracy: {'accuracy': 0.9075}

(1, 192) Loss: 0.4394413232803345
(1, 384) Loss: 0.4083840250968933
(1, 576) Loss: 0.39573395252227783
(1, 768) Loss: 0.4721735417842865
(1, 960) Loss: 0.34679436683654785
(1, 1152) Loss: 0.36328765749931335
(1, 1344) Loss: 0.4348849654197693
(1, 1536) Loss: 0.3743647336959839
(1, 1728) Loss: 0.37261760234832764
(1, 1920) Loss: 0.33669859170913696
(1, 2112) Loss: 0.3476871848106384
(1, 2304) Loss: 0.3723915219306946
(1, 2496) Loss: 0.3419768810272217
(1, 2688) Loss: 0.38653331995010376
(1, 2880) Loss: 0.3343842625617981
(1, 3072) Loss: 0.3418256342411041
(1, 3264) Loss: 0.3138361871242523
(1, 3456) Loss: 0.36272382736206055
(1, 3648) Loss: 0.37122613191604614
(1, 3840) Loss: 0.34688374400138855
(1, 4032) Loss: 0.37724733352661133
(1, 4224) Loss: 0.3912643790245056
(1, 4416) Loss: 0.4246120750904083
(1, 4608) Loss: 0.36630502343177795
(1, 4800) Loss: 0.40783554315567017
(1, 4992) Loss: 0.41457265615463257
(1, 5184) Loss: 0.3538512587547302
(1, 5376) Loss: 0.3741268515586853
(1, 5568) Loss: 0.3645234704017639
(1, 5760) Loss: 0.36900460720062256
(1, 5952) Loss: 0.3742503225803375
(1, 6144) Loss: 0.3481999337673187
(1, 6336) Loss: 0.34951916337013245
Evaluation metrics:
	f1: {'f1': 0.9285714285714286}
	accuracy: {'accuracy': 0.91}

(2, 192) Loss: 0.34481680393218994
(2, 384) Loss: 0.5161224007606506
(2, 576) Loss: 0.35792434215545654
(2, 768) Loss: 0.40236106514930725
(2, 960) Loss: 0.3320358395576477
(2, 1152) Loss: 0.3270360827445984
(2, 1344) Loss: 0.4030403792858124
(2, 1536) Loss: 0.3453937768936157
(2, 1728) Loss: 0.3450688421726227
(2, 1920) Loss: 0.34552085399627686
(2, 2112) Loss: 0.34544286131858826
(2, 2304) Loss: 0.3966805338859558
(2, 2496) Loss: 0.3145080804824829
(2, 2688) Loss: 0.3769368529319763
(2, 2880) Loss: 0.3197330832481384
(2, 3072) Loss: 0.31365442276000977
(2, 3264) Loss: 0.3152680993080139
(2, 3456) Loss: 0.31342267990112305
(2, 3648) Loss: 0.3522862195968628
(2, 3840) Loss: 0.34694528579711914
(2, 4032) Loss: 0.3850363790988922
(2, 4224) Loss: 0.3758939802646637
(2, 4416) Loss: 0.34054169058799744
(2, 4608) Loss: 0.3137892782688141
(2, 4800) Loss: 0.4082137942314148
(2, 4992) Loss: 0.34456315636634827
(2, 5184) Loss: 0.3416706323623657
(2, 5376) Loss: 0.3477761745452881
(2, 5568) Loss: 0.3451010584831238
(2, 5760) Loss: 0.37586361169815063
(2, 5952) Loss: 0.35225439071655273
(2, 6144) Loss: 0.31624260544776917
(2, 6336) Loss: 0.3875906467437744
Evaluation metrics:
	f1: {'f1': 0.9405940594059407}
	accuracy: {'accuracy': 0.925}

(3, 192) Loss: 0.344444215297699
(3, 384) Loss: 0.35634100437164307
(3, 576) Loss: 0.40015625953674316
(3, 768) Loss: 0.3989197909832001
(3, 960) Loss: 0.319056898355484
(3, 1152) Loss: 0.3732411861419678
(3, 1344) Loss: 0.425397127866745
(3, 1536) Loss: 0.37657061219215393
(3, 1728) Loss: 0.3712501525878906
(3, 1920) Loss: 0.3159554600715637
(3, 2112) Loss: 0.3470726013183594
(3, 2304) Loss: 0.34467047452926636
(3, 2496) Loss: 0.31346407532691956
(3, 2688) Loss: 0.31332457065582275
(3, 2880) Loss: 0.34480252861976624
(3, 3072) Loss: 0.35034409165382385
(3, 3264) Loss: 0.3133048415184021
(3, 3456) Loss: 0.34803593158721924
(3, 3648) Loss: 0.3716661036014557
(3, 3840) Loss: 0.31333211064338684
(3, 4032) Loss: 0.3141522705554962
(3, 4224) Loss: 0.344585120677948
(3, 4416) Loss: 0.3696334958076477
(3, 4608) Loss: 0.31456172466278076
(3, 4800) Loss: 0.35847869515419006
(3, 4992) Loss: 0.3439677357673645
(3, 5184) Loss: 0.3137827515602112
(3, 5376) Loss: 0.34467601776123047
(3, 5568) Loss: 0.34470969438552856
(3, 5760) Loss: 0.34491637349128723
(3, 5952) Loss: 0.31400731205940247
(3, 6144) Loss: 0.33371245861053467
(3, 6336) Loss: 0.34506312012672424
Evaluation metrics:
	f1: {'f1': 0.9476861167002012}
	accuracy: {'accuracy': 0.935}

Saving model to src/models/neural_ensemble/neural_ensemble/humor/...
Traceback (most recent call last):
  File "/home2/hsteinm/573/repo/src/neural_ensemble.py", line 339, in <module>
    main(args)
  File "/home2/hsteinm/573/repo/src/neural_ensemble.py", line 285, in main
    test_out_d = {'sentence': dev_sentences, 'predicted': preds, 'transformer': robs, 'featurizer': feats, 'correct_label': dev_labels}
NameError: name 'dev_sentences' is not defined

real	119m48.789s
user	110m15.697s
sys	8m29.279s

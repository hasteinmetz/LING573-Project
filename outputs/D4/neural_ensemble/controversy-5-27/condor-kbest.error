Using the GPU:NVIDIA Quadro RTX 8000
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.7055902276188135
(0, 384) Loss: 0.709941890090704
(0, 576) Loss: 0.6956387124955654
(0, 768) Loss: 0.6911128256469965
(0, 960) Loss: 0.6948383655399084
(0, 1152) Loss: 0.6929268874228001
(0, 1344) Loss: 0.6910699922591448
(0, 1536) Loss: 0.703467819839716
(0, 1728) Loss: 0.6957350932061672
(0, 1920) Loss: 0.6865970734506845
(0, 2112) Loss: 0.6833928897976875
(0, 2304) Loss: 0.6920843049883842
(0, 2496) Loss: 0.6927642971277237
(0, 2688) Loss: 0.697588475421071
(0, 2880) Loss: 0.697122422978282
(0, 3072) Loss: 0.6899766325950623
(0, 3264) Loss: 0.6946688517928123
(0, 3456) Loss: 0.6921098008751869
(0, 3648) Loss: 0.6961017586290836
(0, 3840) Loss: 0.6922518312931061
Evaluation metrics:
	f1: {'f1': 0.023809523809523808}
	accuracy: {'accuracy': 0.5010141987829615}

(1, 192) Loss: 0.6809392422437668
(1, 384) Loss: 0.6786827724426985
(1, 576) Loss: 0.6999553628265858
(1, 768) Loss: 0.6944722197949886
(1, 960) Loss: 0.6939528621733189
(1, 1152) Loss: 0.6894096564501524
(1, 1344) Loss: 0.6929563973098993
(1, 1536) Loss: 0.7036472074687481
(1, 1728) Loss: 0.6910808123648167
(1, 1920) Loss: 0.67987853102386
(1, 2112) Loss: 0.6424119370058179
(1, 2304) Loss: 0.6988869355991483
(1, 2496) Loss: 0.7027854090556502
(1, 2688) Loss: 0.6969588026404381
(1, 2880) Loss: 0.7213486153632402
(1, 3072) Loss: 0.7001164928078651
(1, 3264) Loss: 0.6895793564617634
(1, 3456) Loss: 0.6925583053380251
(1, 3648) Loss: 0.6924223080277443
(1, 3840) Loss: 0.6906375382095575
Evaluation metrics:
	f1: {'f1': 0.11851851851851852}
	accuracy: {'accuracy': 0.5172413793103449}

(2, 192) Loss: 0.6847344320267439
(2, 384) Loss: 0.6818864718079567
(2, 576) Loss: 0.7005542442202568
(2, 768) Loss: 0.6954810358583927
(2, 960) Loss: 0.6963146459311247
(2, 1152) Loss: 0.6874279454350471
(2, 1344) Loss: 0.6854096576571465
(2, 1536) Loss: 0.6968858744949102
(2, 1728) Loss: 0.6842319900169969
(2, 1920) Loss: 0.6616345960646868
(2, 2112) Loss: 0.65582309756428
(2, 2304) Loss: 0.6871608719229698
(2, 2496) Loss: 0.6416834620758891
(2, 2688) Loss: 0.6879761423915625
(2, 2880) Loss: 0.6911261444911361
(2, 3072) Loss: 0.6879213647916913
(2, 3264) Loss: 0.6747276429086924
(2, 3456) Loss: 0.6694553541019559
(2, 3648) Loss: 0.6808533072471619
(2, 3840) Loss: 0.7097602393478155
Evaluation metrics:
	f1: {'f1': 0.06976744186046512}
	accuracy: {'accuracy': 0.513184584178499}

(3, 192) Loss: 0.6719645289704204
(3, 384) Loss: 0.6581134237349033
(3, 576) Loss: 0.6730367634445429
(3, 768) Loss: 0.7064653662964702
(3, 960) Loss: 0.6521173426881433
(3, 1152) Loss: 0.6349448086693883
(3, 1344) Loss: 0.6748004062101245
(3, 1536) Loss: 0.6742779724299908
(3, 1728) Loss: 0.6529372120276093
(3, 1920) Loss: 0.5901936404407024
(3, 2112) Loss: 0.5831078914925456
(3, 2304) Loss: 0.6653241263702512
(3, 2496) Loss: 0.5685849701985717
(3, 2688) Loss: 0.661153725348413
(3, 2880) Loss: 0.5508345179259777
(3, 3072) Loss: 0.5794546063989401
(3, 3264) Loss: 0.5227078422904015
(3, 3456) Loss: 0.5050346134230494
(3, 3648) Loss: 0.5738612981513143
(3, 3840) Loss: 0.6865993579849601
Evaluation metrics:
	f1: {'f1': 0.5185185185185185}
	accuracy: {'accuracy': 0.5253549695740365}

Saving model to src/models/neural_ensemble/neural_ensemble/controversy/...

real	36m20.431s
user	34m41.114s
sys	0m42.769s

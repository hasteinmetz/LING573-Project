Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.708954069763422
(0, 384) Loss: 0.713369645178318
(0, 576) Loss: 0.6969887521117926
(0, 768) Loss: 0.6949584558606148
(0, 960) Loss: 0.6959016565233469
(0, 1152) Loss: 0.6941708773374557
(0, 1344) Loss: 0.6971176788210869
(0, 1536) Loss: 0.698345148935914
(0, 1728) Loss: 0.6927781030535698
(0, 1920) Loss: 0.6923421006649733
(0, 2112) Loss: 0.6906950585544109
(0, 2304) Loss: 0.70492048189044
(0, 2496) Loss: 0.6827087681740522
(0, 2688) Loss: 0.6890359614044428
(0, 2880) Loss: 0.697647213935852
(0, 3072) Loss: 0.6955366153270006
(0, 3264) Loss: 0.6887520775198936
(0, 3456) Loss: 0.692415988072753
(0, 3648) Loss: 0.6869671549648046
(0, 3840) Loss: 0.6981017179787159
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(1, 192) Loss: 0.6817280724644661
(1, 384) Loss: 0.6784180756658316
(1, 576) Loss: 0.6921064294874668
(1, 768) Loss: 0.6875668037682772
(1, 960) Loss: 0.6720818281173706
(1, 1152) Loss: 0.6529158242046833
(1, 1344) Loss: 0.6868782453238964
(1, 1536) Loss: 0.6847165916115046
(1, 1728) Loss: 0.6900245249271393
(1, 1920) Loss: 0.6490295808762312
(1, 2112) Loss: 0.6592452693730593
(1, 2304) Loss: 0.6956879422068596
(1, 2496) Loss: 0.687116963788867
(1, 2688) Loss: 0.6923591569066048
(1, 2880) Loss: 0.6964834742248058
(1, 3072) Loss: 0.6932090595364571
(1, 3264) Loss: 0.6910641137510538
(1, 3456) Loss: 0.6928205005824566
(1, 3648) Loss: 0.6921588778495789
(1, 3840) Loss: 0.6921371631324291
Evaluation metrics:
	f1: {'f1': 0.30994152046783624}
	accuracy: {'accuracy': 0.5212981744421906}

(2, 192) Loss: 0.6881262231618166
(2, 384) Loss: 0.6826868653297424
(2, 576) Loss: 0.6962361615151167
(2, 768) Loss: 0.6940182708203793
(2, 960) Loss: 0.6922572460025549
(2, 1152) Loss: 0.6817942094057798
(2, 1344) Loss: 0.6835405323654413
(2, 1536) Loss: 0.694437307305634
(2, 1728) Loss: 0.6725361524149776
(2, 1920) Loss: 0.6104022087529302
(2, 2112) Loss: 0.6349690584465861
(2, 2304) Loss: 0.6963692717254162
(2, 2496) Loss: 0.6892160233110189
(2, 2688) Loss: 0.6880141934379935
(2, 2880) Loss: 0.6844651559367776
(2, 3072) Loss: 0.6935286913067102
(2, 3264) Loss: 0.691256346181035
(2, 3456) Loss: 0.6933132652193308
(2, 3648) Loss: 0.6935225278139114
(2, 3840) Loss: 0.693116320297122
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(3, 192) Loss: 0.6796692386269569
(3, 384) Loss: 0.6819710806012154
(3, 576) Loss: 0.7053351793438196
(3, 768) Loss: 0.6977608166635036
(3, 960) Loss: 0.70140883885324
(3, 1152) Loss: 0.6934393905103207
(3, 1344) Loss: 0.6932417713105679
(3, 1536) Loss: 0.7008810415863991
(3, 1728) Loss: 0.6908919829875231
(3, 1920) Loss: 0.6937446165829897
(3, 2112) Loss: 0.6923464555293322
(3, 2304) Loss: 0.6928327437490225
(3, 2496) Loss: 0.6908824034035206
(3, 2688) Loss: 0.6920043807476759
(3, 2880) Loss: 0.69698266685009
(3, 3072) Loss: 0.6960495226085186
(3, 3264) Loss: 0.6908945553004742
(3, 3456) Loss: 0.6890177447348833
(3, 3648) Loss: 0.688490454107523
(3, 3840) Loss: 0.695635300129652
Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4949290060851927}

Saving model to src/models/neural_ensemble/neural_ensemble/controversy/...

real	62m0.064s
user	55m36.853s
sys	5m10.812s

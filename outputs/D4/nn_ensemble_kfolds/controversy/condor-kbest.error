Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
	(epoch 1, fold 1, samples 320) FClassifier Loss: 0.697373528033495 Transformer Loss: 0.7062971421401016
	(epoch 1, fold 1, samples 640) FClassifier Loss: 0.6873893346637487 Transformer Loss: 0.7107256901681467
	(epoch 1, fold 1, samples 960) FClassifier Loss: 0.6924481075257063 Transformer Loss: 0.6963437317845091
	(epoch 1, fold 1, samples 1280) FClassifier Loss: 0.6904492769390345 Transformer Loss: 0.6950623318989528
	(epoch 1, fold 1, samples 1600) FClassifier Loss: 0.6921822410076857 Transformer Loss: 0.6931587094567249
	(epoch 1, fold 1, samples 1920) FClassifier Loss: 0.6980005819350481 Transformer Loss: 0.6989116119220853
	(epoch 1, fold 1, samples 2240) FClassifier Loss: 0.6894642468541861 Transformer Loss: 0.6944359370063466
	(epoch 1, fold 1, samples 2560) FClassifier Loss: 0.6899516824632883 Transformer Loss: 0.6910762113116107
(epoch 1, fold 1, samples 640) Regression Accuracy: 0.40625, Loss: 0.7228091955184937
(epoch 1, fold 1, samples 1280) Regression Accuracy: 0.4375, Loss: 0.7113646268844604
Fold 0 accuracies:
Ensemble	f1: {'f1': 0.0}
Ensemble	accuracy: {'accuracy': 0.4969574036511156}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.0}
Featurizer	accuracy: {'accuracy': 0.4969574036511156}

	(epoch 1, fold 2, samples 320) FClassifier Loss: 0.6899188309907913 Transformer Loss: 0.6893923917668872
	(epoch 1, fold 2, samples 640) FClassifier Loss: 0.6953787468373775 Transformer Loss: 0.6921412028968916
	(epoch 1, fold 2, samples 960) FClassifier Loss: 0.6921213120222092 Transformer Loss: 0.6806249036380905
	(epoch 1, fold 2, samples 1280) FClassifier Loss: 0.689340628683567 Transformer Loss: 0.6922996605135268
	(epoch 1, fold 2, samples 1600) FClassifier Loss: 0.6948359739035368 Transformer Loss: 0.7200062690826599
	(epoch 1, fold 2, samples 1920) FClassifier Loss: 0.692764388397336 Transformer Loss: 0.705661548046919
	(epoch 1, fold 2, samples 2240) FClassifier Loss: 0.6944334525614977 Transformer Loss: 0.6979852729855338
	(epoch 1, fold 2, samples 2560) FClassifier Loss: 0.6900631990283728 Transformer Loss: 0.6998061247795704
(epoch 1, fold 2, samples 640) Regression Accuracy: 0.5625, Loss: 0.6929739713668823
(epoch 1, fold 2, samples 1280) Regression Accuracy: 0.5, Loss: 0.6928835511207581
Fold 1 accuracies:
Ensemble	f1: {'f1': 0.0}
Ensemble	accuracy: {'accuracy': 0.4969574036511156}

Transformer	f1: {'f1': 0.0}
Transformer	accuracy: {'accuracy': 0.4969574036511156}

Featurizer	f1: {'f1': 0.0}
Featurizer	accuracy: {'accuracy': 0.4969574036511156}

	(epoch 1, fold 3, samples 320) FClassifier Loss: 0.6854962091892958 Transformer Loss: 0.6897378433786798
	(epoch 1, fold 3, samples 640) FClassifier Loss: 0.7019637934863567 Transformer Loss: 0.6933432063678993
	(epoch 1, fold 3, samples 960) FClassifier Loss: 0.6954676602035761 Transformer Loss: 0.6964899210433941
	(epoch 1, fold 3, samples 1280) FClassifier Loss: 0.6961406841874123 Transformer Loss: 0.683913545566611
	(epoch 1, fold 3, samples 1600) FClassifier Loss: 0.6946655083447695 Transformer Loss: 0.6868496642164246
	(epoch 1, fold 3, samples 1920) FClassifier Loss: 0.691563805565238 Transformer Loss: 0.6961927619413473
	(epoch 1, fold 3, samples 2240) FClassifier Loss: 0.6921877339482307 Transformer Loss: 0.6983297075421433
	(epoch 1, fold 3, samples 2560) FClassifier Loss: 0.6922938488423824 Transformer Loss: 0.6931041204406938

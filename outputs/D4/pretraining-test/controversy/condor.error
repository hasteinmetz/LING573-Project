Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 240) Loss: 0.471701443195343
(0, 480) Loss: 0.38723406195640564
(0, 720) Loss: 0.419018417596817
(0, 960) Loss: 0.32034093141555786
(0, 1200) Loss: 0.30869150161743164
(0, 1440) Loss: 0.2945563495159149
(0, 1680) Loss: 0.33263346552848816
(0, 1920) Loss: 0.28959497809410095
(0, 2160) Loss: 0.33681342005729675
(0, 2400) Loss: 0.34107041358947754
(0, 2640) Loss: 0.5389108657836914
(0, 2880) Loss: 0.30002981424331665
(0, 3120) Loss: 0.31570830941200256
(0, 3360) Loss: 0.3508272171020508
(0, 3600) Loss: 0.32381778955459595
(0, 3840) Loss: 0.3001926839351654
Training metrics:
	mse: {'mse': 0.4559261677639168}

Evaluation metrics:
	mse: {'mse': 0.30620515044865726}

(1, 240) Loss: 0.3038063049316406
(1, 480) Loss: 0.29887476563453674
(1, 720) Loss: 0.2550249695777893
(1, 960) Loss: 0.30400797724723816
(1, 1200) Loss: 0.24633410573005676
(1, 1440) Loss: 0.1945333480834961
(1, 1680) Loss: 0.22598837316036224
(1, 1920) Loss: 0.28686028718948364
(1, 2160) Loss: 0.26884937286376953
(1, 2400) Loss: 0.27420029044151306
(1, 2640) Loss: 0.5194008350372314
(1, 2880) Loss: 0.17428772151470184
(1, 3120) Loss: 0.23737601935863495
(1, 3360) Loss: 0.24860437214374542
(1, 3600) Loss: 0.28829988837242126
(1, 3840) Loss: 0.23709921538829803
Training metrics:
	mse: {'mse': 0.25934395551457534}

Evaluation metrics:
	mse: {'mse': 0.27479967436479946}

(2, 240) Loss: 0.2719489634037018
(2, 480) Loss: 0.16325055062770844
(2, 720) Loss: 0.14174215495586395
(2, 960) Loss: 0.1900719255208969
(2, 1200) Loss: 0.18980161845684052
(2, 1440) Loss: 0.11523568630218506
(2, 1680) Loss: 0.13017626106739044
(2, 1920) Loss: 0.14316287636756897
(2, 2160) Loss: 0.12720349431037903
(2, 2400) Loss: 0.15656384825706482
(2, 2640) Loss: 0.25370436906814575
(2, 2880) Loss: 0.16196362674236298
(2, 3120) Loss: 0.20986716449260712
(2, 3360) Loss: 0.1162416860461235
(2, 3600) Loss: 0.25075170397758484
(2, 3840) Loss: 0.16092465817928314
Training metrics:
	mse: {'mse': 0.17366033324934416}

Evaluation metrics:
	mse: {'mse': 0.278666673963624}

(3, 240) Loss: 0.11039195209741592
(3, 480) Loss: 0.10313596576452255
(3, 720) Loss: 0.055823005735874176
(3, 960) Loss: 0.15407077968120575
(3, 1200) Loss: 0.085080586373806
(3, 1440) Loss: 0.15090425312519073
(3, 1680) Loss: 0.43702608346939087
(3, 1920) Loss: 0.2732313573360443
(3, 2160) Loss: 0.21032750606536865
(3, 2400) Loss: 0.11329515278339386
(3, 2640) Loss: 0.1393464356660843
(3, 2880) Loss: 0.17389117181301117
(3, 3120) Loss: 0.13086611032485962
(3, 3360) Loss: 0.15095241367816925
(3, 3600) Loss: 0.2592482566833496
(3, 3840) Loss: 0.27224069833755493
Training metrics:
	mse: {'mse': 0.1498643104429431}

Evaluation metrics:
	mse: {'mse': 0.3655244218319783}

(4, 240) Loss: 0.13259661197662354
(4, 480) Loss: 0.14247411489486694
(4, 720) Loss: 0.058015741407871246
(4, 960) Loss: 0.10420208424329758
(4, 1200) Loss: 0.04394667223095894
(4, 1440) Loss: 0.302396684885025
(4, 1680) Loss: 0.12871085107326508
(4, 1920) Loss: 0.12673287093639374
(4, 2160) Loss: 0.13353636860847473
(4, 2400) Loss: 0.09562838822603226
(4, 2640) Loss: 0.05668303370475769
(4, 2880) Loss: 0.08191092312335968
(4, 3120) Loss: 0.09259501844644547
(4, 3360) Loss: 0.044364552944898605
(4, 3600) Loss: 0.13829770684242249
(4, 3840) Loss: 0.3501027524471283
Training metrics:
	mse: {'mse': 0.12156201570755114}

Evaluation metrics:
	mse: {'mse': 0.2894536301559127}

(5, 240) Loss: 0.21462340652942657
(5, 480) Loss: 0.08916439861059189
(5, 720) Loss: 0.033748991787433624
(5, 960) Loss: 0.033721476793289185
(5, 1200) Loss: 0.03980996832251549
(5, 1440) Loss: 0.02506047487258911
(5, 1680) Loss: 0.08495926856994629
(5, 1920) Loss: 0.06901901960372925
(5, 2160) Loss: 0.06319274753332138
(5, 2400) Loss: 0.23465989530086517
(5, 2640) Loss: 0.2238730490207672
(5, 2880) Loss: 0.1692429780960083
(5, 3120) Loss: 0.12607866525650024
(5, 3360) Loss: 0.09731075912714005
(5, 3600) Loss: 0.04214640334248543
(5, 3840) Loss: 0.14745235443115234
Training metrics:
	mse: {'mse': 0.10258856864212218}

Evaluation metrics:
	mse: {'mse': 0.5980130614034176}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
(0, 240) Loss: 0.7375319287180901
(0, 480) Loss: 0.7959726780653
(0, 720) Loss: 0.7027027666568757
(0, 960) Loss: 0.6730774752795696
(0, 1200) Loss: 0.6570767335593701
(0, 1440) Loss: 0.6988092601299286
(0, 1680) Loss: 0.7294114716351032
(0, 1920) Loss: 0.7063959728926421
(0, 2160) Loss: 0.6882777854800225
(0, 2400) Loss: 0.666300155967474
(0, 2640) Loss: 0.6851783402264119
(0, 2880) Loss: 0.6601969949901104
(0, 3120) Loss: 0.7246460922062398
(0, 3360) Loss: 0.648982471600175
(0, 3600) Loss: 0.7068871013820172
(0, 3840) Loss: 0.6852343112230301
Training metrics:
	f1: {'f1': 0.57381349868829}
	accuracy: {'accuracy': 0.54702154626109}

Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(1, 240) Loss: 0.6456515796482564
(1, 480) Loss: 0.5415192537009716
(1, 720) Loss: 0.6129463281482459
(1, 960) Loss: 0.7055908478796482
(1, 1200) Loss: 0.5321599818766117
(1, 1440) Loss: 0.7061852961778641
(1, 1680) Loss: 0.6089509960263968
(1, 1920) Loss: 0.5706445761024952
(1, 2160) Loss: 0.6590125717222691
(1, 2400) Loss: 0.6660332638770342
(1, 2640) Loss: 0.7125573191791773
(1, 2880) Loss: 0.6371478602290154
(1, 3120) Loss: 0.7069263517856599
(1, 3360) Loss: 0.5900777462869883
(1, 3600) Loss: 0.7192282451316715
(1, 3840) Loss: 0.6665373722091318
Training metrics:
	f1: {'f1': 0.6586345381526104}
	accuracy: {'accuracy': 0.6337135614702155}

Evaluation metrics:
	f1: {'f1': 0.5433526011560693}
	accuracy: {'accuracy': 0.5192697768762677}

(2, 240) Loss: 0.5946721613407135
(2, 480) Loss: 0.3961968526244164
(2, 720) Loss: 0.3993517269380391
(2, 960) Loss: 0.28380639194510876
(2, 1200) Loss: 0.20800427907379346
(2, 1440) Loss: 0.42969815658871086
(2, 1680) Loss: 0.5780914052971639
(2, 1920) Loss: 0.43704108924139295
(2, 2160) Loss: 0.5881759536452592
(2, 2400) Loss: 0.5449615580961108
(2, 2640) Loss: 0.32792052011936906
(2, 2880) Loss: 0.5057343660853804
(2, 3120) Loss: 0.4696907429024577
(2, 3360) Loss: 0.5613084059208632
(2, 3600) Loss: 0.6547158953268082
(2, 3840) Loss: 0.46535076308064166
Training metrics:
	f1: {'f1': 0.8096836049856184}
	accuracy: {'accuracy': 0.7987325728770596}

Evaluation metrics:
	f1: {'f1': 0.56353591160221}
	accuracy: {'accuracy': 0.5192697768762677}

(3, 240) Loss: 0.32564955623820424
(3, 480) Loss: 0.30405337903648616
(3, 720) Loss: 0.27735253367573026
(3, 960) Loss: 0.4074622621992603
(3, 1200) Loss: 0.10586094579193742
(3, 1440) Loss: 0.1792697930475697
(3, 1680) Loss: 0.6250263703172095
(3, 1920) Loss: 0.2655644965590909
(3, 2160) Loss: 0.3711750484071672
(3, 2400) Loss: 0.3283437166828662
(3, 2640) Loss: 0.19062971529783682
(3, 2880) Loss: 0.1186021718196571
(3, 3120) Loss: 0.21574413821217606
(3, 3360) Loss: 0.3083619807031937
(3, 3600) Loss: 0.4863203091546893
(3, 3840) Loss: 0.5908978722058237
Training metrics:
	f1: {'f1': 0.8867410161090459}
	accuracy: {'accuracy': 0.8841571609632446}

Evaluation metrics:
	f1: {'f1': 0.512}
	accuracy: {'accuracy': 0.5050709939148073}

(4, 240) Loss: 0.3537832085043192
(4, 480) Loss: 0.22198028694838287
(4, 720) Loss: 0.11155297718942166
(4, 960) Loss: 0.2028644245117903
(4, 1200) Loss: 0.05976474164053798
(4, 1440) Loss: 0.122753977146931
(4, 1680) Loss: 0.27726715519092976
(4, 1920) Loss: 0.5610600688494742
(4, 2160) Loss: 0.3222406747750938
(4, 2400) Loss: 0.47053500916808844
(4, 2640) Loss: 0.20584089090116323
(4, 2880) Loss: 0.16791587818879636
(4, 3120) Loss: 0.3278658311115578
(4, 3360) Loss: 0.11646576061611996
(4, 3600) Loss: 0.3705755657283589
(4, 3840) Loss: 0.3645161755383015
Training metrics:
	f1: {'f1': 0.8929926637996458}
	accuracy: {'accuracy': 0.8927756653992396}

Evaluation metrics:
	f1: {'f1': 0.07885304659498209}
	accuracy: {'accuracy': 0.4787018255578093}

(5, 240) Loss: 0.23437820756807926
(5, 480) Loss: 0.1552205379586667
(5, 720) Loss: 0.27180813658051195
(5, 960) Loss: 0.12932570616248995
(5, 1200) Loss: 0.038524227659218015
(5, 1440) Loss: 0.5111420374189038
(5, 1680) Loss: 0.51964174946188
(5, 1920) Loss: 0.1399262772174552
(5, 2160) Loss: 0.294392687221989
(5, 2400) Loss: 0.16378711331635715
(5, 2640) Loss: 0.23245423524640502
(5, 2880) Loss: 0.19810044054174797
(5, 3120) Loss: 0.05814898984390311
(5, 3360) Loss: 0.20723440704750828
(5, 3600) Loss: 0.28487793781096116
(5, 3840) Loss: 0.22322145280777478
Training metrics:
	f1: {'f1': 0.934560327198364}
	accuracy: {'accuracy': 0.9351077313054499}

Evaluation metrics:
	f1: {'f1': 0.41688654353562}
	accuracy: {'accuracy': 0.5517241379310345}


Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6492125988006592
(0, 384) Loss: 0.33392396569252014
(0, 576) Loss: 0.28680258989334106
(0, 768) Loss: 0.30824342370033264
(0, 960) Loss: 0.38642627000808716
(0, 1152) Loss: 0.31180936098098755
(0, 1344) Loss: 0.22802932560443878
(0, 1536) Loss: 0.3355427384376526
(0, 1728) Loss: 0.4332185983657837
(0, 1920) Loss: 0.33265239000320435
(0, 2112) Loss: 0.26077714562416077
(0, 2304) Loss: 0.34024208784103394
(0, 2496) Loss: 0.20023992657661438
(0, 2688) Loss: 0.44426971673965454
(0, 2880) Loss: 0.2592385709285736
(0, 3072) Loss: 0.39288121461868286
(0, 3264) Loss: 0.1541042923927307
(0, 3456) Loss: 0.3570866584777832
(0, 3648) Loss: 0.40562504529953003
(0, 3840) Loss: 0.2724907100200653
Training:mse:
	 {'mse': 0.46071379960088926}

Evaluation:	mse: {'mse': 0.34979231808266553}

(1, 192) Loss: 0.2962852716445923
(1, 384) Loss: 0.19455647468566895
(1, 576) Loss: 0.24757254123687744
(1, 768) Loss: 0.24127459526062012
(1, 960) Loss: 0.3147050142288208
(1, 1152) Loss: 0.23777762055397034
(1, 1344) Loss: 0.19702690839767456
(1, 1536) Loss: 0.20040878653526306
(1, 1728) Loss: 0.2859717309474945
(1, 1920) Loss: 0.32745444774627686
(1, 2112) Loss: 0.2171093225479126
(1, 2304) Loss: 0.25977545976638794
(1, 2496) Loss: 0.1825939416885376
(1, 2688) Loss: 0.3119017481803894
(1, 2880) Loss: 0.3308268189430237
(1, 3072) Loss: 0.3409082591533661
(1, 3264) Loss: 0.17375415563583374
(1, 3456) Loss: 0.318870484828949
(1, 3648) Loss: 0.3186882436275482
(1, 3840) Loss: 0.23800963163375854
Training:mse:
	 {'mse': 0.2640699700012084}

Evaluation:	mse: {'mse': 0.3706301166068783}

(2, 192) Loss: 0.2285090833902359
(2, 384) Loss: 0.10885203629732132
(2, 576) Loss: 0.11663085222244263
(2, 768) Loss: 0.11450538039207458
(2, 960) Loss: 0.12317784130573273
(2, 1152) Loss: 0.15039169788360596
(2, 1344) Loss: 0.09203994274139404
(2, 1536) Loss: 0.0527280792593956
(2, 1728) Loss: 0.17171815037727356
(2, 1920) Loss: 0.12987202405929565
(2, 2112) Loss: 0.15190249681472778
(2, 2304) Loss: 0.18538570404052734
(2, 2496) Loss: 0.19876927137374878
(2, 2688) Loss: 0.14664633572101593
(2, 2880) Loss: 0.28536558151245117
(2, 3072) Loss: 0.2823708653450012
(2, 3264) Loss: 0.14190830290317535
(2, 3456) Loss: 0.17909258604049683
(2, 3648) Loss: 0.1738036572933197
(2, 3840) Loss: 0.178781658411026
Training:mse:
	 {'mse': 0.1652375316920468}

Evaluation:	mse: {'mse': 0.43944182103095075}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
(0, 192) Loss: 0.5536851352080703
(0, 384) Loss: 0.5144420429132879
(0, 576) Loss: 0.44962457031942904
(0, 768) Loss: 0.3249213945819065
(0, 960) Loss: 0.4635440065758303
(0, 1152) Loss: 0.2647234651376493
(0, 1344) Loss: 0.34753002680372447
(0, 1536) Loss: 0.23603917348373216
(0, 1728) Loss: 0.29364486067788675
(0, 1920) Loss: 0.3629307064693421
(0, 2112) Loss: 0.15853118390077725
(0, 2304) Loss: 0.6206983250158373
(0, 2496) Loss: 0.19297937020019162
(0, 2688) Loss: 0.14905041102610994
(0, 2880) Loss: 0.15007100031652953
(0, 3072) Loss: 0.28630361588147935
(0, 3264) Loss: 0.10981778004497755
(0, 3456) Loss: 0.25343789980979636
(0, 3648) Loss: 0.21829782707209233
(0, 3840) Loss: 0.13087208766955882
(0, 4032) Loss: 0.2364402771127061
(0, 4224) Loss: 0.4227439047244843
(0, 4416) Loss: 0.5394556193205062
(0, 4608) Loss: 0.18010905478149652
(0, 4800) Loss: 0.3072005591238849
(0, 4992) Loss: 0.10800110152922571
(0, 5184) Loss: 0.323528473105398
(0, 5376) Loss: 0.23078768624691293
(0, 5568) Loss: 0.13285236709634773
(0, 5760) Loss: 0.4563591022088076
(0, 5952) Loss: 0.09120745840482414
(0, 6144) Loss: 0.09967106306430651
(0, 6336) Loss: 0.24278440324997064
Training:f1:
	 {'f1': 0.9100661095172758}
accuracy:
	 {'accuracy': 0.887326144710111}

Evaluation:	f1: {'f1': 0.609504132231405}
	accuracy: {'accuracy': 0.5275}

(1, 192) Loss: 0.07768664471950615
(1, 384) Loss: 0.20157014185315347
(1, 576) Loss: 0.1858811716811033
(1, 768) Loss: 0.1974698845333478
(1, 960) Loss: 0.07170223724824609
(1, 1152) Loss: 0.23018227474676678
(1, 1344) Loss: 0.20427687774645165
(1, 1536) Loss: 0.1539024787125527
(1, 1728) Loss: 0.21851246256846935
(1, 1920) Loss: 0.13525625609327108
(1, 2112) Loss: 0.07550312929379288
(1, 2304) Loss: 0.4352293536940124
(1, 2496) Loss: 0.051485137242707424
(1, 2688) Loss: 0.11507923558383482
(1, 2880) Loss: 0.02042765779333422
(1, 3072) Loss: 0.15543583670296357
(1, 3264) Loss: 0.012272613668756094
(1, 3456) Loss: 0.1903814014076488
(1, 3648) Loss: 0.13798570578364888
(1, 3840) Loss: 0.004114791965548648
(1, 4032) Loss: 0.04980379159678705
(1, 4224) Loss: 0.29576749958869186
(1, 4416) Loss: 0.3215784840795095
(1, 4608) Loss: 0.13361656572669744
(1, 4800) Loss: 0.21057353018113645
(1, 4992) Loss: 0.1109360389818903
(1, 5184) Loss: 0.06175232346140547
(1, 5376) Loss: 0.04240953332919162
(1, 5568) Loss: 0.09654911489633378
(1, 5760) Loss: 0.1161683319205622
(1, 5952) Loss: 0.09097102462328621
(1, 6144) Loss: 0.1560293710863334
(1, 6336) Loss: 0.05761411034472985
Training:f1:
	 {'f1': 0.9652107668867445}
accuracy:
	 {'accuracy': 0.9571808095014847}

Evaluation:	f1: {'f1': 0.6271356783919598}
	accuracy: {'accuracy': 0.53625}

(2, 192) Loss: 0.007742364507066668
(2, 384) Loss: 0.0053658266206184635
(2, 576) Loss: 0.010283356523359544
(2, 768) Loss: 0.006870584111311473
(2, 960) Loss: 0.013049575572949834
(2, 1152) Loss: 0.010071894488646649
(2, 1344) Loss: 0.080718560011519
(2, 1536) Loss: 0.21454985228956502
(2, 1728) Loss: 0.057438598607404856
(2, 1920) Loss: 0.010216499013040448
(2, 2112) Loss: 0.003820265503236442
(2, 2304) Loss: 0.04021018282401201
(2, 2496) Loss: 0.005355214325390989
(2, 2688) Loss: 0.0009680831990408478
(2, 2880) Loss: 0.10012750688474625
(2, 3072) Loss: 0.15526939333358314
(2, 3264) Loss: 0.007360197778325528
(2, 3456) Loss: 0.07208077004906954
(2, 3648) Loss: 0.03388573697156971
(2, 3840) Loss: 0.06072820168446924
(2, 4032) Loss: 0.06475380064512137
(2, 4224) Loss: 0.23871172968574683
(2, 4416) Loss: 0.009301240746935946
(2, 4608) Loss: 0.0025414335632376606
(2, 4800) Loss: 0.12434548669625656
(2, 4992) Loss: 0.025575777361154906

Using the GPU:Tesla M10
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6492125988006592
(0, 384) Loss: 0.33392396569252014
(0, 576) Loss: 0.28680258989334106
(0, 768) Loss: 0.30824342370033264
(0, 960) Loss: 0.38642627000808716
(0, 1152) Loss: 0.31180936098098755
(0, 1344) Loss: 0.22802932560443878
(0, 1536) Loss: 0.3355427384376526
(0, 1728) Loss: 0.4332185983657837
(0, 1920) Loss: 0.33265239000320435
(0, 2112) Loss: 0.26077714562416077
(0, 2304) Loss: 0.34024208784103394
(0, 2496) Loss: 0.20023992657661438
(0, 2688) Loss: 0.44426971673965454
(0, 2880) Loss: 0.2592385709285736
(0, 3072) Loss: 0.39288121461868286
(0, 3264) Loss: 0.1541042923927307
(0, 3456) Loss: 0.3570866584777832
(0, 3648) Loss: 0.40562504529953003
(0, 3840) Loss: 0.2724907100200653
Training:mse:
	 {'mse': 0.46071379960088926}

Evaluation:	mse: {'mse': 0.34979231808266553}

(1, 192) Loss: 0.2962852716445923
(1, 384) Loss: 0.19455647468566895
(1, 576) Loss: 0.24757254123687744
(1, 768) Loss: 0.24127459526062012
(1, 960) Loss: 0.3147050142288208
(1, 1152) Loss: 0.23777762055397034
(1, 1344) Loss: 0.19702690839767456
(1, 1536) Loss: 0.20040878653526306
(1, 1728) Loss: 0.2859717309474945
(1, 1920) Loss: 0.32745444774627686
(1, 2112) Loss: 0.2171093225479126
(1, 2304) Loss: 0.25977545976638794
(1, 2496) Loss: 0.1825939416885376
(1, 2688) Loss: 0.3119017481803894
(1, 2880) Loss: 0.3308268189430237
(1, 3072) Loss: 0.3409082591533661
(1, 3264) Loss: 0.17375415563583374
(1, 3456) Loss: 0.318870484828949
(1, 3648) Loss: 0.3186882436275482
(1, 3840) Loss: 0.23800963163375854
Training:mse:
	 {'mse': 0.2640699700012084}

Evaluation:	mse: {'mse': 0.3706301166068783}

(2, 192) Loss: 0.2285090833902359
(2, 384) Loss: 0.10885203629732132
(2, 576) Loss: 0.11663085222244263
(2, 768) Loss: 0.11450538039207458
(2, 960) Loss: 0.12317784130573273
(2, 1152) Loss: 0.15039169788360596
(2, 1344) Loss: 0.09203994274139404
(2, 1536) Loss: 0.0527280792593956
(2, 1728) Loss: 0.17171815037727356
(2, 1920) Loss: 0.12987202405929565
(2, 2112) Loss: 0.15190249681472778
(2, 2304) Loss: 0.18538570404052734
(2, 2496) Loss: 0.19876927137374878
(2, 2688) Loss: 0.14664633572101593
(2, 2880) Loss: 0.28536558151245117
(2, 3072) Loss: 0.2823708653450012
(2, 3264) Loss: 0.14190830290317535
(2, 3456) Loss: 0.17909258604049683
(2, 3648) Loss: 0.1738036572933197
(2, 3840) Loss: 0.178781658411026
Training:mse:
	 {'mse': 0.1652375316920468}

Evaluation:	mse: {'mse': 0.43944182103095075}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
(0, 192) Loss: 0.5536851352080703
(0, 384) Loss: 0.5144420429132879
(0, 576) Loss: 0.44962457031942904
(0, 768) Loss: 0.3249213945819065
(0, 960) Loss: 0.4635440065758303
(0, 1152) Loss: 0.2647234651376493
(0, 1344) Loss: 0.34753002680372447
(0, 1536) Loss: 0.23603917348373216
(0, 1728) Loss: 0.29364486067788675
(0, 1920) Loss: 0.3629307064693421
(0, 2112) Loss: 0.15853118390077725
(0, 2304) Loss: 0.6206983250158373
(0, 2496) Loss: 0.19297937020019162
(0, 2688) Loss: 0.14905041102610994
(0, 2880) Loss: 0.15007100031652953
(0, 3072) Loss: 0.28630361588147935
(0, 3264) Loss: 0.10981778004497755
(0, 3456) Loss: 0.25343789980979636
(0, 3648) Loss: 0.21829782707209233
(0, 3840) Loss: 0.13087208766955882
(0, 4032) Loss: 0.2364402771127061
(0, 4224) Loss: 0.4227439047244843
(0, 4416) Loss: 0.5394556193205062
(0, 4608) Loss: 0.18010905478149652
(0, 4800) Loss: 0.3072005591238849
(0, 4992) Loss: 0.10800110152922571
(0, 5184) Loss: 0.323528473105398
(0, 5376) Loss: 0.23078768624691293
(0, 5568) Loss: 0.13285236709634773
(0, 5760) Loss: 0.4563591022088076
(0, 5952) Loss: 0.09120745840482414
(0, 6144) Loss: 0.09967106306430651
(0, 6336) Loss: 0.24278440324997064
Training:f1:
	 {'f1': 0.9100661095172758}
accuracy:
	 {'accuracy': 0.887326144710111}

Evaluation:	f1: {'f1': 0.609504132231405}
	accuracy: {'accuracy': 0.5275}

(1, 192) Loss: 0.07768664471950615

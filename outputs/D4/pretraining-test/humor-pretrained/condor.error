Using the GPU:Tesla M10
Fitted PCA. Previously there were 13204 features. Now there are 6399 features.
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.6133744716644287
(0, 384) Loss: 0.4199633300304413
(0, 576) Loss: 0.34188926219940186
(0, 768) Loss: 0.30889999866485596
(0, 960) Loss: 0.3791618049144745
(0, 1152) Loss: 0.31417202949523926
(0, 1344) Loss: 0.22258752584457397
(0, 1536) Loss: 0.4166909456253052
(0, 1728) Loss: 0.40614572167396545
(0, 1920) Loss: 0.33252012729644775
(0, 2112) Loss: 0.2612415552139282
(0, 2304) Loss: 0.3208632469177246
(0, 2496) Loss: 0.20271611213684082
(0, 2688) Loss: 0.48514997959136963
(0, 2880) Loss: 0.23696130514144897
(0, 3072) Loss: 0.454817533493042
(0, 3264) Loss: 0.14774000644683838
(0, 3456) Loss: 0.3774803578853607
(0, 3648) Loss: 0.4075108468532562
(0, 3840) Loss: 0.24195612967014313
Training metrics:
	rmse: {'mse': 0.550863218143848}

Evaluation metrics:
	rmse: {'mse': 0.37635740256609707}

(1, 192) Loss: 0.27457231283187866
(1, 384) Loss: 0.19213521480560303
(1, 576) Loss: 0.25420111417770386
(1, 768) Loss: 0.2249896377325058
(1, 960) Loss: 0.3249638080596924
(1, 1152) Loss: 0.22345119714736938
(1, 1344) Loss: 0.19542033970355988
(1, 1536) Loss: 0.20928743481636047
(1, 1728) Loss: 0.35140204429626465
(1, 1920) Loss: 0.34262022376060486
(1, 2112) Loss: 0.21298658847808838
(1, 2304) Loss: 0.252616286277771
(1, 2496) Loss: 0.19663111865520477
(1, 2688) Loss: 0.42953920364379883
(1, 2880) Loss: 0.38978326320648193
(1, 3072) Loss: 0.3607644736766815
(1, 3264) Loss: 0.16392077505588531
(1, 3456) Loss: 0.3407708406448364
(1, 3648) Loss: 0.3252512812614441
(1, 3840) Loss: 0.21941432356834412
Training metrics:
	rmse: {'mse': 0.3527288729725807}

Evaluation metrics:
	rmse: {'mse': 0.3650795107648085}

(2, 192) Loss: 0.18158644437789917
(2, 384) Loss: 0.08651778101921082
(2, 576) Loss: 0.13364095985889435
(2, 768) Loss: 0.1280435025691986
(2, 960) Loss: 0.14497070014476776
(2, 1152) Loss: 0.12926995754241943
(2, 1344) Loss: 0.09992460906505585
(2, 1536) Loss: 0.0777980387210846
(2, 1728) Loss: 0.16063952445983887
(2, 1920) Loss: 0.1582905799150467
(2, 2112) Loss: 0.182270348072052
(2, 2304) Loss: 0.18338438868522644
(2, 2496) Loss: 0.14202246069908142
(2, 2688) Loss: 0.1422482281923294
(2, 2880) Loss: 0.28298670053482056
(2, 3072) Loss: 0.29136988520622253
(2, 3264) Loss: 0.1514737755060196
(2, 3456) Loss: 0.19565096497535706
(2, 3648) Loss: 0.24859628081321716
(2, 3840) Loss: 0.14115215837955475
Training metrics:
	rmse: {'mse': 0.26471873344152497}

Evaluation metrics:
	rmse: {'mse': 0.3833350900671339}

(0, 192) Loss: 0.6931891441345215
(0, 384) Loss: 0.6904624700546265
(0, 576) Loss: 0.6438311338424683
(0, 768) Loss: 0.4404240548610687
(0, 960) Loss: 0.4888818562030792
(0, 1152) Loss: 0.5101144313812256
(0, 1344) Loss: 0.6598691940307617
(0, 1536) Loss: 0.27826327085494995
(0, 1728) Loss: 0.27967512607574463
(0, 1920) Loss: 0.4117560386657715
(0, 2112) Loss: 0.25915342569351196
(0, 2304) Loss: 0.3677983283996582
(0, 2496) Loss: 0.40990400314331055
(0, 2688) Loss: 0.08927533030509949
(0, 2880) Loss: 0.1378060132265091
(0, 3072) Loss: 0.3368578255176544
(0, 3264) Loss: 0.10160551965236664
(0, 3456) Loss: 0.40354788303375244
(0, 3648) Loss: 0.3508511185646057
(0, 3840) Loss: 0.0821719765663147
(0, 4032) Loss: 0.3201088309288025
(0, 4224) Loss: 0.3150317668914795
(0, 4416) Loss: 0.5738605856895447
(0, 4608) Loss: 0.2759553790092468
(0, 4800) Loss: 0.30963391065597534
(0, 4992) Loss: 0.2490948736667633
(0, 5184) Loss: 0.5269097089767456
(0, 5376) Loss: 0.25610119104385376
(0, 5568) Loss: 0.21022099256515503
(0, 5760) Loss: 0.29054078459739685
(0, 5952) Loss: 0.07459583878517151
(0, 6144) Loss: 0.13785803318023682
(0, 6336) Loss: 0.20557230710983276
Training metrics:
	f1: {'f1': 0.8846672386321854}
	accuracy: {'accuracy': 0.8529457727769965}

Evaluation metrics:
	f1: {'f1': 0.9314227226202662}
	accuracy: {'accuracy': 0.91625}

(1, 192) Loss: 0.2234337031841278
(1, 384) Loss: 0.23594117164611816
(1, 576) Loss: 0.2568396329879761
(1, 768) Loss: 0.3044431507587433
(1, 960) Loss: 0.09278109669685364
(1, 1152) Loss: 0.2896309494972229
(1, 1344) Loss: 0.2510443329811096
(1, 1536) Loss: 0.13885757327079773
(1, 1728) Loss: 0.18901926279067993
(1, 1920) Loss: 0.15781456232070923
(1, 2112) Loss: 0.06597919762134552
(1, 2304) Loss: 0.4182416796684265
(1, 2496) Loss: 0.03558673709630966
(1, 2688) Loss: 0.1439782828092575
(1, 2880) Loss: 0.07360479235649109
(1, 3072) Loss: 0.18794867396354675
(1, 3264) Loss: 0.013750363141298294
(1, 3456) Loss: 0.12247608602046967
(1, 3648) Loss: 0.07763935625553131
(1, 3840) Loss: 0.051490262150764465
(1, 4032) Loss: 0.041294366121292114
(1, 4224) Loss: 0.22301658987998962
(1, 4416) Loss: 0.03798104450106621
(1, 4608) Loss: 0.16471309959888458
(1, 4800) Loss: 0.15262524783611298
(1, 4992) Loss: 0.13406145572662354
(1, 5184) Loss: 0.24285165965557098
(1, 5376) Loss: 0.17066706717014313
(1, 5568) Loss: 0.17243945598602295
(1, 5760) Loss: 0.12036462873220444
(1, 5952) Loss: 0.007222277577966452
(1, 6144) Loss: 0.014231916517019272
(1, 6336) Loss: 0.06275533884763718
Training metrics:
	f1: {'f1': 0.961499493414387}
	accuracy: {'accuracy': 0.9524925769651508}

Evaluation metrics:
	f1: {'f1': 0.9201520912547528}
	accuracy: {'accuracy': 0.895}

(2, 192) Loss: 0.0823974758386612
(2, 384) Loss: 0.24326661229133606
(2, 576) Loss: 0.04404285550117493
(2, 768) Loss: 0.028425555676221848
(2, 960) Loss: 0.017893843352794647
(2, 1152) Loss: 0.2631884813308716
(2, 1344) Loss: 0.07452045381069183
(2, 1536) Loss: 0.34293073415756226
(2, 1728) Loss: 0.22177615761756897
(2, 1920) Loss: 0.16882343590259552
(2, 2112) Loss: 0.023701675236225128
(2, 2304) Loss: 0.09973589330911636
(2, 2496) Loss: 0.15135647356510162
(2, 2688) Loss: 0.005249373149126768
(2, 2880) Loss: 0.054205648601055145
(2, 3072) Loss: 0.046558380126953125
(2, 3264) Loss: 0.023902658373117447
(2, 3456) Loss: 0.04962204769253731
(2, 3648) Loss: 0.10222010314464569
(2, 3840) Loss: 0.13489733636379242
(2, 4032) Loss: 0.03464045003056526
(2, 4224) Loss: 0.03811270743608475
(2, 4416) Loss: 0.0016617131186649203
(2, 4608) Loss: 0.0016634829808026552
(2, 4800) Loss: 0.03837662935256958
(2, 4992) Loss: 0.03230944648385048
(2, 5184) Loss: 0.08047787845134735
(2, 5376) Loss: 0.0110250823199749
(2, 5568) Loss: 0.00992647185921669
(2, 5760) Loss: 0.010631125420331955
(2, 5952) Loss: 0.0011734750587493181
(2, 6144) Loss: 0.11975454539060593
(2, 6336) Loss: 0.0009738408261910081
Training metrics:
	f1: {'f1': 0.9797365754812563}
	accuracy: {'accuracy': 0.974996093139553}

Evaluation metrics:
	f1: {'f1': 0.9179389312977099}
	accuracy: {'accuracy': 0.8925}

Evaluation metrics:
	f1: {'f1': 0.9179389312977099}
	accuracy: {'accuracy': 0.8925}


Using the GPU:Tesla M10
Fitted PCA. Previously there were 9109 features. Now there are 3945 features.
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(0, 192) Loss: 0.49863940477371216
(0, 384) Loss: 0.4067896008491516
(0, 576) Loss: 0.308868408203125
(0, 768) Loss: 0.27520424127578735
(0, 960) Loss: 0.3992767333984375
(0, 1152) Loss: 0.3245094418525696
(0, 1344) Loss: 0.24714398384094238
(0, 1536) Loss: 0.3626594543457031
(0, 1728) Loss: 0.38935190439224243
(0, 1920) Loss: 0.32822030782699585
(0, 2112) Loss: 0.30338630080223083
(0, 2304) Loss: 0.36368706822395325
(0, 2496) Loss: 0.259095162153244
(0, 2688) Loss: 0.28342968225479126
(0, 2880) Loss: 0.32218196988105774
(0, 3072) Loss: 0.35708025097846985
(0, 3264) Loss: 0.22884076833724976
(0, 3456) Loss: 0.44104254245758057
(0, 3648) Loss: 0.4080458879470825
(0, 3840) Loss: 0.28179776668548584
Training metrics:
	rmse: {'mse': 0.5749696821302037}

Evaluation metrics:
	rmse: {'mse': 0.4101910731338862}

(1, 192) Loss: 0.2965976297855377
(1, 384) Loss: 0.24363389611244202
(1, 576) Loss: 0.2563577890396118
(1, 768) Loss: 0.2726764380931854
(1, 960) Loss: 0.32251283526420593
(1, 1152) Loss: 0.27271971106529236
(1, 1344) Loss: 0.2046629935503006
(1, 1536) Loss: 0.21812093257904053
(1, 1728) Loss: 0.3017100393772125
(1, 1920) Loss: 0.3163330852985382
(1, 2112) Loss: 0.19911125302314758
(1, 2304) Loss: 0.28406262397766113
(1, 2496) Loss: 0.19441604614257812
(1, 2688) Loss: 0.2598283290863037
(1, 2880) Loss: 0.271783709526062
(1, 3072) Loss: 0.3279300630092621
(1, 3264) Loss: 0.17705214023590088
(1, 3456) Loss: 0.3679857850074768
(1, 3648) Loss: 0.3283482491970062
(1, 3840) Loss: 0.26972630620002747
Training metrics:
	rmse: {'mse': 0.35910656624675596}

Evaluation metrics:
	rmse: {'mse': 0.3760328578142353}

(2, 192) Loss: 0.203887477517128
(2, 384) Loss: 0.12204530090093613
(2, 576) Loss: 0.16166573762893677
(2, 768) Loss: 0.16743187606334686
(2, 960) Loss: 0.1830505132675171
(2, 1152) Loss: 0.20197346806526184
(2, 1344) Loss: 0.13428851962089539
(2, 1536) Loss: 0.0652054026722908
(2, 1728) Loss: 0.16058017313480377
(2, 1920) Loss: 0.10909625142812729
(2, 2112) Loss: 0.12958809733390808
(2, 2304) Loss: 0.1481683850288391
(2, 2496) Loss: 0.14735130965709686
(2, 2688) Loss: 0.22444984316825867
(2, 2880) Loss: 0.14058279991149902
(2, 3072) Loss: 0.3206896185874939
(2, 3264) Loss: 0.07811623811721802
(2, 3456) Loss: 0.21825212240219116
(2, 3648) Loss: 0.21889911592006683
(2, 3840) Loss: 0.196811243891716
Training metrics:
	rmse: {'mse': 0.2561813425605111}

Evaluation metrics:
	rmse: {'mse': 0.38718904926288805}

(0, 192) Loss: 0.6967524290084839
(0, 384) Loss: 0.6837454438209534
(0, 576) Loss: 0.7083213329315186
(0, 768) Loss: 0.682218611240387
(0, 960) Loss: 0.6922001838684082
(0, 1152) Loss: 0.6984253525733948
(0, 1344) Loss: 0.6961380243301392
(0, 1536) Loss: 0.6919329166412354
(0, 1728) Loss: 0.692827582359314
(0, 1920) Loss: 0.6943987607955933
(0, 2112) Loss: 0.6863353848457336
(0, 2304) Loss: 0.694883406162262
(0, 2496) Loss: 0.6973724365234375
(0, 2688) Loss: 0.6984403133392334
(0, 2880) Loss: 0.7010234594345093
(0, 3072) Loss: 0.684809148311615
(0, 3264) Loss: 0.6887216567993164
(0, 3456) Loss: 0.6910701990127563
(0, 3648) Loss: 0.6905732154846191
(0, 3840) Loss: 0.6919681429862976
Training metrics:
	f1: {'f1': 0.5211027079775556}
	accuracy: {'accuracy': 0.5024081115335868}

Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(1, 192) Loss: 0.669702410697937
(1, 384) Loss: 0.6792184114456177
(1, 576) Loss: 0.6970257759094238
(1, 768) Loss: 0.6817997694015503
(1, 960) Loss: 0.728785514831543
(1, 1152) Loss: 0.6932306289672852
(1, 1344) Loss: 0.6958351135253906
(1, 1536) Loss: 0.7010793685913086
(1, 1728) Loss: 0.6929559707641602
(1, 1920) Loss: 0.6908450126647949
(1, 2112) Loss: 0.6865686178207397
(1, 2304) Loss: 0.6935745477676392
(1, 2496) Loss: 0.6953049898147583
(1, 2688) Loss: 0.6930716633796692
(1, 2880) Loss: 0.6923508644104004
(1, 3072) Loss: 0.6909689903259277
(1, 3264) Loss: 0.691128134727478
(1, 3456) Loss: 0.6936736106872559
(1, 3648) Loss: 0.6932315826416016
(1, 3840) Loss: 0.6926438808441162
Training metrics:
	f1: {'f1': 0.5447306791569086}
	accuracy: {'accuracy': 0.5072243346007604}

Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

(2, 192) Loss: 0.6761682033538818
(2, 384) Loss: 0.6802221536636353
(2, 576) Loss: 0.7046624422073364
(2, 768) Loss: 0.6942527294158936
(2, 960) Loss: 0.6906603574752808
(2, 1152) Loss: 0.6759617328643799
(2, 1344) Loss: 0.6947504878044128
(2, 1536) Loss: 0.8044874668121338
(2, 1728) Loss: 0.7052035331726074
(2, 1920) Loss: 0.694130539894104
(2, 2112) Loss: 0.6868599653244019
(2, 2304) Loss: 0.6933107972145081
(2, 2496) Loss: 0.6965709924697876
(2, 2688) Loss: 0.692593514919281
(2, 2880) Loss: 0.7006340026855469
(2, 3072) Loss: 0.6925369501113892
(2, 3264) Loss: 0.6882930994033813
(2, 3456) Loss: 0.6951609253883362
(2, 3648) Loss: 0.6936744451522827
(2, 3840) Loss: 0.6933742761611938
Training metrics:
	f1: {'f1': 0.4776119402985075}
	accuracy: {'accuracy': 0.5031685678073511}

Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}

Evaluation metrics:
	f1: {'f1': 0.0}
	accuracy: {'accuracy': 0.4969574036511156}


emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 6399
  Num Epochs = 1
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 200
  0%|          | 0/200 [00:00<?, ?it/s]/projects/assigned/2122_ling573_elibales/repo/src/fine-tune-bertweet.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}
  0%|          | 1/200 [00:02<07:14,  2.18s/it]  1%|          | 2/200 [00:04<06:53,  2.09s/it]  2%|▏         | 3/200 [00:06<06:45,  2.06s/it]  2%|▏         | 4/200 [00:08<06:42,  2.05s/it]  2%|▎         | 5/200 [00:10<06:37,  2.04s/it]  3%|▎         | 6/200 [00:12<06:34,  2.03s/it]  4%|▎         | 7/200 [00:14<06:32,  2.04s/it]  4%|▍         | 8/200 [00:16<06:30,  2.03s/it]  4%|▍         | 9/200 [00:18<06:27,  2.03s/it]  5%|▌         | 10/200 [00:20<06:25,  2.03s/it]  6%|▌         | 11/200 [00:22<06:23,  2.03s/it]  6%|▌         | 12/200 [00:24<06:21,  2.03s/it]  6%|▋         | 13/200 [00:26<06:18,  2.03s/it]  7%|▋         | 14/200 [00:28<06:16,  2.03s/it]  8%|▊         | 15/200 [00:30<06:14,  2.03s/it]  8%|▊         | 16/200 [00:32<06:12,  2.03s/it]  8%|▊         | 17/200 [00:34<06:10,  2.02s/it]  9%|▉         | 18/200 [00:36<06:08,  2.03s/it] 10%|▉         | 19/200 [00:38<06:06,  2.03s/it] 10%|█         | 20/200 [00:40<06:05,  2.03s/it] 10%|█         | 21/200 [00:42<06:03,  2.03s/it] 11%|█         | 22/200 [00:44<06:00,  2.03s/it] 12%|█▏        | 23/200 [00:46<05:58,  2.03s/it] 12%|█▏        | 24/200 [00:48<05:56,  2.03s/it] 12%|█▎        | 25/200 [00:50<05:54,  2.03s/it] 13%|█▎        | 26/200 [00:52<05:52,  2.03s/it] 14%|█▎        | 27/200 [00:54<05:50,  2.03s/it] 14%|█▍        | 28/200 [00:56<05:48,  2.03s/it] 14%|█▍        | 29/200 [00:58<05:46,  2.03s/it] 15%|█▌        | 30/200 [01:00<05:44,  2.03s/it] 16%|█▌        | 31/200 [01:02<05:42,  2.03s/it] 16%|█▌        | 32/200 [01:05<05:41,  2.03s/it] 16%|█▋        | 33/200 [01:07<05:38,  2.03s/it] 17%|█▋        | 34/200 [01:09<05:37,  2.03s/it] 18%|█▊        | 35/200 [01:11<05:35,  2.03s/it] 18%|█▊        | 36/200 [01:13<05:33,  2.03s/it] 18%|█▊        | 37/200 [01:15<05:31,  2.03s/it] 19%|█▉        | 38/200 [01:17<05:29,  2.03s/it] 20%|█▉        | 39/200 [01:19<05:27,  2.03s/it] 20%|██        | 40/200 [01:21<05:26,  2.04s/it] 20%|██        | 41/200 [01:23<05:24,  2.04s/it] 21%|██        | 42/200 [01:25<05:21,  2.04s/it] 22%|██▏       | 43/200 [01:27<05:19,  2.03s/it] 22%|██▏       | 44/200 [01:29<05:17,  2.04s/it] 22%|██▎       | 45/200 [01:31<05:15,  2.04s/it] 23%|██▎       | 46/200 [01:33<05:13,  2.03s/it] 24%|██▎       | 47/200 [01:35<05:11,  2.03s/it] 24%|██▍       | 48/200 [01:37<05:08,  2.03s/it] 24%|██▍       | 49/200 [01:39<05:06,  2.03s/it] 25%|██▌       | 50/200 [01:41<05:04,  2.03s/it] 26%|██▌       | 51/200 [01:43<05:02,  2.03s/it] 26%|██▌       | 52/200 [01:45<05:00,  2.03s/it] 26%|██▋       | 53/200 [01:47<04:58,  2.03s/it] 27%|██▋       | 54/200 [01:49<04:56,  2.03s/it] 28%|██▊       | 55/200 [01:51<04:54,  2.03s/it] 28%|██▊       | 56/200 [01:53<04:52,  2.03s/it] 28%|██▊       | 57/200 [01:55<04:50,  2.03s/it] 29%|██▉       | 58/200 [01:57<04:48,  2.03s/it] 30%|██▉       | 59/200 [01:59<04:46,  2.03s/it] 30%|███       | 60/200 [02:01<04:44,  2.03s/it] 30%|███       | 61/200 [02:03<04:42,  2.03s/it] 31%|███       | 62/200 [02:06<04:40,  2.03s/it] 32%|███▏      | 63/200 [02:08<04:38,  2.03s/it] 32%|███▏      | 64/200 [02:10<04:36,  2.03s/it] 32%|███▎      | 65/200 [02:12<04:35,  2.04s/it] 33%|███▎      | 66/200 [02:14<04:32,  2.04s/it] 34%|███▎      | 67/200 [02:16<04:30,  2.03s/it] 34%|███▍      | 68/200 [02:18<04:28,  2.04s/it] 34%|███▍      | 69/200 [02:20<04:26,  2.03s/it] 35%|███▌      | 70/200 [02:22<04:24,  2.03s/it] 36%|███▌      | 71/200 [02:24<04:22,  2.03s/it] 36%|███▌      | 72/200 [02:26<04:20,  2.03s/it] 36%|███▋      | 73/200 [02:28<04:17,  2.03s/it] 37%|███▋      | 74/200 [02:30<04:16,  2.04s/it] 38%|███▊      | 75/200 [02:32<04:14,  2.04s/it] 38%|███▊      | 76/200 [02:34<04:15,  2.06s/it] 38%|███▊      | 77/200 [02:36<04:12,  2.05s/it] 39%|███▉      | 78/200 [02:38<04:09,  2.05s/it] 40%|███▉      | 79/200 [02:40<04:06,  2.04s/it] 40%|████      | 80/200 [02:42<04:04,  2.03s/it] 40%|████      | 81/200 [02:44<04:01,  2.03s/it] 41%|████      | 82/200 [02:46<03:59,  2.03s/it] 42%|████▏     | 83/200 [02:48<03:57,  2.03s/it] 42%|████▏     | 84/200 [02:50<03:55,  2.03s/it] 42%|████▎     | 85/200 [02:52<03:53,  2.03s/it] 43%|████▎     | 86/200 [02:54<03:51,  2.03s/it] 44%|████▎     | 87/200 [02:56<03:49,  2.03s/it] 44%|████▍     | 88/200 [02:58<03:47,  2.03s/it] 44%|████▍     | 89/200 [03:00<03:45,  2.03s/it] 45%|████▌     | 90/200 [03:03<03:43,  2.03s/it] 46%|████▌     | 91/200 [03:05<03:41,  2.03s/it] 46%|████▌     | 92/200 [03:07<03:39,  2.03s/it] 46%|████▋     | 93/200 [03:09<03:37,  2.03s/it] 47%|████▋     | 94/200 [03:11<03:35,  2.03s/it] 48%|████▊     | 95/200 [03:13<03:34,  2.04s/it] 48%|████▊     | 96/200 [03:15<03:31,  2.04s/it] 48%|████▊     | 97/200 [03:17<03:29,  2.04s/it] 49%|████▉     | 98/200 [03:19<03:27,  2.03s/it] 50%|████▉     | 99/200 [03:21<03:25,  2.03s/it] 50%|█████     | 100/200 [03:23<03:23,  2.03s/it] 50%|█████     | 101/200 [03:25<03:20,  2.03s/it] 51%|█████     | 102/200 [03:27<03:18,  2.03s/it] 52%|█████▏    | 103/200 [03:29<03:17,  2.03s/it] 52%|█████▏    | 104/200 [03:31<03:15,  2.03s/it] 52%|█████▎    | 105/200 [03:33<03:13,  2.03s/it] 53%|█████▎    | 106/200 [03:35<03:10,  2.03s/it] 54%|█████▎    | 107/200 [03:37<03:09,  2.04s/it] 54%|█████▍    | 108/200 [03:39<03:07,  2.04s/it] 55%|█████▍    | 109/200 [03:41<03:05,  2.04s/it] 55%|█████▌    | 110/200 [03:43<03:03,  2.04s/it] 56%|█████▌    | 111/200 [03:45<03:01,  2.04s/it] 56%|█████▌    | 112/200 [03:47<02:59,  2.03s/it] 56%|█████▋    | 113/200 [03:49<02:56,  2.03s/it] 57%|█████▋    | 114/200 [03:51<02:54,  2.03s/it] 57%|█████▊    | 115/200 [03:53<02:52,  2.04s/it] 58%|█████▊    | 116/200 [03:55<02:51,  2.04s/it] 58%|█████▊    | 117/200 [03:57<02:48,  2.03s/it] 59%|█████▉    | 118/200 [03:59<02:46,  2.03s/it] 60%|█████▉    | 119/200 [04:01<02:44,  2.03s/it] 60%|██████    | 120/200 [04:04<02:42,  2.03s/it] 60%|██████    | 121/200 [04:06<02:40,  2.03s/it] 61%|██████    | 122/200 [04:08<02:38,  2.03s/it] 62%|██████▏   | 123/200 [04:10<02:36,  2.03s/it] 62%|██████▏   | 124/200 [04:12<02:34,  2.03s/it] 62%|██████▎   | 125/200 [04:14<02:32,  2.03s/it] 63%|██████▎   | 126/200 [04:16<02:30,  2.04s/it] 64%|██████▎   | 127/200 [04:18<02:28,  2.03s/it] 64%|██████▍   | 128/200 [04:20<02:26,  2.03s/it] 64%|██████▍   | 129/200 [04:22<02:24,  2.03s/it] 65%|██████▌   | 130/200 [04:24<02:22,  2.04s/it] 66%|██████▌   | 131/200 [04:26<02:20,  2.04s/it] 66%|██████▌   | 132/200 [04:28<02:18,  2.04s/it] 66%|██████▋   | 133/200 [04:30<02:16,  2.04s/it] 67%|██████▋   | 134/200 [04:32<02:14,  2.04s/it] 68%|██████▊   | 135/200 [04:34<02:12,  2.03s/it] 68%|██████▊   | 136/200 [04:36<02:10,  2.03s/it] 68%|██████▊   | 137/200 [04:38<02:08,  2.03s/it] 69%|██████▉   | 138/200 [04:40<02:05,  2.03s/it] 70%|██████▉   | 139/200 [04:42<02:03,  2.03s/it] 70%|███████   | 140/200 [04:44<02:01,  2.03s/it] 70%|███████   | 141/200 [04:46<02:00,  2.04s/it] 71%|███████   | 142/200 [04:48<01:58,  2.04s/it] 72%|███████▏  | 143/200 [04:50<01:56,  2.04s/it] 72%|███████▏  | 144/200 [04:52<01:54,  2.04s/it] 72%|███████▎  | 145/200 [04:54<01:51,  2.03s/it] 73%|███████▎  | 146/200 [04:56<01:49,  2.03s/it] 74%|███████▎  | 147/200 [04:58<01:47,  2.03s/it] 74%|███████▍  | 148/200 [05:00<01:45,  2.03s/it] 74%|███████▍  | 149/200 [05:03<01:43,  2.03s/it] 75%|███████▌  | 150/200 [05:05<01:41,  2.03s/it] 76%|███████▌  | 151/200 [05:07<01:39,  2.03s/it] 76%|███████▌  | 152/200 [05:09<01:37,  2.03s/it] 76%|███████▋  | 153/200 [05:11<01:35,  2.03s/it] 77%|███████▋  | 154/200 [05:13<01:33,  2.03s/it] 78%|███████▊  | 155/200 [05:15<01:31,  2.03s/it] 78%|███████▊  | 156/200 [05:17<01:29,  2.04s/it] 78%|███████▊  | 157/200 [05:19<01:28,  2.06s/it] 79%|███████▉  | 158/200 [05:21<01:26,  2.05s/it] 80%|███████▉  | 159/200 [05:23<01:23,  2.04s/it] 80%|████████  | 160/200 [05:25<01:21,  2.04s/it] 80%|████████  | 161/200 [05:27<01:19,  2.04s/it] 81%|████████  | 162/200 [05:29<01:17,  2.03s/it] 82%|████████▏ | 163/200 [05:31<01:15,  2.03s/it] 82%|████████▏ | 164/200 [05:33<01:13,  2.03s/it] 82%|████████▎ | 165/200 [05:35<01:11,  2.03s/it] 83%|████████▎ | 166/200 [05:37<01:09,  2.04s/it] 84%|████████▎ | 167/200 [05:39<01:07,  2.04s/it] 84%|████████▍ | 168/200 [05:41<01:05,  2.04s/it] 84%|████████▍ | 169/200 [05:43<01:03,  2.04s/it] 85%|████████▌ | 170/200 [05:45<01:01,  2.06s/it] 86%|████████▌ | 171/200 [05:47<00:59,  2.05s/it] 86%|████████▌ | 172/200 [05:49<00:57,  2.05s/it] 86%|████████▋ | 173/200 [05:51<00:55,  2.05s/it] 87%|████████▋ | 174/200 [05:54<00:53,  2.05s/it] 88%|████████▊ | 175/200 [05:56<00:51,  2.05s/it] 88%|████████▊ | 176/200 [05:58<00:49,  2.04s/it] 88%|████████▊ | 177/200 [06:00<00:46,  2.04s/it] 89%|████████▉ | 178/200 [06:02<00:44,  2.04s/it] 90%|████████▉ | 179/200 [06:04<00:42,  2.04s/it] 90%|█████████ | 180/200 [06:06<00:40,  2.04s/it] 90%|█████████ | 181/200 [06:08<00:38,  2.04s/it] 91%|█████████ | 182/200 [06:10<00:36,  2.04s/it] 92%|█████████▏| 183/200 [06:12<00:34,  2.04s/it] 92%|█████████▏| 184/200 [06:14<00:32,  2.03s/it] 92%|█████████▎| 185/200 [06:16<00:30,  2.03s/it] 93%|█████████▎| 186/200 [06:18<00:28,  2.04s/it] 94%|█████████▎| 187/200 [06:20<00:26,  2.04s/it] 94%|█████████▍| 188/200 [06:22<00:24,  2.03s/it] 94%|█████████▍| 189/200 [06:24<00:22,  2.03s/it] 95%|█████████▌| 190/200 [06:26<00:20,  2.03s/it] 96%|█████████▌| 191/200 [06:28<00:18,  2.03s/it] 96%|█████████▌| 192/200 [06:30<00:16,  2.03s/it] 96%|█████████▋| 193/200 [06:32<00:14,  2.03s/it] 97%|█████████▋| 194/200 [06:34<00:12,  2.03s/it] 98%|█████████▊| 195/200 [06:36<00:10,  2.03s/it] 98%|█████████▊| 196/200 [06:38<00:08,  2.03s/it] 98%|█████████▊| 197/200 [06:40<00:06,  2.03s/it] 99%|█████████▉| 198/200 [06:42<00:04,  2.03s/it]100%|█████████▉| 199/200 [06:44<00:02,  2.03s/it]100%|██████████| 200/200 [06:46<00:00,  2.02s/it]***** Running Evaluation *****
  Num examples = 800
  Batch size = 32

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:00<00:05,  4.15it/s][A
 12%|█▏        | 3/25 [00:00<00:07,  2.93it/s][A
 16%|█▌        | 4/25 [00:01<00:08,  2.54it/s][A
 20%|██        | 5/25 [00:01<00:08,  2.36it/s][A
 24%|██▍       | 6/25 [00:02<00:08,  2.26it/s][A
 28%|██▊       | 7/25 [00:02<00:08,  2.20it/s][A
 32%|███▏      | 8/25 [00:03<00:07,  2.16it/s][A
 36%|███▌      | 9/25 [00:03<00:07,  2.12it/s][A
 40%|████      | 10/25 [00:04<00:07,  2.11it/s][A
 44%|████▍     | 11/25 [00:04<00:06,  2.10it/s][A
 48%|████▊     | 12/25 [00:05<00:06,  2.09it/s][A
 52%|█████▏    | 13/25 [00:05<00:05,  2.09it/s][A
 56%|█████▌    | 14/25 [00:06<00:05,  2.08it/s][A
 60%|██████    | 15/25 [00:06<00:04,  2.08it/s][A
 64%|██████▍   | 16/25 [00:07<00:04,  2.08it/s][A
 68%|██████▊   | 17/25 [00:07<00:03,  2.08it/s][A
 72%|███████▏  | 18/25 [00:08<00:03,  2.08it/s][A
 76%|███████▌  | 19/25 [00:08<00:02,  2.08it/s][A
 80%|████████  | 20/25 [00:09<00:02,  2.08it/s][A
 84%|████████▍ | 21/25 [00:09<00:01,  2.08it/s][A
 88%|████████▊ | 22/25 [00:10<00:01,  2.08it/s][A
 92%|█████████▏| 23/25 [00:10<00:00,  2.08it/s][A
 96%|█████████▌| 24/25 [00:11<00:00,  2.07it/s][A
100%|██████████| 25/25 [00:11<00:00,  2.07it/s][A                                                 
                                               [A100%|██████████| 200/200 [06:59<00:00,  2.02s/it]
100%|██████████| 25/25 [00:11<00:00,  2.07it/s][A
                                               [A

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [06:59<00:00,  2.02s/it]100%|██████████| 200/200 [06:59<00:00,  2.10s/it]
/projects/assigned/2122_ling573_elibales/repo/src/fine-tune-bertweet.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}
/projects/assigned/2122_ling573_elibales/repo/src/fine-tune-bertweet.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}
Configuration saved in src/models/bertweet-fine-tuned-preproc/config.json
Model weights saved in src/models/bertweet-fine-tuned-preproc/pytorch_model.bin

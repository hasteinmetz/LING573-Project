emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 6399
  Num Epochs = 1
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 200
  0%|          | 0/200 [00:00<?, ?it/s]/projects/assigned/2122_ling573_elibales/repo/src/fine-tune-berttweet.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}
  0%|          | 1/200 [00:01<06:27,  1.95s/it]  1%|          | 2/200 [00:03<06:00,  1.82s/it]  2%|▏         | 3/200 [00:05<05:50,  1.78s/it]  2%|▏         | 4/200 [00:07<05:45,  1.76s/it]  2%|▎         | 5/200 [00:08<05:41,  1.75s/it]  3%|▎         | 6/200 [00:10<05:39,  1.75s/it]  4%|▎         | 7/200 [00:12<05:36,  1.74s/it]  4%|▍         | 8/200 [00:14<05:34,  1.74s/it]  4%|▍         | 9/200 [00:15<05:32,  1.74s/it]  5%|▌         | 10/200 [00:17<05:30,  1.74s/it]  6%|▌         | 11/200 [00:19<05:28,  1.74s/it]  6%|▌         | 12/200 [00:21<05:26,  1.74s/it]  6%|▋         | 13/200 [00:22<05:25,  1.74s/it]  7%|▋         | 14/200 [00:24<05:23,  1.74s/it]  8%|▊         | 15/200 [00:26<05:21,  1.74s/it]  8%|▊         | 16/200 [00:28<05:20,  1.74s/it]  8%|▊         | 17/200 [00:29<05:18,  1.74s/it]  9%|▉         | 18/200 [00:31<05:16,  1.74s/it] 10%|▉         | 19/200 [00:33<05:14,  1.74s/it] 10%|█         | 20/200 [00:34<05:12,  1.74s/it] 10%|█         | 21/200 [00:36<05:11,  1.74s/it] 11%|█         | 22/200 [00:38<05:09,  1.74s/it] 12%|█▏        | 23/200 [00:40<05:07,  1.74s/it] 12%|█▏        | 24/200 [00:41<05:05,  1.74s/it] 12%|█▎        | 25/200 [00:43<05:03,  1.74s/it] 13%|█▎        | 26/200 [00:45<05:02,  1.74s/it] 14%|█▎        | 27/200 [00:47<05:00,  1.74s/it] 14%|█▍        | 28/200 [00:48<04:58,  1.74s/it] 14%|█▍        | 29/200 [00:50<04:57,  1.74s/it] 15%|█▌        | 30/200 [00:52<04:55,  1.74s/it] 16%|█▌        | 31/200 [00:54<04:54,  1.74s/it] 16%|█▌        | 32/200 [00:55<04:52,  1.74s/it] 16%|█▋        | 33/200 [00:57<04:50,  1.74s/it] 17%|█▋        | 34/200 [00:59<04:48,  1.74s/it] 18%|█▊        | 35/200 [01:01<04:46,  1.74s/it] 18%|█▊        | 36/200 [01:02<04:45,  1.74s/it] 18%|█▊        | 37/200 [01:04<04:43,  1.74s/it] 19%|█▉        | 38/200 [01:06<04:41,  1.74s/it] 20%|█▉        | 39/200 [01:07<04:39,  1.74s/it] 20%|██        | 40/200 [01:09<04:37,  1.74s/it] 20%|██        | 41/200 [01:11<04:36,  1.74s/it] 21%|██        | 42/200 [01:13<04:34,  1.74s/it] 22%|██▏       | 43/200 [01:14<04:32,  1.74s/it] 22%|██▏       | 44/200 [01:16<04:31,  1.74s/it] 22%|██▎       | 45/200 [01:18<04:29,  1.74s/it] 23%|██▎       | 46/200 [01:20<04:27,  1.74s/it] 24%|██▎       | 47/200 [01:21<04:25,  1.74s/it] 24%|██▍       | 48/200 [01:23<04:24,  1.74s/it] 24%|██▍       | 49/200 [01:25<04:22,  1.74s/it] 25%|██▌       | 50/200 [01:27<04:20,  1.74s/it] 26%|██▌       | 51/200 [01:28<04:18,  1.74s/it] 26%|██▌       | 52/200 [01:30<04:17,  1.74s/it] 26%|██▋       | 53/200 [01:32<04:15,  1.74s/it] 27%|██▋       | 54/200 [01:34<04:13,  1.74s/it] 28%|██▊       | 55/200 [01:35<04:11,  1.74s/it] 28%|██▊       | 56/200 [01:37<04:10,  1.74s/it] 28%|██▊       | 57/200 [01:39<04:08,  1.74s/it] 29%|██▉       | 58/200 [01:41<04:07,  1.74s/it] 30%|██▉       | 59/200 [01:42<04:05,  1.74s/it] 30%|███       | 60/200 [01:44<04:03,  1.74s/it] 30%|███       | 61/200 [01:46<04:01,  1.74s/it] 31%|███       | 62/200 [01:47<04:00,  1.74s/it] 32%|███▏      | 63/200 [01:49<03:58,  1.74s/it] 32%|███▏      | 64/200 [01:51<03:56,  1.74s/it] 32%|███▎      | 65/200 [01:53<03:54,  1.74s/it] 33%|███▎      | 66/200 [01:54<03:52,  1.74s/it] 34%|███▎      | 67/200 [01:56<03:51,  1.74s/it] 34%|███▍      | 68/200 [01:58<03:49,  1.74s/it] 34%|███▍      | 69/200 [02:00<03:47,  1.74s/it] 35%|███▌      | 70/200 [02:01<03:45,  1.74s/it] 36%|███▌      | 71/200 [02:03<03:44,  1.74s/it] 36%|███▌      | 72/200 [02:05<03:42,  1.74s/it] 36%|███▋      | 73/200 [02:07<03:40,  1.74s/it] 37%|███▋      | 74/200 [02:08<03:38,  1.74s/it] 38%|███▊      | 75/200 [02:10<03:37,  1.74s/it] 38%|███▊      | 76/200 [02:12<03:35,  1.74s/it] 38%|███▊      | 77/200 [02:14<03:33,  1.74s/it] 39%|███▉      | 78/200 [02:15<03:32,  1.74s/it] 40%|███▉      | 79/200 [02:17<03:30,  1.74s/it] 40%|████      | 80/200 [02:19<03:28,  1.74s/it] 40%|████      | 81/200 [02:20<03:26,  1.74s/it] 41%|████      | 82/200 [02:22<03:25,  1.74s/it] 42%|████▏     | 83/200 [02:24<03:23,  1.74s/it] 42%|████▏     | 84/200 [02:26<03:21,  1.74s/it] 42%|████▎     | 85/200 [02:27<03:19,  1.74s/it] 43%|████▎     | 86/200 [02:29<03:18,  1.74s/it] 44%|████▎     | 87/200 [02:31<03:16,  1.74s/it] 44%|████▍     | 88/200 [02:33<03:14,  1.74s/it] 44%|████▍     | 89/200 [02:34<03:12,  1.74s/it] 45%|████▌     | 90/200 [02:36<03:11,  1.74s/it] 46%|████▌     | 91/200 [02:38<03:09,  1.74s/it] 46%|████▌     | 92/200 [02:40<03:07,  1.74s/it] 46%|████▋     | 93/200 [02:41<03:05,  1.74s/it] 47%|████▋     | 94/200 [02:43<03:04,  1.74s/it] 48%|████▊     | 95/200 [02:45<03:02,  1.74s/it] 48%|████▊     | 96/200 [02:47<03:00,  1.74s/it] 48%|████▊     | 97/200 [02:48<02:59,  1.74s/it] 49%|████▉     | 98/200 [02:50<02:57,  1.74s/it] 50%|████▉     | 99/200 [02:52<02:55,  1.74s/it] 50%|█████     | 100/200 [02:54<02:53,  1.74s/it] 50%|█████     | 101/200 [02:55<02:52,  1.74s/it] 51%|█████     | 102/200 [02:57<02:50,  1.74s/it] 52%|█████▏    | 103/200 [02:59<02:48,  1.74s/it] 52%|█████▏    | 104/200 [03:00<02:46,  1.74s/it] 52%|█████▎    | 105/200 [03:02<02:45,  1.74s/it] 53%|█████▎    | 106/200 [03:04<02:43,  1.74s/it] 54%|█████▎    | 107/200 [03:06<02:41,  1.74s/it] 54%|█████▍    | 108/200 [03:07<02:39,  1.74s/it] 55%|█████▍    | 109/200 [03:09<02:38,  1.74s/it] 55%|█████▌    | 110/200 [03:11<02:36,  1.74s/it] 56%|█████▌    | 111/200 [03:13<02:34,  1.74s/it] 56%|█████▌    | 112/200 [03:14<02:33,  1.74s/it] 56%|█████▋    | 113/200 [03:16<02:31,  1.74s/it] 57%|█████▋    | 114/200 [03:18<02:29,  1.74s/it] 57%|█████▊    | 115/200 [03:20<02:27,  1.74s/it] 58%|█████▊    | 116/200 [03:21<02:26,  1.74s/it] 58%|█████▊    | 117/200 [03:23<02:24,  1.74s/it] 59%|█████▉    | 118/200 [03:25<02:22,  1.74s/it] 60%|█████▉    | 119/200 [03:27<02:20,  1.74s/it] 60%|██████    | 120/200 [03:28<02:18,  1.74s/it] 60%|██████    | 121/200 [03:30<02:17,  1.74s/it] 61%|██████    | 122/200 [03:32<02:15,  1.74s/it] 62%|██████▏   | 123/200 [03:33<02:13,  1.74s/it] 62%|██████▏   | 124/200 [03:35<02:12,  1.74s/it] 62%|██████▎   | 125/200 [03:37<02:10,  1.74s/it] 63%|██████▎   | 126/200 [03:39<02:08,  1.74s/it] 64%|██████▎   | 127/200 [03:40<02:06,  1.74s/it] 64%|██████▍   | 128/200 [03:42<02:05,  1.74s/it] 64%|██████▍   | 129/200 [03:44<02:03,  1.74s/it] 65%|██████▌   | 130/200 [03:46<02:01,  1.74s/it] 66%|██████▌   | 131/200 [03:47<01:59,  1.74s/it] 66%|██████▌   | 132/200 [03:49<01:58,  1.74s/it] 66%|██████▋   | 133/200 [03:51<01:56,  1.74s/it] 67%|██████▋   | 134/200 [03:53<01:54,  1.74s/it] 68%|██████▊   | 135/200 [03:54<01:52,  1.74s/it] 68%|██████▊   | 136/200 [03:56<01:51,  1.74s/it] 68%|██████▊   | 137/200 [03:58<01:49,  1.74s/it] 69%|██████▉   | 138/200 [04:00<01:47,  1.74s/it] 70%|██████▉   | 139/200 [04:01<01:46,  1.74s/it] 70%|███████   | 140/200 [04:03<01:44,  1.74s/it] 70%|███████   | 141/200 [04:05<01:42,  1.74s/it] 71%|███████   | 142/200 [04:07<01:40,  1.74s/it] 72%|███████▏  | 143/200 [04:08<01:39,  1.74s/it] 72%|███████▏  | 144/200 [04:10<01:37,  1.74s/it] 72%|███████▎  | 145/200 [04:12<01:35,  1.74s/it] 73%|███████▎  | 146/200 [04:13<01:33,  1.74s/it] 74%|███████▎  | 147/200 [04:15<01:32,  1.74s/it] 74%|███████▍  | 148/200 [04:17<01:30,  1.74s/it] 74%|███████▍  | 149/200 [04:19<01:28,  1.74s/it] 75%|███████▌  | 150/200 [04:20<01:26,  1.74s/it] 76%|███████▌  | 151/200 [04:22<01:25,  1.74s/it] 76%|███████▌  | 152/200 [04:24<01:23,  1.74s/it] 76%|███████▋  | 153/200 [04:26<01:21,  1.74s/it] 77%|███████▋  | 154/200 [04:27<01:20,  1.74s/it] 78%|███████▊  | 155/200 [04:29<01:18,  1.74s/it] 78%|███████▊  | 156/200 [04:31<01:16,  1.74s/it] 78%|███████▊  | 157/200 [04:33<01:14,  1.74s/it] 79%|███████▉  | 158/200 [04:34<01:13,  1.74s/it] 80%|███████▉  | 159/200 [04:36<01:11,  1.74s/it] 80%|████████  | 160/200 [04:38<01:09,  1.74s/it] 80%|████████  | 161/200 [04:40<01:07,  1.74s/it] 81%|████████  | 162/200 [04:41<01:06,  1.74s/it] 82%|████████▏ | 163/200 [04:43<01:04,  1.74s/it] 82%|████████▏ | 164/200 [04:45<01:02,  1.74s/it] 82%|████████▎ | 165/200 [04:47<01:00,  1.74s/it] 83%|████████▎ | 166/200 [04:48<00:59,  1.74s/it] 84%|████████▎ | 167/200 [04:50<00:57,  1.74s/it] 84%|████████▍ | 168/200 [04:52<00:55,  1.74s/it] 84%|████████▍ | 169/200 [04:53<00:53,  1.74s/it] 85%|████████▌ | 170/200 [04:55<00:52,  1.74s/it] 86%|████████▌ | 171/200 [04:57<00:50,  1.74s/it] 86%|████████▌ | 172/200 [04:59<00:48,  1.74s/it] 86%|████████▋ | 173/200 [05:00<00:46,  1.74s/it] 87%|████████▋ | 174/200 [05:02<00:45,  1.74s/it] 88%|████████▊ | 175/200 [05:04<00:43,  1.74s/it] 88%|████████▊ | 176/200 [05:06<00:41,  1.74s/it] 88%|████████▊ | 177/200 [05:07<00:39,  1.74s/it] 89%|████████▉ | 178/200 [05:09<00:38,  1.74s/it] 90%|████████▉ | 179/200 [05:11<00:36,  1.74s/it] 90%|█████████ | 180/200 [05:13<00:34,  1.74s/it] 90%|█████████ | 181/200 [05:14<00:33,  1.74s/it] 91%|█████████ | 182/200 [05:16<00:31,  1.74s/it] 92%|█████████▏| 183/200 [05:18<00:29,  1.74s/it] 92%|█████████▏| 184/200 [05:20<00:27,  1.74s/it] 92%|█████████▎| 185/200 [05:21<00:26,  1.74s/it] 93%|█████████▎| 186/200 [05:23<00:24,  1.74s/it] 94%|█████████▎| 187/200 [05:25<00:22,  1.74s/it] 94%|█████████▍| 188/200 [05:27<00:20,  1.74s/it] 94%|█████████▍| 189/200 [05:28<00:19,  1.74s/it] 95%|█████████▌| 190/200 [05:30<00:17,  1.74s/it] 96%|█████████▌| 191/200 [05:32<00:15,  1.74s/it] 96%|█████████▌| 192/200 [05:33<00:13,  1.74s/it] 96%|█████████▋| 193/200 [05:35<00:12,  1.74s/it] 97%|█████████▋| 194/200 [05:37<00:10,  1.74s/it] 98%|█████████▊| 195/200 [05:39<00:08,  1.74s/it] 98%|█████████▊| 196/200 [05:40<00:06,  1.74s/it] 98%|█████████▊| 197/200 [05:42<00:05,  1.74s/it] 99%|█████████▉| 198/200 [05:44<00:03,  1.74s/it]100%|█████████▉| 199/200 [05:46<00:01,  1.74s/it]100%|██████████| 200/200 [05:47<00:00,  1.72s/it]***** Running Evaluation *****
  Num examples = 800
  Batch size = 32

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:00<00:05,  4.21it/s][A
 12%|█▏        | 3/25 [00:00<00:07,  2.97it/s][A
 16%|█▌        | 4/25 [00:01<00:08,  2.57it/s][A
 20%|██        | 5/25 [00:01<00:08,  2.39it/s][A
 24%|██▍       | 6/25 [00:02<00:08,  2.28it/s][A
 28%|██▊       | 7/25 [00:02<00:08,  2.22it/s][A
 32%|███▏      | 8/25 [00:03<00:07,  2.18it/s][A
 36%|███▌      | 9/25 [00:03<00:07,  2.16it/s][A
 40%|████      | 10/25 [00:04<00:07,  2.14it/s][A
 44%|████▍     | 11/25 [00:04<00:06,  2.13it/s][A
 48%|████▊     | 12/25 [00:05<00:06,  2.12it/s][A
 52%|█████▏    | 13/25 [00:05<00:05,  2.11it/s][A
 56%|█████▌    | 14/25 [00:06<00:05,  2.11it/s][A
 60%|██████    | 15/25 [00:06<00:04,  2.11it/s][A
 64%|██████▍   | 16/25 [00:07<00:04,  2.11it/s][A
 68%|██████▊   | 17/25 [00:07<00:03,  2.11it/s][A
 72%|███████▏  | 18/25 [00:08<00:03,  2.11it/s][A
 76%|███████▌  | 19/25 [00:08<00:02,  2.08it/s][A
 80%|████████  | 20/25 [00:09<00:02,  2.09it/s][A
 84%|████████▍ | 21/25 [00:09<00:01,  2.09it/s][A
 88%|████████▊ | 22/25 [00:10<00:01,  2.10it/s][A
 92%|█████████▏| 23/25 [00:10<00:00,  2.10it/s][A
 96%|█████████▌| 24/25 [00:10<00:00,  2.06it/s][A
100%|██████████| 25/25 [00:11<00:00,  2.07it/s][A                                                 
                                               [A100%|██████████| 200/200 [05:59<00:00,  1.72s/it]
100%|██████████| 25/25 [00:11<00:00,  2.07it/s][A
                                               [A

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 200/200 [05:59<00:00,  1.72s/it]100%|██████████| 200/200 [05:59<00:00,  1.80s/it]
/projects/assigned/2122_ling573_elibales/repo/src/fine-tune-berttweet.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}
/projects/assigned/2122_ling573_elibales/repo/src/fine-tune-berttweet.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}
Configuration saved in src/models/berttweet-fine-tuned-preproc/config.json
Model weights saved in src/models/berttweet-fine-tuned-preproc/pytorch_model.bin
